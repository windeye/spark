!_TAG_FILE_FORMAT	2	/extended format; --format=1 will not append ;" to lines/
!_TAG_FILE_SORTED	1	/0=unsorted, 1=sorted, 2=foldcase/
!_TAG_PROGRAM_AUTHOR	Darren Hiebert	/dhiebert@users.sourceforge.net/
!_TAG_PROGRAM_NAME	Exuberant Ctags	//
!_TAG_PROGRAM_URL	http://ctags.sourceforge.net	/official site/
!_TAG_PROGRAM_VERSION	5.8	//
1	api/python/PythonRDD.scala	/^  override def addInPlace(val1: JList[Array[Byte]], val2: JList[Array[Byte]])$/;"	V
ACTOR_NAME	scheduler/cluster/CoarseGrainedSchedulerBackend.scala	/^  val ACTOR_NAME = "CoarseGrainedScheduler"$/;"	V
AKKA_RETRY_ATTEMPTS	storage/BlockManagerMaster.scala	/^  val AKKA_RETRY_ATTEMPTS: Int = System.getProperty("spark.akka.num.retries", "3").toInt$/;"	V
AKKA_RETRY_INTERVAL_MS	storage/BlockManagerMaster.scala	/^  val AKKA_RETRY_INTERVAL_MS: Int = System.getProperty("spark.akka.retry.wait", "3000").toInt$/;"	V
ALIGN_SIZE	util/SizeEstimator.scala	/^  private val ALIGN_SIZE = 8$/;"	V
APPENDS	api/python/PythonRDD.scala	/^  val APPENDS: Byte = 'e'$/;"	V
ARRAY_SAMPLE_SIZE	util/SizeEstimator.scala	/^  private val ARRAY_SAMPLE_SIZE = 100 \/\/ should be lower than ARRAY_SIZE_FOR_SAMPLING$/;"	V
ARRAY_SIZE_FOR_SAMPLING	util/SizeEstimator.scala	/^  private val ARRAY_SIZE_FOR_SAMPLING = 200$/;"	V
Accumulable	Accumulators.scala	/^class Accumulable[R, T] ($/;"	c
AccumulableParam	Accumulators.scala	/^trait AccumulableParam[R, T] extends Serializable {$/;"	t
Accumulator	Accumulators.scala	/^class Accumulator[T](@transient initialValue: T, param: AccumulatorParam[T])$/;"	c
AccumulatorParam	Accumulators.scala	/^trait AccumulatorParam[T] extends AccumulableParam[T, T] {$/;"	t
Aggregator	Aggregator.scala	/^case class Aggregator[K, V, C] ($/;"	r
AppendOnlyMap	util/AppendOnlyMap.scala	/^class AppendOnlyMap[K, V](initialCapacity: Int = 64) extends Iterable[(K, V)] with Serializable {$/;"	c
ApplicationRemoved	deploy/DeployMessage.scala	/^  case class ApplicationRemoved(message: String)$/;"	r
ApplicationSource	deploy/master/ApplicationSource.scala	/^class ApplicationSource(val application: ApplicationInfo) extends Source {$/;"	c
ApplicationState	deploy/master/ApplicationState.scala	/^  type ApplicationState = Value$/;"	T
AsyncRDDActions	rdd/AsyncRDDActions.scala	/^class AsyncRDDActions[T: ClassManifest](self: RDD[T]) extends Serializable with Logging {$/;"	c
B	storage/BlockMessage.scala	/^    val B = new BlockMessage()$/;"	V
BINUNICODE	api/python/PythonRDD.scala	/^  val BINUNICODE: Byte = 'X'$/;"	V
BLOCK_FAILED	storage/BlockInfo.scala	/^  private val BLOCK_FAILED: Long = -2L$/;"	V
BLOCK_PENDING	storage/BlockInfo.scala	/^  private val BLOCK_PENDING: Long = -1L$/;"	V
BLOCK_SIZE	broadcast/TorrentBroadcast.scala	/^  val BLOCK_SIZE = System.getProperty("spark.broadcast.blockSize", "4096").toInt * 1024$/;"	V
BOOLEAN_SIZE	util/SizeEstimator.scala	/^  private val BOOLEAN_SIZE = 1$/;"	V
BROADCAST	storage/BlockId.scala	/^  val BROADCAST = "broadcast_([0-9]+)".r$/;"	V
BROADCAST_HELPER	storage/BlockId.scala	/^  val BROADCAST_HELPER = "broadcast_([0-9]+)_([A-Za-z0-9]+)".r$/;"	V
BUFFER_MESSAGE	network/Message.scala	/^  val BUFFER_MESSAGE = 1111111111L$/;"	V
BYTE_SIZE	util/SizeEstimator.scala	/^  private val BYTE_SIZE    = 1$/;"	V
BasicBlockFetcherIterator	storage/BlockFetcherIterator.scala	/^  class BasicBlockFetcherIterator($/;"	c
BeginEvent	scheduler/DAGSchedulerEvent.scala	/^case class BeginEvent(task: Task[_], taskInfo: TaskInfo) extends DAGSchedulerEvent$/;"	r
BeginRecovery	deploy/master/MasterMessages.scala	/^  case class BeginRecovery(storedApps: Seq[ApplicationInfo], storedWorkers: Seq[WorkerInfo])$/;"	r
BitSet	util/collection/BitSet.scala	/^class BitSet(numBits: Int) {$/;"	c
BlockException	storage/BlockException.scala	/^case class BlockException(blockId: BlockId, message: String) extends Exception(message)$/;"	r
BlockFetcherIterator	storage/BlockFetcherIterator.scala	/^object BlockFetcherIterator {$/;"	o
BlockFetcherIterator	storage/BlockFetcherIterator.scala	/^trait BlockFetcherIterator extends Iterator[(BlockId, Option[Iterator[Any]])]$/;"	t
BlockManagerInfo	storage/BlockManagerMasterActor.scala	/^  class BlockManagerInfo($/;"	c
BlockManagerMasterActor	storage/BlockManagerMasterActor.scala	/^class BlockManagerMasterActor(val isLocal: Boolean) extends Actor with Logging {$/;"	c
BlockManagerMasterActor	storage/BlockManagerMasterActor.scala	/^object BlockManagerMasterActor {$/;"	o
BlockManagerMasterActor.BlockStatus	storage/StorageUtils.scala	/^import BlockManagerMasterActor.BlockStatus$/;"	i
BlockManagerSlaveActor	storage/BlockManagerSlaveActor.scala	/^class BlockManagerSlaveActor(blockManager: BlockManager) extends Actor {$/;"	c
BlockMessageArray	storage/BlockMessageArray.scala	/^class BlockMessageArray(var blockMessages: Seq[BlockMessage]) extends Seq[BlockMessage] with Logging {$/;"	c
BlockObjectWriter	storage/BlockObjectWriter.scala	/^abstract class BlockObjectWriter(val blockId: BlockId) {$/;"	a
BlockRDD	rdd/BlockRDD.scala	/^class BlockRDD[T: ClassManifest](sc: SparkContext, @transient blockIds: Array[BlockId])$/;"	c
BlockSize	broadcast/MultiTracker.scala	/^  def BlockSize = BlockSize_$/;"	m
BlockSize_	broadcast/MultiTracker.scala	/^  private var BlockSize_ = System.getProperty($/;"	v
BlockStatus	storage/BlockManagerMasterActor.scala	/^  case class BlockStatus(storageLevel: StorageLevel, memSize: Long, diskSize: Long)$/;"	r
BlockStore	storage/BlockStore.scala	/^abstract class BlockStore(val blockManager: BlockManager) extends Logging {$/;"	a
BoundedDouble	partial/BoundedDouble.scala	/^class BoundedDouble(val mean: Double, val confidence: Double, val low: Double, val high: Double) {$/;"	c
BoundedPriorityQueue	util/BoundedPriorityQueue.scala	/^class BoundedPriorityQueue[A](maxSize: Int)(implicit ord: Ordering[A])$/;"	c
Broadcast	broadcast/Broadcast.scala	/^abstract class Broadcast[T](private[spark] val id: Long) extends Serializable {$/;"	a
BroadcastManager	broadcast/Broadcast.scala	/^class BroadcastManager(val _isDriver: Boolean) extends Logging with Serializable {$/;"	c
BufferMessage	network/BufferMessage.scala	/^class BufferMessage(id_ : Int, val buffers: ArrayBuffer[ByteBuffer], var ackId: Int)$/;"	c
ByteBufferInputStream	util/ByteBufferInputStream.scala	/^class ByteBufferInputStream(private var buffer: ByteBuffer, dispose: Boolean = false)$/;"	c
C	storage/BlockMessage.scala	/^    val C = new BlockMessage()$/;"	V
CHAR_SIZE	util/SizeEstimator.scala	/^  private val CHAR_SIZE    = 2$/;"	V
CHUNK_SIZE	util/RateLimitedOutputStream.scala	/^  val CHUNK_SIZE = 8192$/;"	V
CONSOLE_DEFAULT_PERIOD	metrics/sink/ConsoleSink.scala	/^  val CONSOLE_DEFAULT_PERIOD = 10$/;"	V
CONSOLE_DEFAULT_UNIT	metrics/sink/ConsoleSink.scala	/^  val CONSOLE_DEFAULT_UNIT = "SECONDS"$/;"	V
CONSOLE_KEY_PERIOD	metrics/sink/ConsoleSink.scala	/^  val CONSOLE_KEY_PERIOD = "period"$/;"	V
CONSOLE_KEY_UNIT	metrics/sink/ConsoleSink.scala	/^  val CONSOLE_KEY_UNIT = "unit"$/;"	V
CPUS_PER_TASK	scheduler/cluster/ClusterTaskSetManager.scala	/^  val CPUS_PER_TASK = System.getProperty("spark.task.cpus", "1").toInt$/;"	V
CSV_DEFAULT_DIR	metrics/sink/CsvSink.scala	/^  val CSV_DEFAULT_DIR = "\/tmp\/"$/;"	V
CSV_DEFAULT_PERIOD	metrics/sink/CsvSink.scala	/^  val CSV_DEFAULT_PERIOD = 10$/;"	V
CSV_DEFAULT_UNIT	metrics/sink/CsvSink.scala	/^  val CSV_DEFAULT_UNIT = "SECONDS"$/;"	V
CSV_KEY_DIR	metrics/sink/CsvSink.scala	/^  val CSV_KEY_DIR = "directory"$/;"	V
CSV_KEY_PERIOD	metrics/sink/CsvSink.scala	/^  val CSV_KEY_PERIOD = "period"$/;"	V
CSV_KEY_UNIT	metrics/sink/CsvSink.scala	/^  val CSV_KEY_UNIT = "unit"$/;"	V
CartesianPartition	rdd/CartesianRDD.scala	/^class CartesianPartition($/;"	c
CartesianRDD	rdd/CartesianRDD.scala	/^class CartesianRDD[T: ClassManifest, U:ClassManifest]($/;"	c
CheckpointRDD	rdd/CheckpointRDD.scala	/^class CheckpointRDD[T: ClassManifest](sc: SparkContext, val checkpointPath: String)$/;"	c
CheckpointState	rdd/RDDCheckpointData.scala	/^  type CheckpointState = Value$/;"	T
CheckpointState._	rdd/RDDCheckpointData.scala	/^  import CheckpointState._$/;"	i
ClientActor	deploy/client/Client.scala	/^  class ClientActor extends Actor with Logging {$/;"	c
ClusterScheduler	scheduler/cluster/ClusterScheduler.scala	/^object ClusterScheduler {$/;"	o
CoGroupPartition	rdd/CoGroupedRDD.scala	/^class CoGroupPartition(idx: Int, val deps: Array[CoGroupSplitDep])$/;"	c
CoGroupedRDD	rdd/CoGroupedRDD.scala	/^class CoGroupedRDD[K](@transient var rdds: Seq[RDD[_ <: Product2[K, _]]], part: Partitioner)$/;"	c
CoalescedRDD	rdd/CoalescedRDD.scala	/^class CoalescedRDD[T: ClassManifest]($/;"	c
CoalescedRDDPartition	rdd/CoalescedRDD.scala	/^case class CoalescedRDDPartition($/;"	r
CoarseGrainedSchedulerBackend	scheduler/cluster/CoarseGrainedSchedulerBackend.scala	/^class CoarseGrainedSchedulerBackend(scheduler: ClusterScheduler, actorSystem: ActorSystem)$/;"	c
CompletionIterator	util/CompletionIterator.scala	/^abstract class CompletionIterator[+A, +I <: Iterator[A]](sub: I) extends Iterator[A]{$/;"	a
CompletionIterator	util/CompletionIterator.scala	/^object CompletionIterator {$/;"	o
ComplexFutureAction	FutureAction.scala	/^class ComplexFutureAction[T] extends FutureAction[T] {$/;"	c
CompressionCodec	io/CompressionCodec.scala	/^trait CompressionCodec {$/;"	t
Connection	network/Connection.scala	/^abstract class Connection(val channel: SocketChannel, val selector: Selector,$/;"	a
ConsoleSink	metrics/sink/ConsoleSink.scala	/^class ConsoleSink(val property: Properties, val registry: MetricRegistry) extends Sink {$/;"	c
CsvSink	metrics/sink/CsvSink.scala	/^class CsvSink(val property: Properties, val registry: MetricRegistry) extends Sink {$/;"	c
DAGScheduler	scheduler/DAGScheduler.scala	/^class DAGScheduler($/;"	c
DATE_FORMAT	deploy/WebUI.scala	/^  val DATE_FORMAT = new SimpleDateFormat("yyyy\/MM\/dd HH:mm:ss")$/;"	V
DATE_FORMAT	deploy/master/Master.scala	/^  val DATE_FORMAT = new SimpleDateFormat("yyyyMMddHHmmss")  \/\/ For application IDs$/;"	V
DATE_FORMAT	deploy/worker/Worker.scala	/^  val DATE_FORMAT = new SimpleDateFormat("yyyyMMddHHmmss")  \/\/ For worker and executor IDs$/;"	V
DATE_FORMAT	scheduler/JobLogger.scala	/^  private val DATE_FORMAT = new SimpleDateFormat("yyyy\/MM\/dd HH:mm:ss")$/;"	V
DEFAULT_INTEREST	network/Connection.scala	/^  val DEFAULT_INTEREST = SelectionKey.OP_READ$/;"	V
DEFAULT_MINIMUM_SHARE	scheduler/SchedulableBuilder.scala	/^  val DEFAULT_MINIMUM_SHARE = 0$/;"	V
DEFAULT_POOL_NAME	scheduler/SchedulableBuilder.scala	/^  val DEFAULT_POOL_NAME = "default"$/;"	V
DEFAULT_POOL_NAME	ui/jobs/JobProgressListener.scala	/^  val DEFAULT_POOL_NAME = "default"$/;"	V
DEFAULT_PORT	deploy/worker/ui/WorkerWebUI.scala	/^  val DEFAULT_PORT="8081"$/;"	V
DEFAULT_PORT	ui/SparkUI.scala	/^  val DEFAULT_PORT = "4040"$/;"	V
DEFAULT_PREFIX	metrics/MetricsConfig.scala	/^  val DEFAULT_PREFIX = "*"$/;"	V
DEFAULT_SCHEDULER_FILE	scheduler/SchedulableBuilder.scala	/^  val DEFAULT_SCHEDULER_FILE = "fairscheduler.xml"$/;"	V
DEFAULT_SCHEDULING_MODE	scheduler/SchedulableBuilder.scala	/^  val DEFAULT_SCHEDULING_MODE = SchedulingMode.FIFO$/;"	V
DEFAULT_WEIGHT	scheduler/SchedulableBuilder.scala	/^  val DEFAULT_WEIGHT = 1$/;"	V
DISK_ONLY	api/java/StorageLevels.java	/^  public static final StorageLevel DISK_ONLY = new StorageLevel(true, false, false, 1);$/;"	f	class:StorageLevels
DISK_ONLY	storage/StorageLevel.scala	/^  val DISK_ONLY = new StorageLevel(true, false, false)$/;"	V
DISK_ONLY_2	api/java/StorageLevels.java	/^  public static final StorageLevel DISK_ONLY_2 = new StorageLevel(true, false, false, 2);$/;"	f	class:StorageLevels
DISK_ONLY_2	storage/StorageLevel.scala	/^  val DISK_ONLY_2 = new StorageLevel(true, false, false, 2)$/;"	V
DISK_STORE_FAILED_TO_CREATE_DIR	executor/ExecutorExitCode.scala	/^  val DISK_STORE_FAILED_TO_CREATE_DIR = 53$/;"	V
DOUBLE_SIZE	util/SizeEstimator.scala	/^  private val DOUBLE_SIZE  = 8$/;"	V
DRIVER_AKKA_ACTOR_NAME	storage/BlockManagerMaster.scala	/^  val DRIVER_AKKA_ACTOR_NAME = "BlockManagerMaster"$/;"	V
Dependency	Dependency.scala	/^abstract class Dependency[T](val rdd: RDD[T]) extends Serializable$/;"	a
DeserializationStream	serializer/Serializer.scala	/^trait DeserializationStream {$/;"	t
DirectTaskResult	scheduler/TaskResult.scala	/^class DirectTaskResult[T](var value: T, var accumUpdates: Map[Long, Any], var metrics: TaskMetrics)$/;"	c
DiskBlockObjectWriter	storage/BlockObjectWriter.scala	/^class DiskBlockObjectWriter($/;"	c
Distribution	util/Distribution.scala	/^class Distribution(val data: Array[Double], val startIdx: Int, val endIdx: Int) {$/;"	c
Distribution	util/Distribution.scala	/^object Distribution {$/;"	o
DoubleFlatMapFunction	api/java/function/DoubleFlatMapFunction.java	/^public abstract class DoubleFlatMapFunction<T> extends WrappedFunction1<T, Iterable<Double>>$/;"	c
DoubleFunction	api/java/function/DoubleFunction.java	/^public abstract class DoubleFunction<T> extends WrappedFunction1<T, Double>$/;"	c
DoubleRDDFunctions	rdd/DoubleRDDFunctions.scala	/^class DoubleRDDFunctions(self: RDD[Double]) extends Logging with Serializable {$/;"	c
DriverActor	scheduler/cluster/CoarseGrainedSchedulerBackend.scala	/^  class DriverActor(sparkProperties: Seq[(String, String)]) extends Actor {$/;"	c
DriverHostAddress	broadcast/MultiTracker.scala	/^  def DriverHostAddress = DriverHostAddress_$/;"	m
DriverHostAddress_	broadcast/MultiTracker.scala	/^  private var DriverHostAddress_ = System.getProperty($/;"	v
DriverTrackerPort	broadcast/MultiTracker.scala	/^  def DriverTrackerPort = DriverTrackerPort_$/;"	m
DriverTrackerPort_	broadcast/MultiTracker.scala	/^  private var DriverTrackerPort_ = System.getProperty($/;"	v
EMPTY_BYTE_BUFFER	executor/Executor.scala	/^  private val EMPTY_BYTE_BUFFER = ByteBuffer.wrap(new Array[Byte](0))$/;"	V
EMPTY_LIST	api/python/PythonRDD.scala	/^  val EMPTY_LIST: Byte = ']'$/;"	V
EVENT_QUEUE_CAPACITY	scheduler/SparkListenerBus.scala	/^  private val EVENT_QUEUE_CAPACITY = 10000 $/;"	V
EXCEPTION_PRINT_INTERVAL	scheduler/cluster/ClusterTaskSetManager.scala	/^  val EXCEPTION_PRINT_INTERVAL =$/;"	V
EmptyRDD	rdd/EmptyRDD.scala	/^class EmptyRDD[T: ClassManifest](sc: SparkContext) extends RDD[T](sc, Nil) {$/;"	c
EndGameFraction	broadcast/MultiTracker.scala	/^  def EndGameFraction = EndGameFraction_$/;"	m
EndGameFraction_	broadcast/MultiTracker.scala	/^  private var EndGameFraction_ = System.getProperty($/;"	v
Entry	storage/MemoryStore.scala	/^  case class Entry(value: Any, size: Long, deserialized: Boolean)$/;"	r
ExecutorAdded	deploy/DeployMessage.scala	/^  case class ExecutorAdded(id: Int, workerId: String, hostPort: String, cores: Int, memory: Int) {$/;"	r
ExecutorExitCode	executor/ExecutorExitCode.scala	/^object ExecutorExitCode {$/;"	o
ExecutorExited	scheduler/cluster/ExecutorLossReason.scala	/^case class ExecutorExited(val exitCode: Int)$/;"	r
ExecutorGained	scheduler/DAGSchedulerEvent.scala	/^case class ExecutorGained(execId: String, host: String) extends DAGSchedulerEvent$/;"	r
ExecutorLossReason	scheduler/cluster/ExecutorLossReason.scala	/^class ExecutorLossReason(val message: String) {$/;"	c
ExecutorSource	executor/ExecutorSource.scala	/^class ExecutorSource(val executor: Executor, executorId: String) extends Source {$/;"	c
ExecutorState	deploy/ExecutorState.scala	/^  type ExecutorState = Value$/;"	T
ExecutorStateChanged	deploy/DeployMessage.scala	/^  case class ExecutorStateChanged($/;"	r
ExecutorUpdated	deploy/DeployMessage.scala	/^  case class ExecutorUpdated(id: Int, state: ExecutorState, message: Option[String],$/;"	r
FAIR_SCHEDULER_PROPERTIES	scheduler/SchedulableBuilder.scala	/^  val FAIR_SCHEDULER_PROPERTIES = "spark.scheduler.pool"$/;"	V
FIND_BROADCAST_TRACKER	broadcast/MultiTracker.scala	/^  val FIND_BROADCAST_TRACKER = 2$/;"	V
FINISHED_STATES	TaskState.scala	/^  val FINISHED_STATES = Set(FINISHED, FAILED, KILLED, LOST)$/;"	V
FLOAT_SIZE	util/SizeEstimator.scala	/^  private val FLOAT_SIZE   = 4$/;"	V
FetchRequest	storage/BlockFetcherIterator.scala	/^  class FetchRequest(val address: BlockManagerId, val blocks: Seq[(BlockId, Long)]) {$/;"	c
FetchResult	storage/BlockFetcherIterator.scala	/^  class FetchResult(val blockId: BlockId, val size: Long, val deserialize: () => Iterator[Any]) {$/;"	c
FlatMapFunction	api/java/function/FlatMapFunction.scala	/^abstract class FlatMapFunction[T, R] extends Function[T, java.lang.Iterable[R]] {$/;"	a
FlatMapFunction2	api/java/function/FlatMapFunction2.scala	/^abstract class FlatMapFunction2[A, B, C] extends Function2[A, B, java.lang.Iterable[C]] {$/;"	a
FlatMappedRDD	rdd/FlatMappedRDD.scala	/^class FlatMappedRDD[U: ClassManifest, T: ClassManifest]($/;"	c
FlatMappedValuesRDD	rdd/FlatMappedValuesRDD.scala	/^class FlatMappedValuesRDD[K, V, U](prev: RDD[_ <: Product2[K, V]], f: V => TraversableOnce[U])$/;"	c
Function	api/java/function/Function.java	/^public abstract class Function<T, R> extends WrappedFunction1<T, R> implements Serializable {$/;"	c
Function2	api/java/function/Function2.java	/^public abstract class Function2<T1, T2, R> extends WrappedFunction2<T1, T2, R>$/;"	c
Function3	api/java/function/Function3.java	/^public abstract class Function3<T1, T2, T3, R> extends WrappedFunction3<T1, T2, T3, R>$/;"	c
Function4	api/java/function/Function4.java	/^public abstract class Function4<T1, T2, T3, T4, R> extends WrappedFunction4<T1, T2, T3, T4, R>$/;"	c
FutureAction	FutureAction.scala	/^trait FutureAction[T] extends Future[T] {$/;"	t
GANGLIA_DEFAULT_MODE	metrics/sink/GangliaSink.scala	/^  val GANGLIA_DEFAULT_MODE = GMetric.UDPAddressingMode.MULTICAST$/;"	V
GANGLIA_DEFAULT_PERIOD	metrics/sink/GangliaSink.scala	/^  val GANGLIA_DEFAULT_PERIOD = 10$/;"	V
GANGLIA_DEFAULT_TTL	metrics/sink/GangliaSink.scala	/^  val GANGLIA_DEFAULT_TTL = 1$/;"	V
GANGLIA_DEFAULT_UNIT	metrics/sink/GangliaSink.scala	/^  val GANGLIA_DEFAULT_UNIT = TimeUnit.SECONDS$/;"	V
GANGLIA_KEY_HOST	metrics/sink/GangliaSink.scala	/^  val GANGLIA_KEY_HOST = "host"$/;"	V
GANGLIA_KEY_MODE	metrics/sink/GangliaSink.scala	/^  val GANGLIA_KEY_MODE = "mode"$/;"	V
GANGLIA_KEY_PERIOD	metrics/sink/GangliaSink.scala	/^  val GANGLIA_KEY_PERIOD = "period"$/;"	V
GANGLIA_KEY_PORT	metrics/sink/GangliaSink.scala	/^  val GANGLIA_KEY_PORT = "port"$/;"	V
GANGLIA_KEY_TTL	metrics/sink/GangliaSink.scala	/^  val GANGLIA_KEY_TTL = "ttl"$/;"	V
GANGLIA_KEY_UNIT	metrics/sink/GangliaSink.scala	/^  val GANGLIA_KEY_UNIT = "unit"$/;"	V
GB	util/Utils.scala	/^    val GB = 1L << 30$/;"	V
GangliaSink	metrics/sink/GangliaSink.scala	/^class GangliaSink(val property: Properties, val registry: MetricRegistry) extends Sink {$/;"	c
GetLocations	storage/BlockManagerMessages.scala	/^  case class GetLocations(blockId: BlockId) extends ToBlockManagerMaster$/;"	r
GetLocationsMultipleBlockIds	storage/BlockManagerMessages.scala	/^  case class GetLocationsMultipleBlockIds(blockIds: Array[BlockId]) extends ToBlockManagerMaster$/;"	r
GetPeers	storage/BlockManagerMessages.scala	/^  case class GetPeers(blockManagerId: BlockManagerId, size: Int) extends ToBlockManagerMaster$/;"	r
GettingResultEvent	scheduler/DAGSchedulerEvent.scala	/^case class GettingResultEvent(task: Task[_], taskInfo: TaskInfo) extends DAGSchedulerEvent $/;"	r
GrowableAccumulableParam	Accumulators.scala	/^class GrowableAccumulableParam[R <% Growable[T] with TraversableOnce[T] with Serializable, T]$/;"	c
GuideMultipleRequests	broadcast/BitTorrentBroadcast.scala	/^  class GuideMultipleRequests$/;"	c
GuideMultipleRequests	broadcast/TreeBroadcast.scala	/^  class GuideMultipleRequests$/;"	c
GuideSingleRequest	broadcast/BitTorrentBroadcast.scala	/^    class GuideSingleRequest(val clientSocket: Socket)$/;"	c
GuideSingleRequest	broadcast/TreeBroadcast.scala	/^    class GuideSingleRequest(val clientSocket: Socket)$/;"	c
HEADER_SIZE	network/MessageChunkHeader.scala	/^  val HEADER_SIZE = 40$/;"	V
HEADER_SIZE	network/netty/FileHeader.scala	/^  val HEADER_SIZE = 40$/;"	V
HEARTBEAT_MILLIS	deploy/worker/Worker.scala	/^  val HEARTBEAT_MILLIS = System.getProperty("spark.worker.timeout", "60").toLong * 1000 \/ 4$/;"	V
HadoopRDD	rdd/HadoopRDD.scala	/^class HadoopRDD[K, V]($/;"	c
HashPartitioner	Partitioner.scala	/^class HashPartitioner(partitions: Int) extends Partitioner {$/;"	c
HeartBeat	storage/BlockManagerMessages.scala	/^  case class HeartBeat(blockManagerId: BlockManagerId) extends ToBlockManagerMaster$/;"	r
Heartbeat	deploy/DeployMessage.scala	/^  case class Heartbeat(workerId: String) extends DeployMessage$/;"	r
ID_GENERATOR	storage/BlockManager.scala	/^  val ID_GENERATOR = new IdGenerator$/;"	V
INSTANCE_REGEX	metrics/MetricsConfig.scala	/^  val INSTANCE_REGEX = "^(\\\\*|[a-zA-Z]+)\\\\.(.+)".r$/;"	V
INTER_JOB_WAIT_MS	ui/UIWorkloadGenerator.scala	/^  val INTER_JOB_WAIT_MS = 5000$/;"	V
INT_SIZE	util/SizeEstimator.scala	/^  private val INT_SIZE     = 4$/;"	V
INVALID_POS	util/collection/OpenHashSet.scala	/^  val INVALID_POS = -1$/;"	V
Inbox	network/Connection.scala	/^  class Inbox() {$/;"	c
IndirectTaskResult	scheduler/TaskResult.scala	/^case class IndirectTaskResult[T](blockId: BlockId) extends TaskResult[T] with Serializable$/;"	r
Infos	util/SizeEstimator.scala	/^    classInfos.clear()$/;"	c
Infos	util/SizeEstimator.scala	/^    classInfos.put(classOf[Object], new ClassInfo(objectSize, Nil))$/;"	c
Infos	util/SizeEstimator.scala	/^    classInfos.put(cls, newInfo)$/;"	c
InputFormatInfo	scheduler/InputFormatInfo.scala	/^class InputFormatInfo(val configuration: Configuration, val inputFormatClazz: Class[_], $/;"	c
InputFormatInfo	scheduler/InputFormatInfo.scala	/^object InputFormatInfo {$/;"	o
IntHasher	util/collection/OpenHashSet.scala	/^  class IntHasher extends Hasher[Int] {$/;"	c
InterruptibleIterator	InterruptibleIterator.scala	/^class InterruptibleIterator[+T](val context: TaskContext, val delegate: Iterator[T])$/;"	c
JavaDoubleRDD	api/java/JavaDoubleRDD.scala	/^class JavaDoubleRDD(val srdd: RDD[scala.Double]) extends JavaRDDLike[Double, JavaDoubleRDD] {$/;"	c
JavaDoubleRDD	api/java/JavaDoubleRDD.scala	/^object JavaDoubleRDD {$/;"	o
JavaDoubleRDD.fromRDD	api/java/JavaDoubleRDD.scala	/^  import JavaDoubleRDD.fromRDD$/;"	i
JavaPairRDD	api/java/JavaPairRDD.scala	/^class JavaPairRDD[K, V](val rdd: RDD[(K, V)])(implicit val kManifest: ClassManifest[K],$/;"	c
JavaPairRDD	api/java/JavaPairRDD.scala	/^object JavaPairRDD {$/;"	o
JavaPairRDD._	api/java/JavaPairRDD.scala	/^  import JavaPairRDD._$/;"	i
JavaRDD	api/java/JavaRDD.scala	/^class JavaRDD[T](val rdd: RDD[T])(implicit val classManifest: ClassManifest[T]) extends$/;"	c
JavaRDD	api/java/JavaRDD.scala	/^object JavaRDD {$/;"	o
JavaRDDLike	api/java/JavaRDDLike.scala	/^trait JavaRDDLike[T, This <: JavaRDDLike[T, This]] extends Serializable {$/;"	t
JavaSerializer	serializer/JavaSerializer.scala	/^class JavaSerializer extends Serializer {$/;"	c
JavaSparkContext	api/java/JavaSparkContext.scala	/^class JavaSparkContext(val sc: SparkContext) extends JavaSparkContextVarargsWorkaround {$/;"	c
JavaSparkContext	api/java/JavaSparkContext.scala	/^object JavaSparkContext {$/;"	o
JavaSparkContextVarargsWorkaround	api/java/JavaSparkContextVarargsWorkaround.java	/^abstract class JavaSparkContextVarargsWorkaround {$/;"	c
JavaUtils	api/java/JavaUtils.scala	/^object JavaUtils {$/;"	o
JdbcRDD	rdd/JdbcRDD.scala	/^class JdbcRDD[T: ClassManifest]($/;"	c
JdbcRDD	rdd/JdbcRDD.scala	/^object JdbcRDD {$/;"	o
JmxSink	metrics/sink/JmxSink.scala	/^class JmxSink(val property: Properties, val registry: MetricRegistry) extends Sink {$/;"	c
JobLogger	scheduler/JobLogger.scala	/^class JobLogger(val user: String, val logDirName: String)$/;"	c
JvmSource	metrics/source/JvmSource.scala	/^class JvmSource extends Source {$/;"	c
KB	util/Utils.scala	/^    val KB = 1L << 10$/;"	V
KeyOrdering	api/java/JavaPairRDD.scala	/^    class KeyOrdering(val a: K) extends Ordered[K] {$/;"	c
KillExecutor	deploy/DeployMessage.scala	/^  case class KillExecutor(masterUrl: String, appId: String, execId: Int) extends DeployMessage$/;"	r
KillTask	scheduler/cluster/CoarseGrainedClusterMessage.scala	/^  case class KillTask(taskId: Long, executor: String) extends CoarseGrainedClusterMessage$/;"	r
KillTask	scheduler/local/LocalScheduler.scala	/^case class KillTask(taskId: Long)$/;"	r
KryoDeserializationStream	serializer/KryoSerializer.scala	/^class KryoDeserializationStream(kryo: Kryo, inStream: InputStream) extends DeserializationStream {$/;"	c
KryoRegistrator	serializer/KryoSerializer.scala	/^trait KryoRegistrator {$/;"	t
KryoSerializationStream	serializer/KryoSerializer.scala	/^class KryoSerializationStream(kryo: Kryo, outStream: OutputStream) extends SerializationStream {$/;"	c
KryoSerializer	serializer/KryoSerializer.scala	/^class KryoSerializer extends org.apache.spark.serializer.Serializer with Logging {$/;"	c
LOAD_FACTOR	util/AppendOnlyMap.scala	/^  private val LOAD_FACTOR = 0.7$/;"	V
LOCAL_CLUSTER_REGEX	SparkContext.scala	/^    val LOCAL_CLUSTER_REGEX = """local-cluster\\[\\s*([0-9]+)\\s*,\\s*([0-9]+)\\s*,\\s*([0-9]+)\\s*]""".r$/;"	V
LOCAL_N_FAILURES_REGEX	SparkContext.scala	/^    val LOCAL_N_FAILURES_REGEX = """local\\[([0-9]+)\\s*,\\s*([0-9]+)\\]""".r$/;"	V
LOCAL_N_REGEX	SparkContext.scala	/^    val LOCAL_N_REGEX = """local\\[([0-9]+)\\]""".r$/;"	V
LOG_BASE	MapOutputTracker.scala	/^  private val LOG_BASE = 1.1$/;"	V
LONG_SIZE	util/SizeEstimator.scala	/^  private val LONG_SIZE    = 8$/;"	V
LZFCompressionCodec	io/CompressionCodec.scala	/^class LZFCompressionCodec extends CompressionCodec {$/;"	c
LaunchExecutor	deploy/DeployMessage.scala	/^  case class LaunchExecutor($/;"	r
LaunchTask	scheduler/cluster/CoarseGrainedClusterMessage.scala	/^  case class LaunchTask(task: TaskDescription) extends CoarseGrainedClusterMessage$/;"	r
LeadershipStatus	deploy/master/ZooKeeperLeaderElectionAgent.scala	/^    type LeadershipStatus = Value$/;"	T
Loader	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^      classLoader = Thread.currentThread.getContextClassLoader$/;"	c
LocalActor	scheduler/local/LocalScheduler.scala	/^class LocalActor(localScheduler: LocalScheduler, private var freeCores: Int)$/;"	c
LocalReviveOffers	scheduler/local/LocalScheduler.scala	/^case class LocalReviveOffers()$/;"	r
LocalSparkCluster	deploy/LocalSparkCluster.scala	/^class LocalSparkCluster(numWorkers: Int, coresPerWorker: Int, memoryPerWorker: Int) extends Logging {$/;"	c
LocalStatusUpdate	scheduler/local/LocalScheduler.scala	/^case class LocalStatusUpdate(taskId: Long, state: TaskState, serializedData: ByteBuffer)$/;"	r
LocationIterator	rdd/CoalescedRDD.scala	/^  class LocationIterator(prev: RDD[_]) extends Iterator[(String, Partition)] {$/;"	c
Logging	Logging.scala	/^trait Logging {$/;"	t
LongHasher	util/collection/OpenHashSet.scala	/^  class LongHasher extends Hasher[Long] {$/;"	c
MARK	api/python/PythonRDD.scala	/^  val MARK: Byte = '('$/;"	V
MAX_DIR_CREATION_ATTEMPTS	storage/DiskBlockManager.scala	/^  private val MAX_DIR_CREATION_ATTEMPTS: Int = 10$/;"	V
MAX_NUM_RETRY	deploy/master/ApplicationState.scala	/^  val MAX_NUM_RETRY = 10$/;"	V
MAX_RECONNECT_ATTEMPTS	deploy/master/SparkZooKeeperSession.scala	/^  val MAX_RECONNECT_ATTEMPTS = 3$/;"	V
MAX_SLAVE_FAILURES	scheduler/cluster/mesos/CoarseMesosSchedulerBackend.scala	/^  val MAX_SLAVE_FAILURES = 2     \/\/ Blacklist a slave after this many failures$/;"	V
MAX_TASK_FAILURES	scheduler/cluster/ClusterTaskSetManager.scala	/^  val MAX_TASK_FAILURES = System.getProperty("spark.task.maxFailures", "4").toInt$/;"	V
MAX_TASK_FAILURES	scheduler/local/LocalTaskSetManager.scala	/^  val MAX_TASK_FAILURES = sched.maxFailures$/;"	V
MB	util/Utils.scala	/^    val MB = 1L << 20$/;"	V
MEMORY_AND_DISK	api/java/StorageLevels.java	/^  public static final StorageLevel MEMORY_AND_DISK = new StorageLevel(true, true, true, 1);$/;"	f	class:StorageLevels
MEMORY_AND_DISK	storage/StorageLevel.scala	/^  val MEMORY_AND_DISK = new StorageLevel(true, true, true)$/;"	V
MEMORY_AND_DISK_2	api/java/StorageLevels.java	/^  public static final StorageLevel MEMORY_AND_DISK_2 = new StorageLevel(true, true, true, 2);$/;"	f	class:StorageLevels
MEMORY_AND_DISK_2	storage/StorageLevel.scala	/^  val MEMORY_AND_DISK_2 = new StorageLevel(true, true, true, 2)$/;"	V
MEMORY_AND_DISK_SER	api/java/StorageLevels.java	/^  public static final StorageLevel MEMORY_AND_DISK_SER = new StorageLevel(true, true, false, 1);$/;"	f	class:StorageLevels
MEMORY_AND_DISK_SER	storage/StorageLevel.scala	/^  val MEMORY_AND_DISK_SER = new StorageLevel(true, true, false)$/;"	V
MEMORY_AND_DISK_SER_2	api/java/StorageLevels.java	/^  public static final StorageLevel MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, 2);$/;"	f	class:StorageLevels
MEMORY_AND_DISK_SER_2	storage/StorageLevel.scala	/^  val MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, 2)$/;"	V
MEMORY_ONLY	api/java/StorageLevels.java	/^  public static final StorageLevel MEMORY_ONLY = new StorageLevel(false, true, true, 1);$/;"	f	class:StorageLevels
MEMORY_ONLY	storage/StorageLevel.scala	/^  val MEMORY_ONLY = new StorageLevel(false, true, true)$/;"	V
MEMORY_ONLY_2	api/java/StorageLevels.java	/^  public static final StorageLevel MEMORY_ONLY_2 = new StorageLevel(false, true, true, 2);$/;"	f	class:StorageLevels
MEMORY_ONLY_2	storage/StorageLevel.scala	/^  val MEMORY_ONLY_2 = new StorageLevel(false, true, true, 2)$/;"	V
MEMORY_ONLY_SER	api/java/StorageLevels.java	/^  public static final StorageLevel MEMORY_ONLY_SER = new StorageLevel(false, true, false, 1);$/;"	f	class:StorageLevels
MEMORY_ONLY_SER	storage/StorageLevel.scala	/^  val MEMORY_ONLY_SER = new StorageLevel(false, true, false)$/;"	V
MEMORY_ONLY_SER_2	api/java/StorageLevels.java	/^  public static final StorageLevel MEMORY_ONLY_SER_2 = new StorageLevel(false, true, false, 2);$/;"	f	class:StorageLevels
MEMORY_ONLY_SER_2	storage/StorageLevel.scala	/^  val MEMORY_ONLY_SER_2 = new StorageLevel(false, true, false, 2)$/;"	V
MESOS_REGEX	SparkContext.scala	/^    val MESOS_REGEX = """(mesos:\/\/.*)""".r$/;"	V
METRICS_CONF	metrics/MetricsConfig.scala	/^  val METRICS_CONF = "metrics.properties"$/;"	V
MINIMAL_POLL_PERIOD	metrics/MetricsSystem.scala	/^  val MINIMAL_POLL_PERIOD = 1$/;"	V
MINIMAL_POLL_UNIT	metrics/MetricsSystem.scala	/^  val MINIMAL_POLL_UNIT = TimeUnit.SECONDS$/;"	V
MINIMUM_SHARES_PROPERTY	scheduler/SchedulableBuilder.scala	/^  val MINIMUM_SHARES_PROPERTY = "minShare"$/;"	V
Manifest	rdd/SequenceFileRDDFunctions.scala	/^        classManifest[T].erasure$/;"	c
MappedRDD	rdd/MappedRDD.scala	/^class MappedRDD[U: ClassManifest, T: ClassManifest](prev: RDD[T], f: T => U)$/;"	c
MappedValuesRDD	rdd/MappedValuesRDD.scala	/^class MappedValuesRDD[K, V, U](prev: RDD[_ <: Product2[K, V]], f: V => U)$/;"	c
MasterChangeAcknowledged	deploy/DeployMessage.scala	/^  case class MasterChangeAcknowledged(appId: String)$/;"	r
MasterChanged	deploy/DeployMessage.scala	/^  case class MasterChanged(masterUrl: String, masterWebUiUrl: String)$/;"	r
MasterState	deploy/master/RecoveryState.scala	/^  type MasterState = Value$/;"	T
MasterStateResponse	deploy/DeployMessage.scala	/^  case class MasterStateResponse(host: String, port: Int, workers: Array[WorkerInfo],$/;"	r
MasterWebUI	deploy/master/ui/MasterWebUI.scala	/^class MasterWebUI(val master: Master, requestedPort: Int) extends Logging {$/;"	c
MaxChatBlocks	broadcast/MultiTracker.scala	/^  def MaxChatBlocks = MaxChatBlocks_$/;"	m
MaxChatBlocks_	broadcast/MultiTracker.scala	/^  private var MaxChatBlocks_ = System.getProperty($/;"	v
MaxChatSlots	broadcast/MultiTracker.scala	/^  def MaxChatSlots = MaxChatSlots_$/;"	m
MaxChatSlots_	broadcast/MultiTracker.scala	/^  private var MaxChatSlots_ = System.getProperty($/;"	v
MaxChatTime	broadcast/MultiTracker.scala	/^  def MaxChatTime = MaxChatTime_$/;"	m
MaxChatTime_	broadcast/MultiTracker.scala	/^  private var MaxChatTime_ = System.getProperty($/;"	v
MaxDegree	broadcast/MultiTracker.scala	/^  def MaxDegree = MaxDegree_$/;"	m
MaxDegree_	broadcast/MultiTracker.scala	/^  private var MaxDegree_ = System.getProperty($/;"	v
MaxKnockInterval	broadcast/MultiTracker.scala	/^  def MaxKnockInterval = MaxKnockInterval_$/;"	m
MaxKnockInterval_	broadcast/MultiTracker.scala	/^  private var MaxKnockInterval_ = System.getProperty($/;"	v
MaxPeersInGuideResponse	broadcast/MultiTracker.scala	/^  def MaxPeersInGuideResponse = MaxPeersInGuideResponse_$/;"	m
MaxPeersInGuideResponse_	broadcast/MultiTracker.scala	/^  private var MaxPeersInGuideResponse_ = System.getProperty($/;"	v
MaxRetryCount	broadcast/MultiTracker.scala	/^  def MaxRetryCount = MaxRetryCount_$/;"	m
MaxRetryCount_	broadcast/MultiTracker.scala	/^  private var MaxRetryCount_ = System.getProperty($/;"	v
MessageChunk	network/MessageChunk.scala	/^class MessageChunk(val header: MessageChunkHeader, val buffer: ByteBuffer) {$/;"	c
MessageStatus	network/ConnectionManager.scala	/^  class MessageStatus($/;"	c
MetadataCleaner	util/MetadataCleaner.scala	/^class MetadataCleaner(cleanerType: MetadataCleanerType.MetadataCleanerType, cleanupFunc: (Long) => Unit) extends Logging {$/;"	c
MetadataCleaner	util/MetadataCleaner.scala	/^object MetadataCleaner {$/;"	o
MetadataCleanerType	util/MetadataCleaner.scala	/^  type MetadataCleanerType = Value$/;"	T
MetadataCleanerType	util/MetadataCleaner.scala	/^object MetadataCleanerType extends Enumeration("MapOutputTracker", "SparkContext", "HttpBroadcast", "DagScheduler", "ResultTask",$/;"	o
MetricsServlet	metrics/sink/MetricsServlet.scala	/^class MetricsServlet(val property: Properties, val registry: MetricRegistry) extends Sink {$/;"	c
MinKnockInterval	broadcast/MultiTracker.scala	/^  def MinKnockInterval = MinKnockInterval_$/;"	m
MinKnockInterval_	broadcast/MultiTracker.scala	/^  private var MinKnockInterval_ = System.getProperty($/;"	v
Multiplier	util/Vector.scala	/^  class Multiplier(num: Double) {$/;"	c
MutablePair	util/MutablePair.scala	/^case class MutablePair[@specialized(Int, Long, Double, Char, Boolean\/*, AnyRef*\/) T1,$/;"	r
NONE	api/java/StorageLevels.java	/^  public static final StorageLevel NONE = new StorageLevel(false, false, false, 1);$/;"	f	class:StorageLevels
NONE	storage/StorageLevel.scala	/^  val NONE = new StorageLevel(false, false, false)$/;"	V
NONEXISTENCE_MASK	util/collection/OpenHashSet.scala	/^  val NONEXISTENCE_MASK = 0x80000000$/;"	V
NORMAL_APPROX_SAMPLE_SIZE	partial/StudentTCacher.scala	/^  val NORMAL_APPROX_SAMPLE_SIZE = 100  \/\/ For samples bigger than this, use Gaussian approximation$/;"	V
NUM_PARTITIONS	ui/UIWorkloadGenerator.scala	/^  val NUM_PARTITIONS = 100$/;"	V
Name	Logging.scala	/^        className = className.substring(0, className.length - 1)$/;"	c
Name	TaskEndReason.scala	/^    className: String,$/;"	c
NarrowDependency	Dependency.scala	/^abstract class NarrowDependency[T](rdd: RDD[T]) extends Dependency(rdd) {$/;"	a
NettyBlockFetcherIterator	storage/BlockFetcherIterator.scala	/^  class NettyBlockFetcherIterator($/;"	c
NewHadoopPartition	rdd/NewHadoopRDD.scala	/^class NewHadoopPartition(rddId: Int, val index: Int, @transient rawSplit: InputSplit with Writable)$/;"	c
NewHadoopRDD	rdd/NewHadoopRDD.scala	/^class NewHadoopRDD[K, V]($/;"	c
OOM	executor/ExecutorExitCode.scala	/^  val OOM = 52$/;"	V
Of	serializer/KryoSerializer.scala	/^    classOf[Array[Byte]],$/;"	c
Of	serializer/KryoSerializer.scala	/^    classOf[BlockManagerId],$/;"	c
Of	serializer/KryoSerializer.scala	/^    classOf[GetBlock],$/;"	c
Of	serializer/KryoSerializer.scala	/^    classOf[GotBlock],$/;"	c
Of	serializer/KryoSerializer.scala	/^    classOf[MapStatus],$/;"	c
Of	serializer/KryoSerializer.scala	/^    classOf[PutBlock],$/;"	c
Of	serializer/KryoSerializer.scala	/^    classOf[StorageLevel],$/;"	c
OneToOneDependency	Dependency.scala	/^class OneToOneDependency[T](rdd: RDD[T]) extends NarrowDependency[T](rdd) {$/;"	c
OpenHashMap	util/collection/OpenHashMap.scala	/^class OpenHashMap[K >: Null : ClassManifest, @specialized(Long, Int, Double) V: ClassManifest]($/;"	c
OpenHashSet	util/collection/OpenHashSet.scala	/^class OpenHashSet[@specialized(Long, Int) T: ClassManifest]($/;"	c
OpenHashSet	util/collection/OpenHashSet.scala	/^object OpenHashSet {$/;"	o
OpenHashSet._	util/collection/OpenHashSet.scala	/^  import OpenHashSet._$/;"	i
OrderedRDDFunctions	rdd/OrderedRDDFunctions.scala	/^class OrderedRDDFunctions[K <% Ordered[K]: ClassManifest,$/;"	c
POLL_TIMEOUT	scheduler/DAGScheduler.scala	/^  val POLL_TIMEOUT = 10L$/;"	V
POOLS_PROPERTY	scheduler/SchedulableBuilder.scala	/^  val POOLS_PROPERTY = "pool"$/;"	V
POOL_NAME_PROPERTY	scheduler/SchedulableBuilder.scala	/^  val POOL_NAME_PROPERTY = "@name"$/;"	V
POSITION_MASK	util/collection/OpenHashSet.scala	/^  val POSITION_MASK = 0xEFFFFFF$/;"	V
PROTO	api/python/PythonRDD.scala	/^  val PROTO: Byte = 0x80.toByte$/;"	V
Page._	ui/UIUtils.scala	/^  import Page._$/;"	i
PairFlatMapFunction	api/java/function/PairFlatMapFunction.java	/^public abstract class PairFlatMapFunction<T, K, V>$/;"	c
PairFunction	api/java/function/PairFunction.java	/^public abstract class PairFunction<T, K, V> extends WrappedFunction1<T, Tuple2<K, V>>$/;"	c
PairRDDFunctions	rdd/PairRDDFunctions.scala	/^class PairRDDFunctions[K: ClassManifest, V: ClassManifest](self: RDD[(K, V)])$/;"	c
PartialResult	partial/PartialResult.scala	/^class PartialResult[R](initialVal: R, isFinal: Boolean) {$/;"	c
Partition	Partition.scala	/^trait Partition extends Serializable {$/;"	t
PartitionPruningRDD	rdd/PartitionPruningRDD.scala	/^class PartitionPruningRDD[T: ClassManifest]($/;"	c
PartitionPruningRDD	rdd/PartitionPruningRDD.scala	/^object PartitionPruningRDD {$/;"	o
PartitionPruningRDDPartition	rdd/PartitionPruningRDD.scala	/^class PartitionPruningRDDPartition(idx: Int, val parentSplit: Partition) extends Partition {$/;"	c
Partitioner	Partitioner.scala	/^abstract class Partitioner extends Serializable {$/;"	a
Partitioner	Partitioner.scala	/^object Partitioner {$/;"	o
PeerChatterController	broadcast/BitTorrentBroadcast.scala	/^  class PeerChatterController$/;"	c
PipedRDD	rdd/PipedRDD.scala	/^class PipedRDD[T: ClassManifest]($/;"	c
PipedRDD	rdd/PipedRDD.scala	/^object PipedRDD {$/;"	o
PrimitiveKeyOpenHashMap	util/collection/PrimitiveKeyOpenHashMap.scala	/^class PrimitiveKeyOpenHashMap[@specialized(Long, Int) K: ClassManifest,$/;"	c
PrimitiveVector	util/collection/PrimitiveVector.scala	/^class PrimitiveVector[@specialized(Long, Int, Double) V: ClassManifest](initialSize: Int = 64) {$/;"	c
PruneDependency	rdd/PartitionPruningRDD.scala	/^class PruneDependency[T](rdd: RDD[T], @transient partitionFilterFunc: Int => Boolean)$/;"	c
RDD	rdd/RDD.scala	/^abstract class RDD[T: ClassManifest]($/;"	a
RDD	storage/BlockId.scala	/^  val RDD = "rdd_([0-9]+)_([0-9]+)".r$/;"	V
RDDInfo	storage/StorageUtils.scala	/^case class RDDInfo(id: Int, name: String, storageLevel: StorageLevel,$/;"	r
REAPER_ITERATIONS	deploy/master/Master.scala	/^  val REAPER_ITERATIONS = System.getProperty("spark.dead.worker.persistence", "15").toInt$/;"	V
RECOVERY_DIR	deploy/master/Master.scala	/^  val RECOVERY_DIR = System.getProperty("spark.deploy.recoveryDirectory", "")$/;"	V
RECOVERY_MODE	deploy/master/Master.scala	/^  val RECOVERY_MODE = System.getProperty("spark.deploy.recoveryMode", "NONE")$/;"	V
REGISTER_BROADCAST_TRACKER	broadcast/MultiTracker.scala	/^  val REGISTER_BROADCAST_TRACKER = 0$/;"	V
REGISTRATION_RETRIES	deploy/client/Client.scala	/^  val REGISTRATION_RETRIES = 3$/;"	V
REGISTRATION_RETRIES	deploy/worker/Worker.scala	/^  val REGISTRATION_RETRIES = 3$/;"	V
REGISTRATION_TIMEOUT	deploy/client/Client.scala	/^  val REGISTRATION_TIMEOUT = 20.seconds$/;"	V
REGISTRATION_TIMEOUT	deploy/worker/Worker.scala	/^  val REGISTRATION_TIMEOUT = 20.seconds$/;"	V
RESUBMIT_TIMEOUT	scheduler/DAGScheduler.scala	/^  val RESUBMIT_TIMEOUT = 50L$/;"	V
RETAINED_APPLICATIONS	deploy/master/Master.scala	/^  val RETAINED_APPLICATIONS = System.getProperty("spark.deploy.retainedApplications", "200").toInt$/;"	V
RETAINED_STAGES	ui/jobs/JobProgressListener.scala	/^  val RETAINED_STAGES = System.getProperty("spark.ui.retained_stages", "1000").toInt$/;"	V
RETRY_WAIT_MILLIS	deploy/master/SparkZooKeeperSession.scala	/^  val RETRY_WAIT_MILLIS = 5000$/;"	V
RangeDependency	Dependency.scala	/^class RangeDependency[T](rdd: RDD[T], inStart: Int, outStart: Int, length: Int)$/;"	c
RangePartitioner	Partitioner.scala	/^class RangePartitioner[K <% Ordered[K]: ClassManifest, V]($/;"	c
RateLimitedOutputStream	util/RateLimitedOutputStream.scala	/^class RateLimitedOutputStream(out: OutputStream, bytesPerSec: Int) extends OutputStream {$/;"	c
RegisterApplication	deploy/DeployMessage.scala	/^  case class RegisterApplication(appDescription: ApplicationDescription)$/;"	r
RegisterBlockManager	storage/BlockManagerMessages.scala	/^  case class RegisterBlockManager($/;"	r
RegisterExecutor	scheduler/cluster/CoarseGrainedClusterMessage.scala	/^  case class RegisterExecutor(executorId: String, hostPort: String, cores: Int)$/;"	r
RegisterExecutorFailed	scheduler/cluster/CoarseGrainedClusterMessage.scala	/^  case class RegisterExecutorFailed(message: String) extends CoarseGrainedClusterMessage$/;"	r
RegisterWorker	deploy/DeployMessage.scala	/^  case class RegisterWorker($/;"	r
RegisterWorkerFailed	deploy/DeployMessage.scala	/^  case class RegisterWorkerFailed(message: String) extends DeployMessage$/;"	r
RegisteredApplication	deploy/DeployMessage.scala	/^  case class RegisteredApplication(appId: String, masterUrl: String) extends DeployMessage$/;"	r
RegisteredExecutor	scheduler/cluster/CoarseGrainedClusterMessage.scala	/^  case class RegisteredExecutor(sparkProperties: Seq[(String, String)])$/;"	r
RegisteredWorker	deploy/DeployMessage.scala	/^  case class RegisteredWorker(masterUrl: String, masterWebUiUrl: String) extends DeployMessage$/;"	r
RemoveBlock	storage/BlockManagerMessages.scala	/^  case class RemoveBlock(blockId: BlockId) extends ToBlockManagerSlave$/;"	r
RemoveExecutor	scheduler/cluster/CoarseGrainedClusterMessage.scala	/^  case class RemoveExecutor(executorId: String, reason: String) extends CoarseGrainedClusterMessage$/;"	r
RemoveExecutor	storage/BlockManagerMessages.scala	/^  case class RemoveExecutor(execId: String) extends ToBlockManagerMaster$/;"	r
RemoveRdd	storage/BlockManagerMessages.scala	/^  case class RemoveRdd(rddId: Int) extends ToBlockManagerSlave$/;"	r
Responder	ui/JettyUtils.scala	/^  type Responder[T] = HttpServletRequest => T$/;"	T
RuntimePercentage	scheduler/SparkListener.scala	/^case class RuntimePercentage(executorPct: Double, fetchPct: Option[Double], other: Double)$/;"	r
RuntimePercentage	scheduler/SparkListener.scala	/^object RuntimePercentage {$/;"	o
SCHEDULING_MODE_PROPERTY	scheduler/SchedulableBuilder.scala	/^  val SCHEDULING_MODE_PROPERTY = "schedulingMode"$/;"	V
SERVLET_DEFAULT_SAMPLE	metrics/sink/MetricsServlet.scala	/^  val SERVLET_DEFAULT_SAMPLE = false$/;"	V
SERVLET_KEY_PATH	metrics/sink/MetricsServlet.scala	/^  val SERVLET_KEY_PATH = "path"$/;"	V
SERVLET_KEY_SAMPLE	metrics/sink/MetricsServlet.scala	/^  val SERVLET_KEY_SAMPLE = "sample"$/;"	V
SHORT_SIZE	util/SizeEstimator.scala	/^  private val SHORT_SIZE   = 2$/;"	V
SHUFFLE	storage/BlockId.scala	/^  val SHUFFLE = "shuffle_([0-9]+)_([0-9]+)_([0-9]+)".r$/;"	V
SIMR_REGEX	SparkContext.scala	/^    val SIMR_REGEX = """simr:\/\/(.*)""".r$/;"	V
SINK_REGEX	metrics/MetricsSystem.scala	/^  val SINK_REGEX = "^sink\\\\.(.+)\\\\.(.+)".r$/;"	V
SOURCE_REGEX	metrics/MetricsSystem.scala	/^  val SOURCE_REGEX = "^source\\\\.(.+)\\\\.(.+)".r$/;"	V
SPARK_CLASS_REGEX	util/Utils.scala	/^  private val SPARK_CLASS_REGEX = """^org\\.apache\\.spark(\\.api\\.java)?(\\.util)?(\\.rdd)?\\.[A-Z]""".r$/;"	V
SPARK_JOB_DESCRIPTION	SparkContext.scala	/^  private[spark] val SPARK_JOB_DESCRIPTION = "spark.job.description"$/;"	V
SPARK_JOB_GROUP_ID	SparkContext.scala	/^  private[spark] val SPARK_JOB_GROUP_ID = "spark.jobGroup.id"$/;"	V
SPARK_REGEX	SparkContext.scala	/^    val SPARK_REGEX = """spark:\/\/(.*)""".r$/;"	V
SPARK_UNKNOWN_USER	SparkContext.scala	/^  private[spark] val SPARK_UNKNOWN_USER = "<unknown>"$/;"	V
SPECULATION_INTERVAL	scheduler/cluster/ClusterScheduler.scala	/^  val SPECULATION_INTERVAL = System.getProperty("spark.speculation.interval", "100").toLong$/;"	V
SPECULATION_MULTIPLIER	scheduler/cluster/ClusterTaskSetManager.scala	/^  val SPECULATION_MULTIPLIER = System.getProperty("spark.speculation.multiplier", "1.5").toDouble$/;"	V
SPECULATION_QUANTILE	scheduler/cluster/ClusterTaskSetManager.scala	/^  val SPECULATION_QUANTILE = System.getProperty("spark.speculation.quantile", "0.75").toDouble$/;"	V
STARVATION_TIMEOUT	scheduler/cluster/ClusterScheduler.scala	/^  val STARVATION_TIMEOUT = System.getProperty("spark.starvation.timeout", "15000").toLong$/;"	V
STATIC_RESOURCE_DIR	deploy/master/ui/MasterWebUI.scala	/^  val STATIC_RESOURCE_DIR = "org\/apache\/spark\/ui\/static"$/;"	V
STATIC_RESOURCE_DIR	deploy/worker/ui/WorkerWebUI.scala	/^  val STATIC_RESOURCE_DIR = "org\/apache\/spark\/ui\/static"$/;"	V
STATIC_RESOURCE_DIR	ui/SparkUI.scala	/^  val STATIC_RESOURCE_DIR = "org\/apache\/spark\/ui\/static"$/;"	V
STOP	api/python/PythonRDD.scala	/^  val STOP: Byte = '.'$/;"	V
STREAM	storage/BlockId.scala	/^  val STREAM = "input-([0-9]+)-([0-9]+)".r$/;"	V
SYNC_INTERVAL	util/RateLimitedOutputStream.scala	/^  val SYNC_INTERVAL = NANOSECONDS.convert(10, SECONDS)$/;"	V
SampledRDD	rdd/SampledRDD.scala	/^class SampledRDD[T: ClassManifest]($/;"	c
SampledRDDPartition	rdd/SampledRDD.scala	/^class SampledRDDPartition(val prev: Partition, val seed: Int) extends Partition with Serializable {$/;"	c
SchedulingMode	scheduler/SchedulingMode.scala	/^  type SchedulingMode = Value$/;"	T
SchedulingMode	scheduler/SchedulingMode.scala	/^object SchedulingMode extends Enumeration("FAIR", "FIFO", "NONE") {$/;"	o
SendingConnection	network/Connection.scala	/^class SendingConnection(val address: InetSocketAddress, selector_ : Selector,$/;"	c
SequenceFileRDDFunctions	rdd/SequenceFileRDDFunctions.scala	/^class SequenceFileRDDFunctions[K <% Writable: ClassManifest, V <% Writable : ClassManifest]($/;"	c
SerializableBuffer	util/SerializableBuffer.scala	/^class SerializableBuffer(@transient var buffer: ByteBuffer) extends Serializable {$/;"	c
SerializableWritable	SerializableWritable.scala	/^class SerializableWritable[T <: Writable](@transient var t: T) extends Serializable {$/;"	c
SerializationStream	serializer/Serializer.scala	/^trait SerializationStream {$/;"	t
Serializer	serializer/Serializer.scala	/^trait Serializer {$/;"	t
SerializerInstance	serializer/Serializer.scala	/^trait SerializerInstance {$/;"	t
ServeMultipleRequests	broadcast/BitTorrentBroadcast.scala	/^  class ServeMultipleRequests$/;"	c
ServeMultipleRequests	broadcast/TreeBroadcast.scala	/^  class ServeMultipleRequests$/;"	c
ServeSingleRequest	broadcast/BitTorrentBroadcast.scala	/^    class ServeSingleRequest(val clientSocket: Socket)$/;"	c
ServeSingleRequest	broadcast/TreeBroadcast.scala	/^    class ServeSingleRequest(val clientSocket: Socket)$/;"	c
ServerSocketTimeout	broadcast/MultiTracker.scala	/^  def ServerSocketTimeout = ServerSocketTimeout_$/;"	m
ServerSocketTimeout_	broadcast/MultiTracker.scala	/^  private var ServerSocketTimeout_ = System.getProperty($/;"	v
ShuffleBlockId	storage/BlockId.scala	/^case class ShuffleBlockId(shuffleId: Int, mapId: Int, reduceId: Int) extends BlockId {$/;"	r
ShuffleBlockManager	storage/ShuffleBlockManager.scala	/^class ShuffleBlockManager(blockManager: BlockManager) {$/;"	c
ShuffleBlockManager	storage/ShuffleBlockManager.scala	/^object ShuffleBlockManager {$/;"	o
ShuffleDependency	Dependency.scala	/^class ShuffleDependency[K, V]($/;"	c
ShuffleId	storage/ShuffleBlockManager.scala	/^  type ShuffleId = Int$/;"	T
ShuffleReadMetrics	executor/TaskMetrics.scala	/^class ShuffleReadMetrics extends Serializable {$/;"	c
ShuffleWriteMetrics	executor/TaskMetrics.scala	/^class ShuffleWriteMetrics extends Serializable {$/;"	c
ShuffledRDD	rdd/ShuffledRDD.scala	/^class ShuffledRDD[K, V, P <: Product2[K, V] : ClassManifest]($/;"	c
SimpleFutureAction	FutureAction.scala	/^class SimpleFutureAction[T] private[spark](jobWaiter: JobWaiter[_], resultFunc: => T)$/;"	c
Sink	metrics/sink/Sink.scala	/^trait Sink {$/;"	t
Size	util/SizeEstimator.scala	/^    objectSize = if (!is64bit) 8 else {$/;"	o
SlaveLost	scheduler/cluster/ExecutorLossReason.scala	/^case class SlaveLost(_message: String = "Slave lost")$/;"	r
SnappyCompressionCodec	io/CompressionCodec.scala	/^class SnappyCompressionCodec extends CompressionCodec {$/;"	c
Source	metrics/source/Source.scala	/^trait Source {$/;"	t
SparkContext	SparkContext.scala	/^class SparkContext($/;"	c
SparkContext	SparkContext.scala	/^object SparkContext {$/;"	o
SparkEnv	SparkEnv.scala	/^class SparkEnv ($/;"	c
SparkEnv	SparkEnv.scala	/^object SparkEnv extends Logging {$/;"	o
SparkException	SparkException.scala	/^class SparkException(message: String, cause: Throwable)$/;"	c
SparkFiles	SparkFiles.java	/^  private SparkFiles() {}$/;"	m	class:SparkFiles	file:
SparkFiles	SparkFiles.java	/^public class SparkFiles {$/;"	c
SparkHadoopUtil	deploy/SparkHadoopUtil.scala	/^class SparkHadoopUtil {$/;"	c
SparkHadoopUtil	deploy/SparkHadoopUtil.scala	/^object SparkHadoopUtil {$/;"	o
SparkHadoopWriter	SparkHadoopWriter.scala	/^class SparkHadoopWriter(@transient jobConf: JobConf)$/;"	c
SparkHadoopWriter	SparkHadoopWriter.scala	/^object SparkHadoopWriter {$/;"	o
SparkListener	scheduler/SparkListener.scala	/^trait SparkListener {$/;"	t
SparkListenerJobEnd	scheduler/SparkListener.scala	/^case class SparkListenerJobEnd(job: ActiveJob, jobResult: JobResult)$/;"	r
SparkListenerJobStart	scheduler/SparkListener.scala	/^case class SparkListenerJobStart(job: ActiveJob, properties: Properties = null)$/;"	r
SparkListenerStageSubmitted	scheduler/SparkListener.scala	/^case class SparkListenerStageSubmitted(stage: StageInfo, properties: Properties)$/;"	r
SparkListenerTaskEnd	scheduler/SparkListener.scala	/^case class SparkListenerTaskEnd(task: Task[_], reason: TaskEndReason, taskInfo: TaskInfo,$/;"	r
SparkListenerTaskGettingResult	scheduler/SparkListener.scala	/^case class SparkListenerTaskGettingResult($/;"	r
SparkListenerTaskStart	scheduler/SparkListener.scala	/^case class SparkListenerTaskStart(task: Task[_], taskInfo: TaskInfo) extends SparkListenerEvents$/;"	r
SparkZooKeeperWatcher	deploy/master/SparkZooKeeperSession.scala	/^trait SparkZooKeeperWatcher {$/;"	t
SplitInfo	scheduler/SplitInfo.scala	/^class SplitInfo(val inputFormatClazz: Class[_], val hostLocation: String, val path: String,$/;"	c
SplitInfo	scheduler/SplitInfo.scala	/^object SplitInfo {$/;"	o
StageCompleted	scheduler/SparkListener.scala	/^case class StageCompleted(val stage: StageInfo) extends SparkListenerEvents$/;"	r
StageInfo	scheduler/StageInfo.scala	/^class StageInfo($/;"	c
StatCounter	util/StatCounter.scala	/^class StatCounter(values: TraversableOnce[Double]) extends Serializable {$/;"	c
StatCounter	util/StatCounter.scala	/^object StatCounter {$/;"	o
StatsReportListener	scheduler/SparkListener.scala	/^class StatsReportListener extends SparkListener with Logging {$/;"	c
StatsReportListener	scheduler/SparkListener.scala	/^object StatsReportListener extends Logging {$/;"	o
StatusUpdate	scheduler/cluster/CoarseGrainedClusterMessage.scala	/^  case class StatusUpdate(executorId: String, taskId: Long, state: TaskState,$/;"	r
StatusUpdate	scheduler/cluster/CoarseGrainedClusterMessage.scala	/^  object StatusUpdate {$/;"	o
StopBroadcast	broadcast/SourceInfo.scala	/^  val StopBroadcast = -2$/;"	V
StorageLevel	storage/StorageLevel.scala	/^class StorageLevel private($/;"	c
StorageLevel	storage/StorageLevel.scala	/^object StorageLevel {$/;"	o
StorageLevels	api/java/StorageLevels.java	/^public class StorageLevels {$/;"	c
StoragePerfTester	storage/StoragePerfTester.scala	/^object StoragePerfTester {$/;"	o
StorageStatus	storage/StorageUtils.scala	/^case class StorageStatus(blockManagerId: BlockManagerId, maxMem: Long,$/;"	r
StorageUtils	storage/StorageUtils.scala	/^object StorageUtils {$/;"	o
TASKRESULT	storage/BlockId.scala	/^  val TASKRESULT = "taskresult_([0-9]+)".r$/;"	V
TB	util/Utils.scala	/^    val TB = 1L << 40$/;"	V
TEST	storage/BlockId.scala	/^  val TEST = "test_(.*)".r$/;"	V
THREADS	scheduler/cluster/TaskResultGetter.scala	/^  private val THREADS = System.getProperty("spark.resultGetter.threads", "4").toInt$/;"	V
TUPLE2	api/python/PythonRDD.scala	/^  val TUPLE2: Byte = 0x86.toByte$/;"	V
TWO	api/python/PythonRDD.scala	/^  val TWO: Byte = 0x02.toByte$/;"	V
TYPE_GET_BLOCK	storage/BlockMessage.scala	/^  val TYPE_GET_BLOCK: Int = 1$/;"	V
TYPE_GOT_BLOCK	storage/BlockMessage.scala	/^  val TYPE_GOT_BLOCK: Int = 2$/;"	V
TYPE_NON_INITIALIZED	storage/BlockMessage.scala	/^  val TYPE_NON_INITIALIZED: Int = 0$/;"	V
TYPE_PUT_BLOCK	storage/BlockMessage.scala	/^  val TYPE_PUT_BLOCK: Int = 3$/;"	V
TalkToGuide	broadcast/BitTorrentBroadcast.scala	/^  class TalkToGuide(gInfo: SourceInfo)$/;"	c
TalkToPeer	broadcast/BitTorrentBroadcast.scala	/^    class TalkToPeer(peerToTalkTo: SourceInfo)$/;"	c
TaskContext	TaskContext.scala	/^class TaskContext($/;"	c
TaskInfo	scheduler/TaskInfo.scala	/^class TaskInfo($/;"	c
TaskLocality	scheduler/TaskLocality.scala	/^  type TaskLocality = Value$/;"	T
TaskLocality.{PROCESS_LOCAL, NODE_LOCAL, RACK_LOCAL, ANY}	scheduler/cluster/ClusterTaskSetManager.scala	/^    import TaskLocality.{PROCESS_LOCAL, NODE_LOCAL, RACK_LOCAL, ANY}$/;"	i
TaskLocation	scheduler/TaskLocation.scala	/^class TaskLocation private (val host: String, val executorId: Option[String]) extends Serializable {$/;"	c
TaskMetrics	executor/TaskMetrics.scala	/^class TaskMetrics extends Serializable {$/;"	c
TaskMetrics	executor/TaskMetrics.scala	/^object TaskMetrics {$/;"	o
TaskRunner	executor/Executor.scala	/^  class TaskRunner(execBackend: ExecutorBackend, taskId: Long, serializedTask: ByteBuffer)$/;"	c
TaskSetFailed	scheduler/DAGSchedulerEvent.scala	/^case class TaskSetFailed(taskSet: TaskSet, reason: String) extends DAGSchedulerEvent$/;"	r
TaskState	TaskState.scala	/^  type TaskState = Value$/;"	T
TestListener	deploy/client/TestClient.scala	/^  class TestListener extends ClientListener with Logging {$/;"	c
TimeStampedHashMap	util/TimeStampedHashMap.scala	/^class TimeStampedHashMap[A, B] extends Map[A, B]() with Logging {$/;"	c
TimeStampedHashSet	util/TimeStampedHashSet.scala	/^class TimeStampedHashSet[A] extends Set[A] {$/;"	c
TrackMultipleValues	broadcast/MultiTracker.scala	/^  class TrackMultipleValues$/;"	c
TrackerSocketTimeout	broadcast/MultiTracker.scala	/^  def TrackerSocketTimeout = TrackerSocketTimeout_$/;"	m
TrackerSocketTimeout_	broadcast/MultiTracker.scala	/^  private var TrackerSocketTimeout_ = System.getProperty($/;"	v
TxNotStartedRetry	broadcast/SourceInfo.scala	/^  val TxNotStartedRetry = -1$/;"	V
TxOverGoToDefault	broadcast/SourceInfo.scala	/^  val TxOverGoToDefault = -3$/;"	V
UNCAUGHT_EXCEPTION	executor/ExecutorExitCode.scala	/^  val UNCAUGHT_EXCEPTION = 50$/;"	V
UNCAUGHT_EXCEPTION_TWICE	executor/ExecutorExitCode.scala	/^  val UNCAUGHT_EXCEPTION_TWICE = 51$/;"	V
UNREGISTER_BROADCAST_TRACKER	broadcast/MultiTracker.scala	/^  val UNREGISTER_BROADCAST_TRACKER = 1$/;"	V
UnionRDD	rdd/UnionRDD.scala	/^class UnionRDD[T: ClassManifest]($/;"	c
UnusedParam	broadcast/SourceInfo.scala	/^  val UnusedParam = 0$/;"	V
UpdateBlockInfo	storage/BlockManagerMessages.scala	/^  class UpdateBlockInfo($/;"	c
UpdateBlockInfo	storage/BlockManagerMessages.scala	/^  object UpdateBlockInfo {$/;"	o
Utils.bytesToString	storage/StorageUtils.scala	/^    import Utils.bytesToString$/;"	i
Vector	util/Vector.scala	/^class Vector(val elements: Array[Double]) extends Serializable {$/;"	c
Vector	util/Vector.scala	/^object Vector {$/;"	o
VoidFunction	api/java/function/VoidFunction.scala	/^abstract class VoidFunction[T] extends Serializable {$/;"	a
VoidFunction	api/java/function/VoidFunction.scala	/^object VoidFunction {$/;"	o
WEIGHT_PROPERTY	scheduler/SchedulableBuilder.scala	/^  val WEIGHT_PROPERTY = "weight"$/;"	V
WORKER_TIMEOUT	deploy/master/Master.scala	/^  val WORKER_TIMEOUT = System.getProperty("spark.worker.timeout", "60").toLong * 1000$/;"	V
WORKING_DIR	deploy/master/ZooKeeperLeaderElectionAgent.scala	/^  val WORKING_DIR = System.getProperty("spark.deploy.zookeeper.dir", "\/spark") + "\/leader_election"$/;"	V
WORKING_DIR	deploy/master/ZooKeeperPersistenceEngine.scala	/^  val WORKING_DIR = System.getProperty("spark.deploy.zookeeper.dir", "\/spark") + "\/master_status"$/;"	V
WebUIPortResponse	deploy/master/MasterMessages.scala	/^  case class WebUIPortResponse(webUIBoundPort: Int)$/;"	r
WorkerOffer	scheduler/cluster/WorkerOffer.scala	/^class WorkerOffer(val executorId: String, val host: String, val cores: Int)$/;"	c
WorkerSchedulerStateResponse	deploy/DeployMessage.scala	/^  case class WorkerSchedulerStateResponse(id: String, executors: List[ExecutorDescription])$/;"	r
WorkerState	deploy/master/WorkerState.scala	/^  type WorkerState = Value$/;"	T
WorkerStateResponse	deploy/DeployMessage.scala	/^  case class WorkerStateResponse(host: String, port: Int, workerId: String,$/;"	r
WorkerWebUI	deploy/worker/ui/WorkerWebUI.scala	/^class WorkerWebUI(val worker: Worker, val workDir: File, requestedPort: Option[Int] = None)$/;"	c
ZK_ACL	deploy/master/SparkZooKeeperSession.scala	/^  val ZK_ACL = ZooDefs.Ids.OPEN_ACL_UNSAFE$/;"	V
ZK_CHECK_PERIOD_MILLIS	deploy/master/SparkZooKeeperSession.scala	/^  val ZK_CHECK_PERIOD_MILLIS = 10000$/;"	V
ZK_TIMEOUT_MILLIS	deploy/master/SparkZooKeeperSession.scala	/^  val ZK_TIMEOUT_MILLIS = 30000$/;"	V
ZK_URL	deploy/master/SparkZooKeeperSession.scala	/^  val ZK_URL = System.getProperty("spark.deploy.zookeeper.url", "")$/;"	V
ZippedPartitionsBaseRDD	rdd/ZippedPartitionsRDD.scala	/^abstract class ZippedPartitionsBaseRDD[V: ClassManifest]($/;"	a
ZippedPartitionsRDD2	rdd/ZippedPartitionsRDD.scala	/^class ZippedPartitionsRDD2[A: ClassManifest, B: ClassManifest, V: ClassManifest]($/;"	c
ZippedPartitionsRDD3	rdd/ZippedPartitionsRDD.scala	/^class ZippedPartitionsRDD3$/;"	c
ZippedPartitionsRDD4	rdd/ZippedPartitionsRDD.scala	/^class ZippedPartitionsRDD4$/;"	c
ZippedRDD	rdd/ZippedRDD.scala	/^class ZippedRDD[T: ClassManifest, U: ClassManifest]($/;"	c
ZooKeeperPersistenceEngine	deploy/master/ZooKeeperPersistenceEngine.scala	/^class ZooKeeperPersistenceEngine(serialization: Serialization)$/;"	c
_	broadcast/MultiTracker.scala	/^  private var MaxKnockInterval_ = System.getProperty($/;"	V
_	broadcast/MultiTracker.scala	/^  private var MinKnockInterval_ = System.getProperty($/;"	V
_1	util/MutablePair.scala	/^  (var _1: T1, var _2: T2)$/;"	v
_array	util/collection/PrimitiveVector.scala	/^  private var _array: Array[V] = _$/;"	v
_bitset	util/collection/OpenHashSet.scala	/^  protected var _bitset = new BitSet(_capacity)$/;"	v
_blocks	storage/BlockManagerMasterActor.scala	/^    private val _blocks = new JHashMap[BlockId, BlockStatus]$/;"	V
_cancelled	FutureAction.scala	/^  @volatile private var _cancelled: Boolean = false$/;"	v
_capacity	util/collection/OpenHashSet.scala	/^  protected var _capacity = nextPowerOf2(initialCapacity)$/;"	v
_data	util/collection/OpenHashSet.scala	/^  protected var _data: Array[T] = _$/;"	v
_default	serializer/SerializerManager.scala	/^  private var _default: Serializer = _$/;"	v
_env	executor/Executor.scala	/^      val _env = SparkEnv.createFromSystemProperties(executorId, slaveHostname, 0,$/;"	V
_fetchWaitTime	storage/BlockFetcherIterator.scala	/^    private var _fetchWaitTime = 0l$/;"	v
_isDriver	broadcast/Broadcast.scala	/^class BroadcastManager(val _isDriver: Boolean) extends Logging with Serializable {$/;"	V
_isDriver	broadcast/MultiTracker.scala	/^  private var _isDriver = false$/;"	v
_jobFinished	scheduler/JobWaiter.scala	/^  private var _jobFinished = totalTasks == 0$/;"	v
_keyList	scheduler/cluster/ClusterScheduler.scala	/^    val _keyList = new ArrayBuffer[K](map.size)$/;"	V
_keySet	util/collection/OpenHashMap.scala	/^  protected var _keySet = new OpenHashSet[K](initialCapacity)$/;"	v
_keySet	util/collection/PrimitiveKeyOpenHashMap.scala	/^  protected var _keySet: OpenHashSet[K] = _$/;"	v
_killed	scheduler/Task.scala	/^  @volatile @transient private var _killed = false$/;"	v
_lastSeenMs	storage/BlockManagerMasterActor.scala	/^    private var _lastSeenMs: Long = timeMs$/;"	v
_listener	ui/exec/ExecutorsUI.scala	/^  private var _listener: Option[ExecutorsListener] = None$/;"	v
_listener	ui/jobs/JobProgressUI.scala	/^  private var _listener: Option[JobProgressListener] = None$/;"	v
_mask	util/collection/OpenHashSet.scala	/^  protected var _mask = _capacity - 1$/;"	v
_nextObj	api/python/PythonRDD.scala	/^      var _nextObj = read()$/;"	v
_numBlocksToFetch	storage/BlockFetcherIterator.scala	/^    protected var _numBlocksToFetch = 0$/;"	v
_numElements	util/collection/PrimitiveVector.scala	/^  private var _numElements = 0$/;"	v
_oldValues	util/collection/OpenHashMap.scala	/^  @transient private var _oldValues: Array[V] = null$/;"	v
_oldValues	util/collection/PrimitiveKeyOpenHashMap.scala	/^  private var _oldValues: Array[V] = null$/;"	v
_remainingMem	storage/BlockManagerMasterActor.scala	/^    private var _remainingMem: Long = maxMem$/;"	v
_remoteBytesRead	storage/BlockFetcherIterator.scala	/^    private var _remoteBytesRead = 0l$/;"	v
_remoteFetchTime	storage/BlockFetcherIterator.scala	/^    private var _remoteFetchTime = 0l$/;"	v
_retryCount	deploy/master/ApplicationInfo.scala	/^  private var _retryCount = 0$/;"	v
_size	util/collection/OpenHashSet.scala	/^  protected var _size = 0$/;"	v
_timeWriting	storage/BlockObjectWriter.scala	/^    private var _timeWriting = 0L$/;"	v
_timeWriting	storage/BlockObjectWriter.scala	/^  private var _timeWriting = 0L$/;"	v
_values	util/collection/OpenHashMap.scala	/^  private var _values: Array[V] = _$/;"	v
_values	util/collection/PrimitiveKeyOpenHashMap.scala	/^  private var _values: Array[V] = _$/;"	v
a	api/java/JavaPairRDD.scala	/^    class KeyOrdering(val a: K) extends Ordered[K] {$/;"	V
a	rdd/RDD.scala	/^      val a = constructA(index)$/;"	V
ab	network/MessageChunk.scala	/^    val ab = new ArrayBuffer[ByteBuffer]()$/;"	V
abort	scheduler/cluster/ClusterTaskSetManager.scala	/^  def abort(message: String) {$/;"	m
absolutePath	util/Utils.scala	/^    val absolutePath = file.getAbsolutePath()$/;"	V
acceptConnection	network/ConnectionManager.scala	/^  def acceptConnection(key: SelectionKey) {$/;"	m
accessedFields	util/ClosureCleaner.scala	/^    val accessedFields = Map[Class[_], Set[String]]()$/;"	V
accumUpdates	executor/Executor.scala	/^        val accumUpdates = Accumulators.values$/;"	V
accums	Accumulators.scala	/^      val accums = localAccums.getOrElseUpdate(Thread.currentThread, Map())$/;"	V
accumulable	SparkContext.scala	/^  def accumulable[T, R](initialValue: T)(implicit param: AccumulableParam[T, R]) =$/;"	m
accumulable	api/java/JavaSparkContext.scala	/^  def accumulable[T, R](initialValue: T, param: AccumulableParam[T, R]): Accumulable[T, R] =$/;"	m
accumulableCollection	SparkContext.scala	/^  def accumulableCollection[R <% Growable[T] with TraversableOnce[T] with Serializable, T](initialValue: R) = {$/;"	m
accumulator	SparkContext.scala	/^  def accumulator[T](initialValue: T)(implicit param: AccumulatorParam[T]) =$/;"	m
accumulator	api/java/JavaSparkContext.scala	/^  def accumulator(initialValue: Double): Accumulator[java.lang.Double] =$/;"	m
accumulator	api/java/JavaSparkContext.scala	/^  def accumulator(initialValue: Int): Accumulator[java.lang.Integer] = intAccumulator(initialValue)$/;"	m
accumulator	api/java/JavaSparkContext.scala	/^  def accumulator[T](initialValue: T, accumulatorParam: AccumulatorParam[T]): Accumulator[T] =$/;"	m
ackId	network/BufferMessage.scala	/^class BufferMessage(id_ : Int, val buffers: ArrayBuffer[ByteBuffer], var ackId: Int)$/;"	v
ackMessage	network/ConnectionManager.scala	/^          val ackMessage = if (onReceiveCallback != null) {$/;"	V
ackMessage	network/ConnectionManager.scala	/^    var ackMessage: Option[Message] = None$/;"	v
acked	network/ConnectionManager.scala	/^    var acked = false$/;"	v
activeApps	deploy/master/ui/IndexPage.scala	/^    val activeApps = state.activeApps.sortBy(_.startTime).reverse$/;"	V
activeAppsTable	deploy/master/ui/IndexPage.scala	/^    val activeAppsTable = UIUtils.listingTable(appHeaders, appRow, activeApps)$/;"	V
activeExecutorIds	scheduler/cluster/ClusterScheduler.scala	/^  val activeExecutorIds = new HashSet[String]$/;"	V
activeJobs	scheduler/DAGScheduler.scala	/^  val activeJobs = new HashSet[ActiveJob]$/;"	V
activeMasterUrl	deploy/client/Client.scala	/^  var activeMasterUrl: String = null$/;"	v
activeMasterUrl	deploy/worker/Worker.scala	/^  var activeMasterUrl: String = ""$/;"	v
activeMasterWebUiUrl	deploy/worker/Worker.scala	/^  var activeMasterWebUiUrl : String = ""$/;"	v
activeStages	ui/jobs/IndexPage.scala	/^      val activeStages = listener.activeStages.toSeq$/;"	V
activeStages	ui/jobs/JobProgressListener.scala	/^  val activeStages = HashSet[StageInfo]()$/;"	V
activeStages	ui/jobs/PoolPage.scala	/^      val activeStages = poolToActiveStages.get(poolName).toSeq.flatten$/;"	V
activeStages	ui/jobs/PoolTable.scala	/^    val activeStages = poolToActiveStages.get(p.name) match {$/;"	V
activeStagesTable	ui/jobs/IndexPage.scala	/^      val activeStagesTable = new StageTable(activeStages.sortBy(_.submissionTime).reverse, parent)$/;"	V
activeStagesTable	ui/jobs/PoolPage.scala	/^      val activeStagesTable = new StageTable(activeStages.sortBy(_.submissionTime).reverse, parent)$/;"	V
activeTaskSets	scheduler/cluster/ClusterScheduler.scala	/^  val activeTaskSets = new HashMap[String, ClusterTaskSetManager]$/;"	V
activeTaskSets	scheduler/local/LocalScheduler.scala	/^  val activeTaskSets = new HashMap[String, LocalTaskSetManager]$/;"	V
activeTasks	ui/exec/ExecutorsUI.scala	/^      val activeTasks = executorToTasksActive.getOrElseUpdate(eid, new HashSet[TaskInfo]())$/;"	V
activeTasks	ui/exec/ExecutorsUI.scala	/^    val activeTasks = listener.executorToTasksActive.getOrElse(execId, HashSet.empty[Long]).size$/;"	V
activeTime	ui/jobs/IndexPage.scala	/^      var activeTime = 0L$/;"	v
activeTime	ui/jobs/StagePage.scala	/^      var activeTime = 0L$/;"	v
actor	deploy/client/Client.scala	/^        val actor = context.actorFor(Master.toAkkaUrl(masterUrl))$/;"	V
actor	deploy/client/Client.scala	/^  var actor: ActorRef = null$/;"	v
actor	deploy/master/Master.scala	/^    val actor = actorSystem.actorOf(Props(new Master(host, boundPort, webUiPort)), name = actorName)$/;"	V
actor	deploy/master/WorkerInfo.scala	/^    val actor: ActorRef,$/;"	V
actor	deploy/worker/Worker.scala	/^      val actor = context.actorFor(Master.toAkkaUrl(masterUrl))$/;"	V
actor	deploy/worker/Worker.scala	/^    val actor = actorSystem.actorOf(Props(new Worker(host, boundPort, webUiPort, cores, memory,$/;"	V
actor	executor/CoarseGrainedExecutorBackend.scala	/^    val actor = actorSystem.actorOf($/;"	V
actorName	deploy/master/Master.scala	/^  private val actorName = "Master"$/;"	V
actorSystem	SparkEnv.scala	/^    val actorSystem: ActorSystem,$/;"	V
actorSystem	storage/ThreadingTest.scala	/^    val actorSystem = ActorSystem("test")$/;"	V
actorSystem	util/AkkaUtils.scala	/^    val actorSystem = ActorSystem(name, akkaConf)$/;"	V
actorToApp	deploy/master/Master.scala	/^  val actorToApp = new HashMap[ActorRef, ApplicationInfo]$/;"	V
actorToExecutorId	scheduler/cluster/CoarseGrainedSchedulerBackend.scala	/^    private val actorToExecutorId = new HashMap[ActorRef, String]$/;"	V
actorToWorker	deploy/master/Master.scala	/^  val actorToWorker = new HashMap[ActorRef, WorkerInfo]$/;"	V
add	Accumulators.scala	/^  def add(term: T) { value_ = param.addAccumulator(value_, term) }$/;"	m
add	Accumulators.scala	/^  def add(values: Map[Long, Any]): Unit = synchronized {$/;"	m
add	util/Vector.scala	/^  def add(other: Vector) = this + other$/;"	m
add	util/collection/OpenHashSet.scala	/^  def add(k: T) {$/;"	m
addAccumulator	Accumulators.scala	/^  def addAccumulator(growable: R, elem: T): R = {$/;"	m
addAccumulator	Accumulators.scala	/^  def addAccumulator(r: R, t: T): R$/;"	m
addAccumulator	Accumulators.scala	/^  def addAccumulator(t1: T, t2: T): T = {$/;"	m
addApplication	deploy/master/PersistenceEngine.scala	/^  def addApplication(app: ApplicationInfo)$/;"	m
addConnection	network/ConnectionManager.scala	/^  def addConnection(connection: Connection) {$/;"	m
addCredentials	deploy/SparkHadoopUtil.scala	/^  def addCredentials(conf: JobConf) {}$/;"	m
addExecutor	deploy/master/ApplicationInfo.scala	/^  def addExecutor(worker: WorkerInfo, cores: Int, useID: Option[Int] = None): ExecutorInfo = {$/;"	m
addExecutor	deploy/master/WorkerInfo.scala	/^  def addExecutor(exec: ExecutorInfo) {$/;"	m
addFile	HttpFileServer.scala	/^  def addFile(file: File) : String = {$/;"	m
addFile	SparkContext.scala	/^  def addFile(path: String) {$/;"	m
addFile	api/java/JavaSparkContext.scala	/^  def addFile(path: String) {$/;"	m
addFileToDir	HttpFileServer.scala	/^  def addFileToDir(file: File, dir: File) : String = {$/;"	m
addInPlace	Accumulators.scala	/^  def addInPlace(r1: R, r2: R): R$/;"	m
addInPlace	Accumulators.scala	/^  def addInPlace(t1: R, t2: R): R = {$/;"	m
addInPlace	SparkContext.scala	/^    def addInPlace(t1: Double, t2: Double): Double = t1 + t2$/;"	m
addInPlace	SparkContext.scala	/^    def addInPlace(t1: Float, t2: Float) = t1 + t2$/;"	m
addInPlace	SparkContext.scala	/^    def addInPlace(t1: Int, t2: Int): Int = t1 + t2$/;"	m
addInPlace	SparkContext.scala	/^    def addInPlace(t1: Long, t2: Long) = t1 + t2$/;"	m
addInPlace	util/Vector.scala	/^    def addInPlace(t1: Vector, t2: Vector) = t1 + t2$/;"	m
addInPlace	util/Vector.scala	/^  def addInPlace(other: Vector) = this +=other$/;"	m
addJar	HttpFileServer.scala	/^  def addJar(file: File) : String = {$/;"	m
addJar	SparkContext.scala	/^  def addJar(path: String) {$/;"	m
addJar	api/java/JavaSparkContext.scala	/^  def addJar(path: String) {$/;"	m
addListener	scheduler/SparkListenerBus.scala	/^  def addListener(listener: SparkListener) {$/;"	m
addMasters	deploy/FaultToleranceTest.scala	/^  def addMasters(num: Int) {$/;"	m
addMessage	network/Connection.scala	/^    def addMessage(message: Message) {$/;"	m
addOnCompleteCallback	TaskContext.scala	/^  def addOnCompleteCallback(f: () => Unit) {$/;"	m
addOutputLoc	scheduler/Stage.scala	/^  def addOutputLoc(partition: Int, status: MapStatus) {$/;"	m
addPartToPGroup	rdd/CoalescedRDD.scala	/^  def addPartToPGroup(part: Partition, pgroup: PartitionGroup): Boolean = {$/;"	m
addRunningTask	scheduler/cluster/ClusterTaskSetManager.scala	/^  def addRunningTask(tid: Long) {$/;"	m
addSchedulable	scheduler/Schedulable.scala	/^  def addSchedulable(schedulable: Schedulable): Unit$/;"	m
addSparkListener	SparkContext.scala	/^  def addSparkListener(listener: SparkListener) {$/;"	m
addSparkListener	scheduler/DAGScheduler.scala	/^  def addSparkListener(listener: SparkListener) {$/;"	m
addTaskSetManager	scheduler/SchedulableBuilder.scala	/^  def addTaskSetManager(manager: Schedulable, properties: Properties)$/;"	m
addTo	scheduler/cluster/ClusterTaskSetManager.scala	/^    def addTo(list: ArrayBuffer[Int]) {$/;"	m
addWithoutResize	util/collection/OpenHashSet.scala	/^  def addWithoutResize(k: T): Int = putInto(_bitset, _data, k)$/;"	m
addWorker	deploy/master/PersistenceEngine.scala	/^  def addWorker(worker: WorkerInfo)$/;"	m
addWorkers	deploy/FaultToleranceTest.scala	/^  def addWorkers(num: Int) {$/;"	m
addedFiles	SparkContext.scala	/^  private[spark] val addedFiles = HashMap[String, Long]()$/;"	V
addedFiles	ui/env/EnvironmentUI.scala	/^    val addedFiles = sc.addedFiles.iterator.toSeq.map{case (path, time) => (path, "Added By User")}$/;"	V
addedJars	SparkContext.scala	/^  private[spark] val addedJars = HashMap[String, Long]()$/;"	V
addedJars	ui/env/EnvironmentUI.scala	/^    val addedJars = sc.addedJars.iterator.toSeq.map{case (path, time) => (path, "Added By User")}$/;"	V
address	BlockStoreShuffleFetcher.scala	/^              val address = statuses(mapId.toInt)._1$/;"	V
address	network/Connection.scala	/^class SendingConnection(val address: InetSocketAddress, selector_ : Selector,$/;"	V
address	network/MessageChunkHeader.scala	/^    val address: InetSocketAddress) {$/;"	V
address	storage/BlockFetcherIterator.scala	/^  class FetchRequest(val address: BlockManagerId, val blocks: Seq[(BlockId, Long)]) {$/;"	V
address	util/Utils.scala	/^      val address = InetAddress.getLocalHost$/;"	V
addressToApp	deploy/master/Master.scala	/^  val addressToApp = new HashMap[Address, ApplicationInfo]$/;"	V
addressToExecutorId	scheduler/cluster/CoarseGrainedSchedulerBackend.scala	/^    private val addressToExecutorId = new HashMap[Address, String]$/;"	V
addressToWorker	deploy/master/Master.scala	/^  val addressToWorker = new HashMap[Address, WorkerInfo]$/;"	V
aggregate	api/java/JavaRDDLike.scala	/^  def aggregate[U](zeroValue: U)(seqOp: JFunction2[U, T, U],$/;"	m
aggregate	rdd/RDD.scala	/^  def aggregate[U: ClassManifest](zeroValue: U)(seqOp: (U, T) => U, combOp: (U, U) => U): U = {$/;"	m
aggregatePartition	rdd/RDD.scala	/^    val aggregatePartition = (it: Iterator[T]) => it.aggregate(zeroValue)(cleanSeqOp, cleanCombOp)$/;"	V
aggregator	rdd/PairRDDFunctions.scala	/^    val aggregator = new Aggregator[K, V, C](createCombiner, mergeValue, mergeCombiners)$/;"	V
akka.actor.Actor	storage/BlockManagerSlaveActor.scala	/^import akka.actor.Actor$/;"	i
akka.actor.ActorRef	deploy/master/ApplicationInfo.scala	/^import akka.actor.ActorRef$/;"	i
akka.actor.ActorRef	deploy/master/WorkerInfo.scala	/^import akka.actor.ActorRef$/;"	i
akka.actor.ActorRef	deploy/master/ZooKeeperLeaderElectionAgent.scala	/^import akka.actor.ActorRef$/;"	i
akka.actor.ActorRef	deploy/worker/ExecutorRunner.scala	/^import akka.actor.ActorRef$/;"	i
akka.actor.ActorRef	storage/BlockManagerMaster.scala	/^import akka.actor.ActorRef$/;"	i
akka.actor.ActorRef	storage/BlockManagerMessages.scala	/^import akka.actor.ActorRef$/;"	i
akka.actor.ActorSystem	deploy/LocalSparkCluster.scala	/^import akka.actor.ActorSystem$/;"	i
akka.actor.Terminated	deploy/client/Client.scala	/^import akka.actor.Terminated$/;"	i
akka.actor.Terminated	deploy/master/Master.scala	/^import akka.actor.Terminated$/;"	i
akka.actor._	MapOutputTracker.scala	/^import akka.actor._$/;"	i
akka.actor._	deploy/client/Client.scala	/^import akka.actor._$/;"	i
akka.actor._	deploy/master/Master.scala	/^import akka.actor._$/;"	i
akka.actor._	deploy/worker/Worker.scala	/^import akka.actor._$/;"	i
akka.actor._	scheduler/cluster/CoarseGrainedSchedulerBackend.scala	/^import akka.actor._$/;"	i
akka.actor._	scheduler/local/LocalScheduler.scala	/^import akka.actor._$/;"	i
akka.actor._	storage/ThreadingTest.scala	/^import akka.actor._$/;"	i
akka.actor.{Actor, ActorRef, Cancellable}	storage/BlockManagerMasterActor.scala	/^import akka.actor.{Actor, ActorRef, Cancellable}$/;"	i
akka.actor.{Actor, ActorRef, Props, ActorSystemImpl, ActorSystem}	SparkEnv.scala	/^import akka.actor.{Actor, ActorRef, Props, ActorSystemImpl, ActorSystem}$/;"	i
akka.actor.{Actor, ActorRef}	deploy/master/LeaderElectionAgent.scala	/^import akka.actor.{Actor, ActorRef}$/;"	i
akka.actor.{ActorRef, Actor, Props, Terminated}	executor/CoarseGrainedExecutorBackend.scala	/^import akka.actor.{ActorRef, Actor, Props, Terminated}$/;"	i
akka.actor.{ActorSystem, Cancellable, Props}	storage/BlockManager.scala	/^import akka.actor.{ActorSystem, Cancellable, Props}$/;"	i
akka.actor.{ActorSystem, ExtendedActorSystem}	util/AkkaUtils.scala	/^import akka.actor.{ActorSystem, ExtendedActorSystem}$/;"	i
akka.dispatch.Await	deploy/client/Client.scala	/^import akka.dispatch.Await$/;"	i
akka.dispatch.Await	deploy/master/Master.scala	/^import akka.dispatch.Await$/;"	i
akka.dispatch.Await	deploy/master/ui/ApplicationPage.scala	/^import akka.dispatch.Await$/;"	i
akka.dispatch.Await	deploy/master/ui/IndexPage.scala	/^import akka.dispatch.Await$/;"	i
akka.dispatch.Await	deploy/worker/ui/IndexPage.scala	/^import akka.dispatch.Await$/;"	i
akka.dispatch.Await	network/ConnectionManagerTest.scala	/^import akka.dispatch.Await$/;"	i
akka.dispatch.Await	scheduler/cluster/CoarseGrainedSchedulerBackend.scala	/^import akka.dispatch.Await$/;"	i
akka.dispatch.Future	storage/BlockManagerMasterActor.scala	/^import akka.dispatch.Future$/;"	i
akka.dispatch._	MapOutputTracker.scala	/^import akka.dispatch._$/;"	i
akka.dispatch.{Await, Future}	storage/BlockManager.scala	/^import akka.dispatch.{Await, Future}$/;"	i
akka.dispatch.{Await, Future}	storage/BlockManagerMaster.scala	/^import akka.dispatch.{Await, Future}$/;"	i
akka.dispatch.{Await, Promise, ExecutionContext, Future}	network/ConnectionManager.scala	/^import akka.dispatch.{Await, Promise, ExecutionContext, Future}$/;"	i
akka.pattern.ask	MapOutputTracker.scala	/^import akka.pattern.ask$/;"	i
akka.pattern.ask	deploy/client/Client.scala	/^import akka.pattern.ask$/;"	i
akka.pattern.ask	deploy/master/Master.scala	/^import akka.pattern.ask$/;"	i
akka.pattern.ask	deploy/master/ui/ApplicationPage.scala	/^import akka.pattern.ask$/;"	i
akka.pattern.ask	deploy/master/ui/IndexPage.scala	/^import akka.pattern.ask$/;"	i
akka.pattern.ask	deploy/worker/ui/IndexPage.scala	/^import akka.pattern.ask$/;"	i
akka.pattern.ask	scheduler/cluster/CoarseGrainedSchedulerBackend.scala	/^import akka.pattern.ask$/;"	i
akka.pattern.ask	storage/BlockManagerMaster.scala	/^import akka.pattern.ask$/;"	i
akka.pattern.ask	storage/BlockManagerMasterActor.scala	/^import akka.pattern.ask$/;"	i
akka.remote.RemoteActorRefProvider	SparkEnv.scala	/^import akka.remote.RemoteActorRefProvider$/;"	i
akka.remote.RemoteActorRefProvider	util/AkkaUtils.scala	/^import akka.remote.RemoteActorRefProvider$/;"	i
akka.remote.RemoteClientDisconnected	deploy/client/Client.scala	/^import akka.remote.RemoteClientDisconnected$/;"	i
akka.remote.RemoteClientLifeCycleEvent	deploy/client/Client.scala	/^import akka.remote.RemoteClientLifeCycleEvent$/;"	i
akka.remote.RemoteClientShutdown	deploy/client/Client.scala	/^import akka.remote.RemoteClientShutdown$/;"	i
akka.remote._	MapOutputTracker.scala	/^import akka.remote._$/;"	i
akka.remote.{RemoteClientLifeCycleEvent, RemoteClientDisconnected, RemoteClientShutdown}	deploy/master/Master.scala	/^import akka.remote.{RemoteClientLifeCycleEvent, RemoteClientDisconnected, RemoteClientShutdown}$/;"	i
akka.remote.{RemoteClientLifeCycleEvent, RemoteClientShutdown, RemoteClientDisconnected}	deploy/worker/Worker.scala	/^import akka.remote.{RemoteClientLifeCycleEvent, RemoteClientShutdown, RemoteClientDisconnected}$/;"	i
akka.remote.{RemoteClientLifeCycleEvent, RemoteClientShutdown, RemoteClientDisconnected}	executor/CoarseGrainedExecutorBackend.scala	/^import akka.remote.{RemoteClientLifeCycleEvent, RemoteClientShutdown, RemoteClientDisconnected}$/;"	i
akka.remote.{RemoteClientShutdown, RemoteClientDisconnected, RemoteClientLifeCycleEvent}	scheduler/cluster/CoarseGrainedSchedulerBackend.scala	/^import akka.remote.{RemoteClientShutdown, RemoteClientDisconnected, RemoteClientLifeCycleEvent}$/;"	i
akka.serialization.Serialization	deploy/master/FileSystemPersistenceEngine.scala	/^import akka.serialization.Serialization$/;"	i
akka.serialization.Serialization	deploy/master/ZooKeeperPersistenceEngine.scala	/^import akka.serialization.Serialization$/;"	i
akka.serialization.SerializationExtension	deploy/master/Master.scala	/^import akka.serialization.SerializationExtension$/;"	i
akka.util.Duration	MapOutputTracker.scala	/^import akka.util.Duration$/;"	i
akka.util.Duration	deploy/client/Client.scala	/^import akka.util.Duration$/;"	i
akka.util.Duration	deploy/master/ui/MasterWebUI.scala	/^import akka.util.Duration$/;"	i
akka.util.Duration	network/ConnectionManager.scala	/^import akka.util.Duration$/;"	i
akka.util.Duration	scheduler/cluster/CoarseGrainedSchedulerBackend.scala	/^import akka.util.Duration$/;"	i
akka.util.Duration	storage/BlockManager.scala	/^import akka.util.Duration$/;"	i
akka.util.Duration	storage/BlockManagerMaster.scala	/^import akka.util.Duration$/;"	i
akka.util.Duration	storage/BlockManagerMasterActor.scala	/^import akka.util.Duration$/;"	i
akka.util.Duration	ui/jobs/JobProgressUI.scala	/^import akka.util.Duration$/;"	i
akka.util.Duration	ui/storage/BlockManagerUI.scala	/^import akka.util.Duration$/;"	i
akka.util.duration._	deploy/client/Client.scala	/^import akka.util.duration._$/;"	i
akka.util.duration._	deploy/master/Master.scala	/^import akka.util.duration._$/;"	i
akka.util.duration._	deploy/master/ui/ApplicationPage.scala	/^import akka.util.duration._$/;"	i
akka.util.duration._	deploy/master/ui/IndexPage.scala	/^import akka.util.duration._$/;"	i
akka.util.duration._	deploy/worker/Worker.scala	/^import akka.util.duration._$/;"	i
akka.util.duration._	deploy/worker/ui/IndexPage.scala	/^import akka.util.duration._$/;"	i
akka.util.duration._	network/ConnectionManager.scala	/^import akka.util.duration._$/;"	i
akka.util.duration._	network/ConnectionManagerTest.scala	/^import akka.util.duration._$/;"	i
akka.util.duration._	scheduler/cluster/CoarseGrainedSchedulerBackend.scala	/^import akka.util.duration._$/;"	i
akka.util.duration._	storage/BlockManager.scala	/^import akka.util.duration._$/;"	i
akka.util.duration._	storage/BlockManagerMasterActor.scala	/^import akka.util.duration._$/;"	i
akka.util.duration._	util/AkkaUtils.scala	/^import akka.util.duration._$/;"	i
akka.util.{Duration, Timeout}	deploy/master/Master.scala	/^import akka.util.{Duration, Timeout}$/;"	i
akka.util.{Duration, Timeout}	deploy/worker/ui/WorkerWebUI.scala	/^import akka.util.{Duration, Timeout}$/;"	i
akkaBatchSize	util/AkkaUtils.scala	/^    val akkaBatchSize = System.getProperty("spark.akka.batchSize", "15").toInt$/;"	V
akkaConf	util/AkkaUtils.scala	/^    val akkaConf = ConfigFactory.parseString("""$/;"	V
akkaFrameSize	executor/Executor.scala	/^  private val akkaFrameSize = {$/;"	V
akkaFrameSize	util/AkkaUtils.scala	/^    val akkaFrameSize = System.getProperty("spark.akka.frameSize", "10").toInt$/;"	V
akkaThreads	util/AkkaUtils.scala	/^    val akkaThreads = System.getProperty("spark.akka.threads", "4").toInt$/;"	V
akkaTimeout	storage/BlockManagerMasterActor.scala	/^  val akkaTimeout = Duration.create($/;"	V
akkaTimeout	util/AkkaUtils.scala	/^    val akkaTimeout = System.getProperty("spark.akka.timeout", "60").toInt$/;"	V
akkaWriteTimeout	util/AkkaUtils.scala	/^    val akkaWriteTimeout = System.getProperty("spark.akka.writeTimeout", "30").toInt$/;"	V
allFileGroups	storage/ShuffleBlockManager.scala	/^    val allFileGroups = new ConcurrentLinkedQueue[ShuffleFileGroup]()$/;"	V
allHandlers	ui/SparkUI.scala	/^  val allHandlers = storage.getHandlers ++ jobs.getHandlers ++ env.getHandlers ++$/;"	V
allKeys	network/ConnectionManager.scala	/^              val allKeys = selector.keys().iterator()$/;"	V
allPendingTasks	scheduler/cluster/ClusterTaskSetManager.scala	/^  val allPendingTasks = new ArrayBuffer[Int]$/;"	V
allowedLocality	scheduler/cluster/ClusterTaskSetManager.scala	/^      var allowedLocality = getAllowedLocalityLevel(curTime)$/;"	v
alreadyDead	deploy/client/Client.scala	/^    var alreadyDead = false  \/\/ To avoid calling listener.dead() multiple times$/;"	v
alreadyDisconnected	deploy/client/Client.scala	/^    var alreadyDisconnected = false  \/\/ To avoid calling listener.disconnected() multiple times$/;"	v
alreadyPicked	broadcast/BitTorrentBroadcast.scala	/^            var alreadyPicked = new BitSet(listOfSources.size)$/;"	v
amountRead	util/SerializableBuffer.scala	/^    var amountRead = 0$/;"	v
amountToGet	util/ByteBufferInputStream.scala	/^      val amountToGet = math.min(buffer.remaining(), length)$/;"	V
amountToSkip	util/ByteBufferInputStream.scala	/^      val amountToSkip = math.min(bytes, buffer.remaining).toInt$/;"	V
ans	util/Vector.scala	/^    var ans = 0.0$/;"	v
anyToWritable	SparkContext.scala	/^    def anyToWritable[U <% Writable](u: U): Writable = u$/;"	m
anyToWritable	rdd/SequenceFileRDDFunctions.scala	/^    def anyToWritable[U <% Writable](u: U): Writable = u$/;"	m
app	deploy/master/Master.scala	/^            val app = idToApp.get(exec.appId).get$/;"	V
app	deploy/master/Master.scala	/^        val app = createApplication(description, sender)$/;"	V
app	deploy/master/ui/ApplicationPage.scala	/^    val app = state.activeApps.find(_.id == appId).getOrElse({$/;"	V
appAddress	deploy/master/Master.scala	/^    val appAddress = app.driver.path.address$/;"	V
appDesc	deploy/worker/ExecutorRunner.scala	/^    val appDesc: ApplicationDescription,$/;"	V
appDesc	scheduler/cluster/SparkDeploySchedulerBackend.scala	/^    val appDesc = new ApplicationDescription(appName, maxCores, executorMemory, command, sparkHome,$/;"	V
appFile	deploy/master/FileSystemPersistenceEngine.scala	/^    val appFile = new File(dir + File.separator + "app_" + app.id)$/;"	V
appFiles	deploy/master/FileSystemPersistenceEngine.scala	/^    val appFiles = sortedFiles.filter(_.getName.startsWith("app_"))$/;"	V
appFiles	deploy/master/ZooKeeperPersistenceEngine.scala	/^    val appFiles = sortedFiles.filter(_.startsWith("app_"))$/;"	V
appHeaders	deploy/master/ui/IndexPage.scala	/^    val appHeaders = Seq("ID", "Name", "Cores", "Memory per Node", "Submitted Time", "User",$/;"	V
appId	deploy/ExecutorDescription.scala	/^    val appId: String,$/;"	V
appId	deploy/client/Client.scala	/^  var appId: String = null$/;"	v
appId	deploy/master/Master.scala	/^    val appId = "app-%s-%04d".format(DATE_FORMAT.format(submitDate), nextAppNumber)$/;"	V
appId	deploy/master/ui/ApplicationPage.scala	/^    val appId = request.getParameter("appId")$/;"	V
appId	deploy/worker/ExecutorRunner.scala	/^    val appId: String,$/;"	V
appId	deploy/worker/ui/WorkerWebUI.scala	/^    val appId = request.getParameter("appId")$/;"	V
appInfo	deploy/master/Master.scala	/^            val appInfo = idToApp(appId)$/;"	V
appName	SparkContext.scala	/^    val appName: String,$/;"	V
appName	ui/UIWorkloadGenerator.scala	/^    val appName = "Spark UI Tester"$/;"	V
appRow	deploy/master/ui/IndexPage.scala	/^  def appRow(app: ApplicationInfo): Seq[Node] = {$/;"	m
appSource	deploy/master/ApplicationInfo.scala	/^  @transient var appSource: ApplicationSource = _$/;"	v
appUiUrl	deploy/ApplicationDescription.scala	/^    val appUiUrl: String)$/;"	V
appUiUrl	deploy/master/ApplicationInfo.scala	/^    val appUiUrl: String)$/;"	V
application	deploy/master/ApplicationSource.scala	/^class ApplicationSource(val application: ApplicationInfo) extends Source {$/;"	V
application	deploy/master/ExecutorInfo.scala	/^    val application: ApplicationInfo,$/;"	V
applicationMetricsSystem	deploy/master/Master.scala	/^  val applicationMetricsSystem = MetricsSystem.createMetricsSystem("applications")$/;"	V
applicationPage	deploy/master/ui/MasterWebUI.scala	/^  val applicationPage = new ApplicationPage(this)$/;"	V
apply	scheduler/SparkListener.scala	/^  def apply(totalTime: Long, metrics: TaskMetrics): RuntimePercentage = {$/;"	m
apply	scheduler/TaskLocation.scala	/^  def apply(host: String) = new TaskLocation(host, None)$/;"	m
apply	scheduler/TaskLocation.scala	/^  def apply(host: String, executorId: String) = new TaskLocation(host, Some(executorId))$/;"	m
apply	scheduler/cluster/CoarseGrainedClusterMessage.scala	/^    def apply(executorId: String, taskId: Long, state: TaskState, data: ByteBuffer)$/;"	m
apply	storage/BlockId.scala	/^  def apply(id: String) = id match {$/;"	m
apply	storage/BlockManagerId.scala	/^  def apply(execId: String, host: String, port: Int, nettyPort: Int) =$/;"	m
apply	storage/BlockManagerId.scala	/^  def apply(in: ObjectInput) = {$/;"	m
apply	storage/BlockManagerMessages.scala	/^    def apply(blockManagerId: BlockManagerId,$/;"	m
apply	storage/BlockMessageArray.scala	/^  def apply(i: Int) = blockMessages(i) $/;"	m
apply	storage/ShuffleBlockManager.scala	/^    def apply(bucketId: Int) = files(bucketId)$/;"	m
apply	storage/StorageLevel.scala	/^  def apply(flags: Int, replication: Int) =$/;"	m
apply	storage/StorageLevel.scala	/^  def apply(in: ObjectInput) = {$/;"	m
apply	storage/StorageLevel.scala	/^  def apply(useDisk: Boolean, useMemory: Boolean, deserialized: Boolean, replication: Int = 1) =$/;"	m
apply	util/AppendOnlyMap.scala	/^  def apply(key: K): V = {$/;"	m
apply	util/CompletionIterator.scala	/^  def apply[A, I <: Iterator[A]](sub: I, completionFunction: => Unit) : CompletionIterator[A,I] = {$/;"	m
apply	util/Distribution.scala	/^  def apply(data: Traversable[Double]): Option[Distribution] = {$/;"	m
apply	util/StatCounter.scala	/^  def apply(values: Double*) = new StatCounter(values)$/;"	m
apply	util/StatCounter.scala	/^  def apply(values: TraversableOnce[Double]) = new StatCounter(values)$/;"	m
apply	util/Vector.scala	/^  def apply(elements: Array[Double]) = new Vector(elements)$/;"	m
apply	util/Vector.scala	/^  def apply(elements: Double*) = new Vector(elements.toArray)$/;"	m
apply	util/Vector.scala	/^  def apply(index: Int) = elements(index)$/;"	m
apply	util/Vector.scala	/^  def apply(length: Int, initializer: Int => Double): Vector = {$/;"	m
apply	util/collection/OpenHashMap.scala	/^  def apply(k: K): V = {$/;"	m
apply	util/collection/PrimitiveKeyOpenHashMap.scala	/^  def apply(k: K): V = {$/;"	m
apply	util/collection/PrimitiveVector.scala	/^  def apply(index: Int): V = {$/;"	m
apps	deploy/master/FileSystemPersistenceEngine.scala	/^    val apps = appFiles.map(deserializeFromFile[ApplicationInfo])$/;"	V
apps	deploy/master/Master.scala	/^  val apps = new HashSet[ApplicationInfo]$/;"	V
apps	deploy/master/ZooKeeperPersistenceEngine.scala	/^    val apps = appFiles.map(deserializeFromFile[ApplicationInfo])$/;"	V
argTypes	util/ClosureCleaner.scala	/^        val argTypes = Type.getArgumentTypes(desc)$/;"	V
args	deploy/master/Master.scala	/^    val args = new MasterArguments(argStrings)$/;"	V
args	deploy/worker/Worker.scala	/^    val args = new WorkerArguments(argStrings)$/;"	V
args	scheduler/cluster/SparkDeploySchedulerBackend.scala	/^    val args = Seq(driverUrl, "{{EXECUTOR_ID}}", "{{HOSTNAME}}", "{{CORES}}")$/;"	V
arr	api/java/JavaRDDLike.scala	/^    val arr: java.util.Collection[T] = rdd.collect().toSeq$/;"	V
arr	api/java/JavaRDDLike.scala	/^    val arr: java.util.Collection[T] = rdd.take(num).toSeq$/;"	V
arr	api/java/JavaRDDLike.scala	/^    val arr: java.util.Collection[T] = rdd.takeSample(withReplacement, num, seed).toSeq$/;"	V
arr	api/java/JavaRDDLike.scala	/^    val arr: java.util.Collection[T] = topElems.toSeq$/;"	V
arr	api/python/PythonRDD.scala	/^      val arr = elem.asInstanceOf[Array[Byte]]$/;"	V
arr	rdd/CoalescedRDD.scala	/^  var arr = mutable.ArrayBuffer[Partition]()$/;"	v
arrSize	util/SizeEstimator.scala	/^    var arrSize: Long = alignSize(objectSize + INT_SIZE)$/;"	v
array	MapOutputTracker.scala	/^      var array = arrayOpt.get$/;"	v
array	MapOutputTracker.scala	/^    var array = mapStatuses(shuffleId)$/;"	v
array	rdd/CartesianRDD.scala	/^    val array = new Array[Partition](rdd1.partitions.size * rdd2.partitions.size)$/;"	V
array	rdd/CoGroupedRDD.scala	/^    val array = new Array[Partition](part.numPartitions)$/;"	V
array	rdd/HadoopRDD.scala	/^    val array = new Array[Partition](inputSplits.size)$/;"	V
array	rdd/ParallelCollectionRDD.scala	/^        val array = seq.toArray \/\/ To prevent O(n^2) operations for List etc$/;"	V
array	rdd/SubtractedRDD.scala	/^    val array = new Array[Partition](part.numPartitions)$/;"	V
array	rdd/UnionRDD.scala	/^    val array = new Array[Partition](rdds.map(_.partitions.size).sum)$/;"	V
array	rdd/ZippedPartitionsRDD.scala	/^    val array = new Array[Partition](sizes(0))$/;"	V
array	rdd/ZippedRDD.scala	/^    val array = new Array[Partition](rdd1.partitions.size)$/;"	V
array	util/collection/PrimitiveVector.scala	/^  def array: Array[V] = _array$/;"	m
arrayOfBlocks	broadcast/BitTorrentBroadcast.scala	/^  @transient var arrayOfBlocks: Array[BroadcastBlock] = null$/;"	v
arrayOfBlocks	broadcast/TorrentBroadcast.scala	/^  @transient var arrayOfBlocks: Array[TorrentBlock] = null$/;"	v
arrayOfBlocks	broadcast/TreeBroadcast.scala	/^  @transient var arrayOfBlocks: Array[BroadcastBlock] = null$/;"	v
arrayOpt	MapOutputTracker.scala	/^    var arrayOpt = mapStatuses.get(shuffleId)$/;"	v
asIterator	serializer/Serializer.scala	/^  def asIterator: Iterator[Any] = new NextIterator[Any] {$/;"	m
asJavaPairRDD	api/python/PythonRDD.scala	/^  val asJavaPairRDD : JavaPairRDD[Long, Array[Byte]] = JavaPairRDD.fromRDD(this)$/;"	V
asJavaRDD	api/python/PythonRDD.scala	/^  val asJavaRDD : JavaRDD[Array[Byte]] = JavaRDD.fromRDD(this)$/;"	V
asRDDId	storage/BlockId.scala	/^  def asRDDId = if (isRDD) Some(asInstanceOf[RDDBlockId]) else None$/;"	m
ascending	Partitioner.scala	/^    private val ascending: Boolean = true) $/;"	V
askForBytes	storage/BlockManager.scala	/^              val askForBytes = level.replication > 1$/;"	V
askTracker	MapOutputTracker.scala	/^  def askTracker(message: Any): Any = {$/;"	m
assertTrue	deploy/FaultToleranceTest.scala	/^  def assertTrue(bool: Boolean, message: String = "") {$/;"	m
assertUsable	deploy/FaultToleranceTest.scala	/^  def assertUsable() = {$/;"	m
assertValidClusterState	deploy/FaultToleranceTest.scala	/^  def assertValidClusterState() = {$/;"	m
assigned	deploy/master/Master.scala	/^        val assigned = new Array[Int](numUsable) \/\/ Number of cores to give on each node$/;"	V
asyncReregisterLock	storage/BlockManager.scala	/^  val asyncReregisterLock = new Object$/;"	V
asyncReregisterTask	storage/BlockManager.scala	/^  var asyncReregisterTask: Future[Unit] = null$/;"	v
attempt	scheduler/TaskSet.scala	/^    val attempt: Int,$/;"	V
attemptID	SparkHadoopWriter.scala	/^  private var attemptID = 0$/;"	v
attemptId	TaskContext.scala	/^  val attemptId: Long,$/;"	V
attemptId	broadcast/TorrentBroadcast.scala	/^    var attemptId = 10$/;"	v
attemptId	rdd/NewHadoopRDD.scala	/^      val attemptId = newTaskAttemptID(jobtrackerId, id, true, split.index, 0)$/;"	V
attemptId	rdd/PairRDDFunctions.scala	/^      val attemptId = newTaskAttemptID(jobtrackerID, stageId, false, context.partitionId, attemptNumber)$/;"	V
attemptId	scheduler/local/LocalScheduler.scala	/^  val attemptId = new AtomicInteger$/;"	V
attemptNumber	rdd/PairRDDFunctions.scala	/^      val attemptNumber = (context.attemptId % Int.MaxValue).toInt$/;"	V
attempted	network/ConnectionManager.scala	/^    var attempted = false$/;"	v
attemptedTask	executor/Executor.scala	/^      var attemptedTask: Option[Task[Any]] = None$/;"	v
attempts	storage/BlockManagerMaster.scala	/^    var attempts = 0$/;"	v
attempts	util/Utils.scala	/^    var attempts = 0$/;"	v
attemptsLeft	deploy/master/SparkZooKeeperSession.scala	/^        val attemptsLeft = MAX_RECONNECT_ATTEMPTS - reconnectAttempts$/;"	V
availableCpus	scheduler/cluster/ClusterScheduler.scala	/^    val availableCpus = offers.map(o => o.cores).toArray$/;"	V
awaitResult	partial/ApproximateActionListener.scala	/^  def awaitResult(): PartialResult[R] = synchronized {$/;"	m
awaitResult	scheduler/JobWaiter.scala	/^  def awaitResult(): JobResult = synchronized {$/;"	m
awaitTime	network/ConnectionManagerTest.scala	/^    val awaitTime = (if (args.length > 5) args(5).toInt else 600 ).second$/;"	V
bMsg	storage/BlockMessage.scala	/^    val bMsg = B.toBufferMessage$/;"	V
backButton	deploy/worker/ui/WorkerWebUI.scala	/^    val backButton =$/;"	V
backend	SparkContext.scala	/^        val backend = if (coarseGrained) {$/;"	V
backend	SparkContext.scala	/^        val backend = new CoarseGrainedSchedulerBackend(scheduler, this.env.actorSystem)$/;"	V
backend	SparkContext.scala	/^        val backend = new SimrSchedulerBackend(scheduler, this, simrUrl)$/;"	V
backend	SparkContext.scala	/^        val backend = new SparkDeploySchedulerBackend(scheduler, this, masterUrls, appName)$/;"	V
backend	SparkContext.scala	/^        val backend = try {$/;"	V
backend	scheduler/cluster/ClusterScheduler.scala	/^  var backend: SchedulerBackend = null$/;"	v
bais	broadcast/MultiTracker.scala	/^    val bais = new ByteArrayInputStream(byteArray)$/;"	V
bais	broadcast/TorrentBroadcast.scala	/^    val bais = new ByteArrayInputStream(byteArray)$/;"	V
baos	broadcast/MultiTracker.scala	/^    val baos = new ByteArrayOutputStream$/;"	V
baos	util/ClosureCleaner.scala	/^    val baos = new ByteArrayOutputStream(128)$/;"	V
baseData	ui/UIWorkloadGenerator.scala	/^    val baseData = sc.makeRDD(1 to NUM_PARTITIONS * 10, NUM_PARTITIONS)$/;"	V
baseDir	HttpFileServer.scala	/^  var baseDir : File = null$/;"	v
basename	scheduler/cluster/mesos/CoarseMesosSchedulerBackend.scala	/^      val basename = uri.split('\/').last.split('.').head$/;"	V
basename	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^      val basename = uri.split('\/').last.split('.').head$/;"	V
basicSparkPage	ui/UIUtils.scala	/^  def basicSparkPage(content: => Seq[Node], title: String): Seq[Node] = {$/;"	m
bb	scheduler/TaskResult.scala	/^    val bb = objectSer.serialize(value)$/;"	V
bbval	util/Utils.scala	/^      val bbval = new Array[Byte](bb.remaining())$/;"	V
bcBlock	broadcast/BitTorrentBroadcast.scala	/^              val bcBlock = oisSource.readObject.asInstanceOf[BroadcastBlock]$/;"	V
bcBlock	broadcast/TreeBroadcast.scala	/^        val bcBlock = oisSource.readObject.asInstanceOf[BroadcastBlock]$/;"	V
bean	deploy/worker/WorkerArguments.scala	/^      val bean = ManagementFactory.getOperatingSystemMXBean()$/;"	V
bean	util/SizeEstimator.scala	/^      val bean = ManagementFactory.newPlatformMXBeanProxy(server, $/;"	V
beanClass	deploy/worker/WorkerArguments.scala	/^        val beanClass = Class.forName("com.ibm.lang.management.OperatingSystemMXBean")$/;"	V
beanClass	deploy/worker/WorkerArguments.scala	/^        val beanClass = Class.forName("com.sun.management.OperatingSystemMXBean")$/;"	V
becameUnavailable	scheduler/Stage.scala	/^    var becameUnavailable = false$/;"	v
beginRecovery	deploy/master/Master.scala	/^  def beginRecovery(storedApps: Seq[ApplicationInfo], storedWorkers: Seq[WorkerInfo]) {$/;"	m
bis	serializer/JavaSerializer.scala	/^    val bis = new ByteBufferInputStream(bytes)$/;"	V
bis	util/Utils.scala	/^    val bis = new ByteArrayInputStream(bytes)$/;"	V
bitmask	util/collection/BitSet.scala	/^    val bitmask = 1L << (index & 0x3f)   \/\/ mod 64 and shift$/;"	V
bitmask	util/collection/BitSet.scala	/^    val bitmask = 1L << (index & 0x3f)  \/\/ mod 64 and shift$/;"	V
blen	scheduler/TaskResult.scala	/^    val blen = in.readInt()$/;"	V
block	storage/ThreadingTest.scala	/^        val block = (1 to blockSize).map(_ => Random.nextInt())$/;"	V
blockFetcherItr	BlockStoreShuffleFetcher.scala	/^    val blockFetcherItr = blockManager.getMultiple(blocksByAddress, serializer)$/;"	V
blockFile	storage/ShuffleBlockManager.scala	/^          val blockFile = blockManager.diskBlockManager.getFile(blockId)$/;"	V
blockHeaders	ui/storage/RDDPage.scala	/^    val blockHeaders = Seq("Block Name", "Storage Level", "Size in Memory", "Size on Disk",$/;"	V
blockID	broadcast/MultiTracker.scala	/^    var blockID = 0$/;"	v
blockID	broadcast/TorrentBroadcast.scala	/^    var blockID = 0$/;"	v
blockId	BlockStoreShuffleFetcher.scala	/^      val blockId = blockPair._1$/;"	V
blockId	broadcast/BitTorrentBroadcast.scala	/^  def blockId = BroadcastBlockId(id)$/;"	m
blockId	broadcast/HttpBroadcast.scala	/^  def blockId = BroadcastBlockId(id)$/;"	m
blockId	broadcast/TreeBroadcast.scala	/^  def blockId = BroadcastBlockId(id)$/;"	m
blockId	executor/Executor.scala	/^            val blockId = TaskResultBlockId(taskId)$/;"	V
blockId	network/netty/FileHeader.scala	/^    val blockId = BlockId(idBuilder.toString())$/;"	V
blockId	network/netty/FileHeader.scala	/^  val blockId: BlockId) extends Logging {$/;"	V
blockId	network/netty/ShuffleCopier.scala	/^    val blockId = BlockId(args(2))$/;"	V
blockId	rdd/BlockRDD.scala	/^    val blockId = split.asInstanceOf[BlockRDDPartition].blockId$/;"	V
blockId	rdd/BlockRDD.scala	/^private[spark] class BlockRDDPartition(val blockId: BlockId, idx: Int) extends Partition {$/;"	V
blockId	storage/BlockFetcherIterator.scala	/^            val blockId = blockMessage.getId$/;"	V
blockId	storage/BlockFetcherIterator.scala	/^  class FetchResult(val blockId: BlockId, val size: Long, val deserialize: () => Iterator[Any]) {$/;"	V
blockId	storage/BlockManagerMasterActor.scala	/^      val blockId = iterator.next$/;"	V
blockId	storage/BlockManagerMessages.scala	/^      var blockId: BlockId,$/;"	v
blockId	storage/BlockMessage.scala	/^    val blockId = TestBlockId("ABC")$/;"	V
blockId	storage/BlockObjectWriter.scala	/^abstract class BlockObjectWriter(val blockId: BlockId) {$/;"	V
blockId	storage/MemoryStore.scala	/^          val blockId = pair.getKey$/;"	V
blockId	storage/ShuffleBlockManager.scala	/^          val blockId = ShuffleBlockId(shuffleId, mapId, bucketId)$/;"	V
blockId	storage/ThreadingTest.scala	/^        val blockId = TestBlockId("b-" + id + "-" + i)$/;"	V
blockIds	scheduler/DAGScheduler.scala	/^      val blockIds = rdd.partitions.indices.map(index=> RDDBlockId(rdd.id, index)).toArray[BlockId]$/;"	V
blockInfo	storage/BlockManager.scala	/^  private val blockInfo = new TimeStampedHashMap[BlockId, BlockInfo]$/;"	V
blockInfoInitThreads	storage/BlockInfo.scala	/^  private val blockInfoInitThreads = new ConcurrentHashMap[BlockInfo, Thread]$/;"	V
blockLocationPairs	storage/StorageUtils.scala	/^    val blockLocationPairs = storageStatusList$/;"	V
blockLocations	storage/BlockManager.scala	/^    val blockLocations: Seq[Seq[BlockManagerId]] = if (env != null) {$/;"	V
blockLocations	storage/BlockManagerMasterActor.scala	/^  private val blockLocations = new JHashMap[BlockId, mutable.HashSet[BlockManagerId]]$/;"	V
blockLocations	ui/storage/RDDPage.scala	/^    val blockLocations = StorageUtils.blockLocationsFromStorageStatus(filteredStorageStatusList)$/;"	V
blockLocationsFromStorageStatus	storage/StorageUtils.scala	/^  def blockLocationsFromStorageStatus(storageStatusList: Seq[StorageStatus]) = {$/;"	m
blockManager	BlockStoreShuffleFetcher.scala	/^    val blockManager = SparkEnv.get.blockManager$/;"	V
blockManager	SparkEnv.scala	/^    val blockManager = new BlockManager(executorId, actorSystem, blockManagerMaster, serializer)$/;"	V
blockManager	SparkEnv.scala	/^    val blockManager: BlockManager,$/;"	V
blockManager	rdd/BlockRDD.scala	/^    val blockManager = SparkEnv.get.blockManager$/;"	V
blockManager	scheduler/ShuffleMapTask.scala	/^    val blockManager = SparkEnv.get.blockManager$/;"	V
blockManager	storage/BlockFetcherIterator.scala	/^      private val blockManager: BlockManager,$/;"	V
blockManager	storage/BlockManagerMasterActor.scala	/^        val blockManager = blockManagerInfo.get(blockManagerId)$/;"	V
blockManager	storage/BlockManagerSource.scala	/^private[spark] class BlockManagerSource(val blockManager: BlockManager, sc: SparkContext)$/;"	V
blockManager	storage/BlockManagerWorker.scala	/^    val blockManager = blockManagerWorker.blockManager$/;"	V
blockManager	storage/BlockManagerWorker.scala	/^private[spark] class BlockManagerWorker(val blockManager: BlockManager) extends Logging {$/;"	V
blockManager	storage/BlockStore.scala	/^abstract class BlockStore(val blockManager: BlockManager) extends Logging {$/;"	V
blockManager	storage/StoragePerfTester.scala	/^    val blockManager = sc.env.blockManager$/;"	V
blockManager	storage/ThreadingTest.scala	/^    val blockManager = new BlockManager($/;"	V
blockManager._	storage/BlockFetcherIterator.scala	/^    import blockManager._$/;"	i
blockManagerId	storage/BlockManager.scala	/^  val blockManagerId = BlockManagerId($/;"	V
blockManagerId	storage/BlockManagerMasterActor.scala	/^      val blockManagerId: BlockManagerId,$/;"	V
blockManagerId	storage/BlockManagerMessages.scala	/^      var blockManagerId: BlockManagerId,$/;"	v
blockManagerIdByExecutor	storage/BlockManagerMasterActor.scala	/^  private val blockManagerIdByExecutor = new mutable.HashMap[String, BlockManagerId]$/;"	V
blockManagerIdCache	storage/BlockManagerId.scala	/^  val blockManagerIdCache = new ConcurrentHashMap[BlockManagerId, BlockManagerId]()$/;"	V
blockManagerInfo	storage/BlockManagerMasterActor.scala	/^  private val blockManagerInfo =$/;"	V
blockManagerMaster	SparkEnv.scala	/^    val blockManagerMaster = new BlockManagerMaster(registerOrLookup($/;"	V
blockManagerMaster	storage/ThreadingTest.scala	/^    val blockManagerMaster = new BlockManagerMaster($/;"	V
blockManagerSource	SparkContext.scala	/^  private val blockManagerSource = new BlockManagerSource(SparkEnv.get.blockManager, this)$/;"	V
blockManagerWorker	storage/BlockManagerWorker.scala	/^  private var blockManagerWorker: BlockManagerWorker = null$/;"	v
blockManagers	storage/BlockManager.scala	/^    val blockManagers = new HashMap[BlockId, Seq[BlockManagerId]]$/;"	V
blockMessage	storage/BlockManagerWorker.scala	/^    val blockMessage = BlockMessage.fromGetBlock(msg)$/;"	V
blockMessage	storage/BlockManagerWorker.scala	/^    val blockMessage = BlockMessage.fromPutBlock(msg)$/;"	V
blockMessageArray	storage/BlockFetcherIterator.scala	/^          val blockMessageArray = BlockMessageArray.fromBufferMessage(bufferMessage)$/;"	V
blockMessageArray	storage/BlockFetcherIterator.scala	/^      val blockMessageArray = new BlockMessageArray(req.blocks.map {$/;"	V
blockMessageArray	storage/BlockManagerWorker.scala	/^    val blockMessageArray = new BlockMessageArray(blockMessage)$/;"	V
blockMessageArray	storage/BlockMessageArray.scala	/^    val blockMessageArray = new BlockMessageArray(blockMessages)$/;"	V
blockMessages	storage/BlockManagerWorker.scala	/^          val blockMessages = BlockMessageArray.fromBufferMessage(bufferMessage)$/;"	V
blockMessages	storage/BlockMessageArray.scala	/^    val blockMessages =$/;"	V
blockMessages	storage/BlockMessageArray.scala	/^class BlockMessageArray(var blockMessages: Seq[BlockMessage]) extends Seq[BlockMessage] with Logging {$/;"	v
blockNum	broadcast/MultiTracker.scala	/^    var blockNum = (byteArray.length \/ BlockSize)$/;"	v
blockNum	broadcast/TorrentBroadcast.scala	/^    var blockNum = (byteArray.length \/ BLOCK_SIZE)$/;"	v
blockOffsets	storage/ShuffleBlockManager.scala	/^      val blockOffsets = blockOffsetsByReducer(reducerId)$/;"	V
blockOffsetsByReducer	storage/ShuffleBlockManager.scala	/^    private val blockOffsetsByReducer = Array.fill[PrimitiveVector[Long]](files.length) {$/;"	V
blockOption	BlockStoreShuffleFetcher.scala	/^      val blockOption = blockPair._2$/;"	V
blockRow	ui/storage/RDDPage.scala	/^  def blockRow(row: (BlockId, BlockStatus, Seq[String])): Seq[Node] = {$/;"	m
blockSize	io/CompressionCodec.scala	/^    val blockSize = System.getProperty("spark.io.compression.snappy.block.size", "32768").toInt$/;"	V
blockSize	storage/ThreadingTest.scala	/^        val blockSize = Random.nextInt(1000)$/;"	V
blockStatus	storage/BlockManagerMasterActor.scala	/^        val blockStatus: BlockStatus = _blocks.get(blockId)$/;"	V
blockStatuses	ui/storage/RDDPage.scala	/^    val blockStatuses = filteredStorageStatusList.flatMap(_.blocks).toArray.$/;"	V
blockTable	ui/storage/RDDPage.scala	/^    val blockTable = listingTable(blockHeaders, blockRow, blocks)$/;"	V
blockToAskFor	broadcast/BitTorrentBroadcast.scala	/^        var blockToAskFor = -1$/;"	v
blockToSend	broadcast/BitTorrentBroadcast.scala	/^            var blockToSend = ois.readObject.asInstanceOf[Int]$/;"	v
blockWasRemoved	storage/BlockManager.scala	/^        val blockWasRemoved = memoryStore.remove(blockId)$/;"	V
blockifyObject	broadcast/MultiTracker.scala	/^  def blockifyObject[IN](obj: IN): VariableInfo = {$/;"	m
blockifyObject	broadcast/TorrentBroadcast.scala	/^  def blockifyObject[T](obj: T): TorrentInfo = {$/;"	m
blocks	storage/BlockManagerMasterActor.scala	/^    def blocks: JHashMap[BlockId, BlockStatus] = _blocks$/;"	m
blocks	storage/BlockManagerMasterActor.scala	/^    val blocks = blockLocations.keys.flatMap(_.asRDDId).filter(_.rddId == rddId)$/;"	V
blocks	ui/storage/RDDPage.scala	/^    val blocks = blockStatuses.map {$/;"	V
blocksByAddress	BlockStoreShuffleFetcher.scala	/^    val blocksByAddress: Seq[(BlockManagerId, Seq[(BlockId, Long)])] = splitsByAddress.toSeq.map {$/;"	V
blocksByAddress	storage/BlockFetcherIterator.scala	/^      val blocksByAddress: Seq[(BlockManagerId, Seq[(BlockId, Long)])],$/;"	V
blocksInRequestBitVector	broadcast/BitTorrentBroadcast.scala	/^    private var blocksInRequestBitVector = new BitSet(totalBlocks)$/;"	v
blocksToRemove	storage/BlockManager.scala	/^    val blocksToRemove = blockInfo.keys.flatMap(_.asRDDId).filter(_.rddId == rddId)$/;"	V
bms	storage/BlockManagerMasterActor.scala	/^      val bms: mutable.HashSet[BlockManagerId] = blockLocations.get(blockId)$/;"	V
booleanInt	storage/BlockMessage.scala	/^      val booleanInt = buffer.getInt()$/;"	V
boot	api/python/PythonRDD.scala	/^              val boot = bootTime - startTime$/;"	V
bootTime	api/python/PythonRDD.scala	/^              val bootTime = stream.readLong()$/;"	V
bos	serializer/JavaSerializer.scala	/^    val bos = new ByteArrayOutputStream()$/;"	V
bos	util/Utils.scala	/^    val bos = new ByteArrayOutputStream()$/;"	V
boundPort	deploy/master/ui/MasterWebUI.scala	/^  var boundPort: Option[Int] = None$/;"	v
boundPort	deploy/worker/ui/WorkerWebUI.scala	/^  var boundPort: Option[Int] = None$/;"	v
boundPort	ui/SparkUI.scala	/^  var boundPort: Option[Int] = None$/;"	v
boundPort	util/AkkaUtils.scala	/^    val boundPort = provider.asInstanceOf[RemoteActorRefProvider].transport.address.port.get$/;"	V
bounds	Partitioner.scala	/^        val bounds = new Array[K](partitions - 1)$/;"	V
broadcast	SparkContext.scala	/^  def broadcast[T](value: T) = env.broadcastManager.newBroadcast[T](value, isLocal)$/;"	m
broadcast	api/java/JavaSparkContext.scala	/^  def broadcast[T](value: T): Broadcast[T] = sc.broadcast(value)$/;"	m
broadcastCleaner	storage/BlockManager.scala	/^  private val broadcastCleaner = new MetadataCleaner(MetadataCleanerType.BROADCAST_VARS, this.dropOldBroadcastBlocks)$/;"	V
broadcastDir	broadcast/HttpBroadcast.scala	/^  private var broadcastDir: File = null$/;"	v
broadcastFactory	broadcast/Broadcast.scala	/^  private var broadcastFactory: BroadcastFactory = null$/;"	v
broadcastFactoryClass	broadcast/Broadcast.scala	/^        val broadcastFactoryClass = System.getProperty($/;"	V
broadcastId	broadcast/TorrentBroadcast.scala	/^  def broadcastId = BroadcastBlockId(id)$/;"	m
broadcastManager	SparkEnv.scala	/^    val broadcastManager = new BroadcastManager(isDriver)$/;"	V
broadcastManager	SparkEnv.scala	/^    val broadcastManager: BroadcastManager,$/;"	V
bs	storage/BlockObjectWriter.scala	/^  private var bs: OutputStream = null$/;"	v
bucketId	scheduler/ShuffleMapTask.scala	/^        val bucketId = dep.partitioner.getPartition(pair._1)$/;"	V
buf	api/python/PythonWorkerFactory.scala	/^              val buf = new Array[Byte](1024)$/;"	V
buf	api/python/PythonWorkerFactory.scala	/^            val buf = new Array[Byte](1024)$/;"	V
buf	network/netty/FileHeader.scala	/^    val buf = Unpooled.buffer()$/;"	V
buf	network/netty/FileHeader.scala	/^    val buf = header.buffer$/;"	V
buf	rdd/AsyncRDDActions.scala	/^        val buf = new Array[Array[T]](p.size)$/;"	V
buf	rdd/OrderedRDDFunctions.scala	/^      val buf = iter.toArray$/;"	V
buf	rdd/PairRDDFunctions.scala	/^          val buf = new ArrayBuffer[V]$/;"	V
buf	rdd/PipedRDD.scala	/^    val buf = new ArrayBuffer[String]$/;"	V
buf	rdd/RDD.scala	/^    val buf = new ArrayBuffer[T]$/;"	V
buf	util/Utils.scala	/^    val buf = new ArrayBuffer[String]$/;"	V
buf	util/Utils.scala	/^    val buf = new Array[Byte](8192)$/;"	V
buff	util/Utils.scala	/^    val buff = new Array[Byte]((effectiveEnd-effectiveStart).toInt)$/;"	V
buffer	network/BufferMessage.scala	/^      val buffer = buffers(0)$/;"	V
buffer	network/BufferMessage.scala	/^    val buffer = buffers(0)$/;"	V
buffer	network/Connection.scala	/^          val buffer = currentBuffers(0)$/;"	V
buffer	network/ConnectionManager.scala	/^    val buffer = ByteBuffer.allocate(size).put(Array.tabulate[Byte](size)(x => x.toByte))$/;"	V
buffer	network/ConnectionManagerTest.scala	/^        val buffer = ByteBuffer.allocate(size).put(Array.tabulate[Byte](size)(x => x.toByte))$/;"	V
buffer	network/MessageChunkHeader.scala	/^  lazy val buffer = {$/;"	V
buffer	network/ReceiverTest.scala	/^      val buffer = ByteBuffer.wrap("response".getBytes())$/;"	V
buffer	network/SenderTest.scala	/^          val buffer = response.asInstanceOf[BufferMessage].buffers(0)$/;"	V
buffer	network/SenderTest.scala	/^    val buffer = ByteBuffer.allocate(size).put(Array.tabulate[Byte](size)(x => x.toByte))$/;"	V
buffer	network/netty/FileHeader.scala	/^  lazy val buffer = {$/;"	V
buffer	scheduler/TaskDescription.scala	/^  private val buffer = new SerializableBuffer(_serializedTask)$/;"	V
buffer	serializer/Serializer.scala	/^    val buffer = ByteBuffer.allocate(stream.position.toInt)$/;"	V
buffer	storage/BlockManagerWorker.scala	/^        val buffer = getBlock(gB.id)$/;"	V
buffer	storage/BlockManagerWorker.scala	/^    val buffer = blockManager.getLocalBytes(id) match {$/;"	V
buffer	storage/BlockMessage.scala	/^    val buffer = bufferMsg.buffers.apply(0)$/;"	V
buffer	storage/BlockMessage.scala	/^    var buffer = ByteBuffer.allocate(4 + 4 + id.name.length * 2)$/;"	v
buffer	storage/BlockMessageArray.scala	/^          val buffer =  ByteBuffer.allocate(100)$/;"	V
buffer	storage/BlockMessageArray.scala	/^    val buffer = bufferMessage.buffers(0)$/;"	V
buffer	storage/DiskStore.scala	/^      val buffer = getBytes(blockId).get$/;"	V
buffer	storage/DiskStore.scala	/^    val buffer = try {$/;"	V
buffer	storage/MemoryStore.scala	/^      val buffer = entry.value.asInstanceOf[ByteBuffer].duplicate() \/\/ Doesn't actually copy data$/;"	V
buffer	util/ByteBufferInputStream.scala	/^class ByteBufferInputStream(private var buffer: ByteBuffer, dispose: Boolean = false)$/;"	v
buffer	util/SerializableBuffer.scala	/^class SerializableBuffer(@transient var buffer: ByteBuffer) extends Serializable {$/;"	v
bufferMessage	network/Connection.scala	/^          val bufferMessage = inbox.getMessageForChunk(currentChunk).get$/;"	V
bufferMessage	network/ConnectionManager.scala	/^          val bufferMessage = Message.createBufferMessage(buffer.duplicate)$/;"	V
bufferMessage	network/ConnectionManager.scala	/^      val bufferMessage = Message.createBufferMessage(buffer.duplicate)$/;"	V
bufferMessage	network/ConnectionManager.scala	/^      val bufferMessage = Message.createBufferMessage(buffers(count - 1 - i).duplicate)$/;"	V
bufferMessage	network/ConnectionManagerTest.scala	/^          val bufferMessage = Message.createBufferMessage(buffer.duplicate)$/;"	V
bufferMessage	storage/BlockFetcherIterator.scala	/^          val bufferMessage = message.asInstanceOf[BufferMessage]$/;"	V
bufferMessage	storage/BlockManagerWorker.scala	/^        val bufferMessage = message.asInstanceOf[BufferMessage]$/;"	V
bufferMessage	storage/BlockMessageArray.scala	/^      val bufferMessage = blockMessage.toBufferMessage$/;"	V
bufferMessage	storage/BlockMessageArray.scala	/^    val bufferMessage = blockMessageArray.toBufferMessage$/;"	V
bufferSize	SparkContext.scala	/^    val bufferSize = System.getProperty("spark.buffer.size", "65536")$/;"	V
bufferSize	api/python/PythonRDD.scala	/^  val bufferSize = System.getProperty("spark.buffer.size", "65536").toInt$/;"	V
bufferSize	broadcast/HttpBroadcast.scala	/^  private var bufferSize: Int = 65536$/;"	v
bufferSize	rdd/CheckpointRDD.scala	/^    val bufferSize = System.getProperty("spark.buffer.size", "65536").toInt$/;"	V
bufferSize	serializer/KryoSerializer.scala	/^  private val bufferSize = {$/;"	V
bufferSize	storage/ShuffleBlockManager.scala	/^  private val bufferSize = System.getProperty("spark.shuffle.file.buffer.kb", "100").toInt * 1024$/;"	V
bufferView	storage/BlockManager.scala	/^      val bufferView = data.right.get.duplicate() \/\/ Doesn't copy the bytes, just creates a wrapper$/;"	V
buffers	network/BufferMessage.scala	/^class BufferMessage(id_ : Int, val buffers: ArrayBuffer[ByteBuffer], var ackId: Int)$/;"	V
buffers	network/Connection.scala	/^                val buffers = chunk.buffers$/;"	V
buffers	network/ConnectionManager.scala	/^    val buffers = Array.tabulate(count)(i => ByteBuffer.allocate(size * (i + 1)).put(Array.tabulate[Byte](size * (i + 1))(x => x.toByte)))$/;"	V
buffers	network/MessageChunk.scala	/^  lazy val buffers = {$/;"	V
buffers	storage/BlockMessage.scala	/^    val buffers = new ArrayBuffer[ByteBuffer]()$/;"	V
buffers	storage/BlockMessageArray.scala	/^    val buffers = new ArrayBuffer[ByteBuffer]()$/;"	V
bufs	rdd/PairRDDFunctions.scala	/^    val bufs = combineByKey[ArrayBuffer[V]]($/;"	V
buildCommandSeq	deploy/worker/ExecutorRunner.scala	/^  def buildCommandSeq(): Seq[String] = {$/;"	m
buildJavaOpts	deploy/worker/ExecutorRunner.scala	/^  def buildJavaOpts(): Seq[String] = {$/;"	m
builder	deploy/worker/ExecutorRunner.scala	/^      val builder = new ProcessBuilder(command: _*).directory(executorDir)$/;"	V
builder	util/Utils.scala	/^    val builder = new ProcessBuilder(command: _*)$/;"	V
bySize	Partitioner.scala	/^    val bySize = (Seq(rdd) ++ others).sortBy(_.partitions.size).reverse$/;"	V
byteArray	broadcast/MultiTracker.scala	/^    val byteArray = baos.toByteArray$/;"	V
byteArray	broadcast/TorrentBroadcast.scala	/^    val byteArray = Utils.serialize[T](obj)$/;"	V
byteLength	deploy/worker/ui/WorkerWebUI.scala	/^    val byteLength = Option(request.getParameter("byteLength")).map(_.toInt).getOrElse(defaultBytes)$/;"	V
byteRead	api/python/PythonRDD.scala	/^      val byteRead = in.read()$/;"	V
byteStream	storage/BlockManager.scala	/^    val byteStream = new FastBufferedOutputStream(outputStream)$/;"	V
byteStream	storage/BlockManager.scala	/^    val byteStream = new FastByteArrayOutputStream(4096)$/;"	V
byteVal	scheduler/TaskResult.scala	/^    val byteVal = new Array[Byte](blen)$/;"	V
bytes	MapOutputTracker.scala	/^    val bytes = serializeStatuses(statuses)$/;"	V
bytes	network/Connection.scala	/^    val bytes = new Array[Byte](buffer.remaining)$/;"	V
bytes	network/Connection.scala	/^    val bytes = new Array[Byte](length)$/;"	V
bytes	scheduler/ResultTask.scala	/^        val bytes = out.toByteArray$/;"	V
bytes	scheduler/ResultTask.scala	/^      val bytes = ResultTask.serializeInfo($/;"	V
bytes	scheduler/ResultTask.scala	/^    val bytes = new Array[Byte](numBytes)$/;"	V
bytes	scheduler/ShuffleMapTask.scala	/^        val bytes = out.toByteArray$/;"	V
bytes	scheduler/ShuffleMapTask.scala	/^      val bytes = ShuffleMapTask.serializeInfo(stageId, rdd, dep)$/;"	V
bytes	scheduler/ShuffleMapTask.scala	/^    val bytes = new Array[Byte](numBytes)$/;"	V
bytes	scheduler/local/LocalTaskSetManager.scala	/^          val bytes = Task.serializeWithDependencies($/;"	V
bytes	storage/BlockManager.scala	/^          val bytes: ByteBuffer = diskStore.getBytes(blockId) match {$/;"	V
bytes	storage/DiskStore.scala	/^    val bytes = _bytes.duplicate()$/;"	V
bytes	storage/MemoryStore.scala	/^      val bytes = blockManager.dataSerialize(blockId, values.iterator)$/;"	V
bytes	storage/MemoryStore.scala	/^    val bytes = _bytes.duplicate()$/;"	V
bytesAfterPut	storage/BlockManager.scala	/^    var bytesAfterPut: ByteBuffer = null$/;"	v
bytesInFlight	storage/BlockFetcherIterator.scala	/^    private var bytesInFlight = 0L$/;"	v
bytesPerFile	storage/StoragePerfTester.scala	/^    val bytesPerFile = (totalBytes.get() \/ (numOutputSplits * numMaps.toDouble)).toLong$/;"	V
bytesPerSecond	storage/StoragePerfTester.scala	/^    val bytesPerSecond = totalBytes.get() \/ time$/;"	V
bytesRead	network/Connection.scala	/^        val bytesRead = channel.read(currentChunk.buffer)$/;"	V
bytesToString	util/Utils.scala	/^  def bytesToString(size: Long): String = {$/;"	m
bytesWritten	storage/BlockObjectWriter.scala	/^    val bytesWritten = lastValidPosition - initialPosition$/;"	V
bytesWrittenSinceSync	util/RateLimitedOutputStream.scala	/^  var bytesWrittenSinceSync: Long = 0$/;"	v
c	rdd/SequenceFileRDDFunctions.scala	/^    val c = {$/;"	V
cache	api/java/JavaDoubleRDD.scala	/^  def cache(): JavaDoubleRDD = fromRDD(srdd.cache())$/;"	m
cache	api/java/JavaPairRDD.scala	/^  def cache(): JavaPairRDD[K, V] = new JavaPairRDD[K, V](rdd.cache())$/;"	m
cache	api/java/JavaRDD.scala	/^  def cache(): JavaRDD[T] = wrapRDD(rdd.cache())$/;"	m
cache	partial/StudentTCacher.scala	/^  val cache = Array.fill[Double](NORMAL_APPROX_SAMPLE_SIZE)(-1.0)$/;"	V
cache	rdd/RDD.scala	/^  def cache(): RDD[T] = persist()$/;"	m
cacheEpoch	MapOutputTracker.scala	/^  var cacheEpoch = epoch$/;"	v
cacheLocs	scheduler/DAGScheduler.scala	/^  private val cacheLocs = new HashMap[Int, Array[Seq[TaskLocation]]]$/;"	V
cacheManager	SparkEnv.scala	/^    val cacheManager = new CacheManager(blockManager)$/;"	V
cacheManager	SparkEnv.scala	/^    val cacheManager: CacheManager,$/;"	V
cached	scheduler/DAGScheduler.scala	/^    val cached = getCacheLocs(rdd)(partition)$/;"	V
cached	util/Utils.scala	/^      var cached = hostPortParseResults.get(hostPort)$/;"	v
cachedPeers	storage/BlockManager.scala	/^  var cachedPeers: Seq[BlockManagerId] = null$/;"	v
cachedSerializedStatuses	MapOutputTracker.scala	/^  private val cachedSerializedStatuses = new TimeStampedHashMap[Int, Array[Byte]]$/;"	V
cachedSerializer	rdd/PairRDDFunctions.scala	/^    lazy val cachedSerializer = SparkEnv.get.closureSerializer.newInstance()$/;"	V
call	api/java/function/Function3.java	/^  public abstract R call(T1 t1, T2 t2, T3 t3) throws Exception;$/;"	m	class:Function3
call	api/java/function/Function4.java	/^  public abstract R call(T1 t1, T2 t2, T3 t3, T4 t4) throws Exception;$/;"	m	class:Function4
call	api/java/function/VoidFunction.scala	/^  def call(t: T) : Unit$/;"	m
call	api/java/function/WrappedFunction1.scala	/^  def call(t: T): R$/;"	m
call	api/java/function/WrappedFunction2.scala	/^  def call(t1: T1, t2: T2): R$/;"	m
call	api/java/function/WrappedFunction3.scala	/^  def call(t1: T1, t2: T2, t3: T3): R$/;"	m
call	api/java/function/WrappedFunction4.scala	/^  def call(t1: T1, t2: T2, t3: T3, t4: T4): R$/;"	m
callOnExceptionCallback	network/Connection.scala	/^  def callOnExceptionCallback(e: Exception) {$/;"	m
callSite	SparkContext.scala	/^    val callSite = Utils.formatSparkCallSite$/;"	V
callSite	scheduler/ActiveJob.scala	/^    val callSite: String,$/;"	V
callSiteInfo	util/Utils.scala	/^    val callSiteInfo = getCallSiteInfo$/;"	V
canCompleteRecovery	deploy/master/Master.scala	/^  def canCompleteRecovery =$/;"	m
canUse	deploy/master/Master.scala	/^  def canUse(app: ApplicationInfo, worker: WorkerInfo): Boolean = {$/;"	m
cancelJob	scheduler/DAGScheduler.scala	/^  def cancelJob(jobId: Int) {$/;"	m
cancelJobGroup	SparkContext.scala	/^  def cancelJobGroup(groupId: String) {$/;"	m
cancelJobGroup	scheduler/DAGScheduler.scala	/^  def cancelJobGroup(groupId: String) {$/;"	m
cancelTasks	scheduler/TaskScheduler.scala	/^  def cancelTasks(stageId: Int)$/;"	m
cancelled	FutureAction.scala	/^  def cancelled: Boolean = _cancelled$/;"	m
capacity	util/AppendOnlyMap.scala	/^  private var capacity = nextPowerOf2(initialCapacity)$/;"	v
capacity	util/collection/OpenHashSet.scala	/^  def capacity: Int = _capacity$/;"	m
capacity	util/collection/PrimitiveVector.scala	/^  def capacity: Int = _array.length$/;"	m
cardinality	util/collection/BitSet.scala	/^  def cardinality(): Int = {$/;"	m
cartesian	api/java/JavaRDDLike.scala	/^  def cartesian[U](other: JavaRDDLike[U, _]): JavaPairRDD[T, U] =$/;"	m
cartesian	rdd/RDD.scala	/^  def cartesian[U: ClassManifest](other: RDD[U]): RDD[(T, U)] = new CartesianRDD(sc, this, other)$/;"	m
causeOfFailure	scheduler/cluster/ClusterTaskSetManager.scala	/^  var causeOfFailure = ""$/;"	v
cern.jet.random.Poisson	rdd/SampledRDD.scala	/^import cern.jet.random.Poisson$/;"	i
cern.jet.random.engine.DRand	rdd/SampledRDD.scala	/^import cern.jet.random.engine.DRand$/;"	i
cern.jet.stat.Probability	partial/CountEvaluator.scala	/^import cern.jet.stat.Probability$/;"	i
cern.jet.stat.Probability	partial/GroupedCountEvaluator.scala	/^import cern.jet.stat.Probability$/;"	i
cern.jet.stat.Probability	partial/MeanEvaluator.scala	/^import cern.jet.stat.Probability$/;"	i
cern.jet.stat.Probability	partial/StudentTCacher.scala	/^import cern.jet.stat.Probability$/;"	i
cern.jet.stat.Probability	partial/SumEvaluator.scala	/^import cern.jet.stat.Probability$/;"	i
cg	rdd/PairRDDFunctions.scala	/^    val cg = new CoGroupedRDD[K](Seq(self, other), partitioner)$/;"	V
cg	rdd/PairRDDFunctions.scala	/^    val cg = new CoGroupedRDD[K](Seq(self, other1, other2), partitioner)$/;"	V
changeConnectionKeyInterest	network/Connection.scala	/^  def changeConnectionKeyInterest(ops: Int) {$/;"	m
changeConnectionKeyInterest	network/ConnectionManager.scala	/^  def changeConnectionKeyInterest(connection: Connection, ops: Int) {$/;"	m
changeInterestForRead	network/Connection.scala	/^  def changeInterestForRead(): Boolean$/;"	m
changeInterestForWrite	network/Connection.scala	/^  def changeInterestForWrite(): Boolean$/;"	m
changeMaster	deploy/client/Client.scala	/^    def changeMaster(url: String) {$/;"	m
changeMaster	deploy/worker/Worker.scala	/^  def changeMaster(url: String, uiUrl: String) {$/;"	m
changeValue	util/AppendOnlyMap.scala	/^  def changeValue(key: K, updateFunc: (Boolean, V) => V): V = {$/;"	m
changeValue	util/collection/OpenHashMap.scala	/^  def changeValue(k: K, defaultValue: => V, mergeValue: (V) => V): V = {$/;"	m
changeValue	util/collection/PrimitiveKeyOpenHashMap.scala	/^  def changeValue(k: K, defaultValue: => V, mergeValue: (V) => V): V = {$/;"	m
channel	network/Connection.scala	/^abstract class Connection(val channel: SocketChannel, val selector: Selector,$/;"	V
channel	storage/BlockObjectWriter.scala	/^  private var channel: FileChannel = null$/;"	v
channel	storage/DiskStore.scala	/^    val channel = new FileOutputStream(file).getChannel()$/;"	V
channel	storage/DiskStore.scala	/^    val channel = new RandomAccessFile(segment.file, "r").getChannel()$/;"	V
channel	util/SerializableBuffer.scala	/^    val channel = Channels.newChannel(in)$/;"	V
checkHost	util/Utils.scala	/^  def checkHost(host: String, message: String = "") {$/;"	m
checkHostPort	util/Utils.scala	/^  def checkHostPort(hostPort: String, message: String = "") {$/;"	m
checkMinimalPollingPeriod	metrics/MetricsSystem.scala	/^  def checkMinimalPollingPeriod(pollUnit: TimeUnit, pollPeriod: Int) {$/;"	m
checkSpeculatableTasks	scheduler/Schedulable.scala	/^  def checkSpeculatableTasks(): Boolean$/;"	m
checkTimeoutInterval	storage/BlockManagerMasterActor.scala	/^  val checkTimeoutInterval = System.getProperty("spark.storage.blockManagerTimeoutIntervalMs",$/;"	V
checkpoint	api/java/JavaRDDLike.scala	/^  def checkpoint() = rdd.checkpoint()$/;"	m
checkpointData	rdd/RDD.scala	/^  private[spark] var checkpointData: Option[RDDCheckpointData[T]] = None$/;"	v
checkpointDir	SparkContext.scala	/^  private[spark] var checkpointDir: Option[String] = None$/;"	v
checkpointPath	rdd/CheckpointRDD.scala	/^class CheckpointRDD[T: ClassManifest](sc: SparkContext, val checkpointPath: String)$/;"	V
checkpointRDD	rdd/RDDCheckpointData.scala	/^  def checkpointRDD: Option[RDD[T]] = {$/;"	m
chunk	network/Connection.scala	/^          val chunk = message.getChunkForSending(defaultChunkSize)$/;"	V
chunkSize	network/MessageChunkHeader.scala	/^    val chunkSize = buffer.getInt()$/;"	V
chunkSize	network/MessageChunkHeader.scala	/^    val chunkSize: Int,$/;"	V
classInfo	util/SizeEstimator.scala	/^      val classInfo = getClassInfo(cls)$/;"	V
classInfos	util/SizeEstimator.scala	/^  private val classInfos = new ConcurrentHashMap[Class[_], ClassInfo]$/;"	V
classLoader	SparkEnv.scala	/^    val classLoader = Thread.currentThread.getContextClassLoader$/;"	V
classLoader	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^  var classLoader: ClassLoader = null$/;"	v
classLoader	serializer/KryoSerializer.scala	/^    val classLoader = Thread.currentThread.getContextClassLoader$/;"	V
classManifest	api/java/JavaDoubleRDD.scala	/^  override val classManifest: ClassManifest[Double] = implicitly[ClassManifest[Double]]$/;"	V
classManifest	api/java/JavaPairRDD.scala	/^  override val classManifest: ClassManifest[(K, V)] =$/;"	V
classManifest	api/java/JavaRDDLike.scala	/^  implicit val classManifest: ClassManifest[T]$/;"	V
className	Logging.scala	/^      var className = this.getClass.getName$/;"	v
className	util/ClosureCleaner.scala	/^    val className = cls.getName.replaceFirst("^.*\\\\.", "") + ".class"$/;"	V
classPath	deploy/worker/ExecutorRunner.scala	/^    val classPath = Utils.executeAndGetOutput($/;"	V
classPath	metrics/MetricsSystem.scala	/^      val classPath = kv._2.getProperty("class")$/;"	V
classPath	ui/env/EnvironmentUI.scala	/^    val classPath = (addedJars ++ addedFiles ++ classPathEntries).sorted$/;"	V
classPathEntries	ui/env/EnvironmentUI.scala	/^    val classPathEntries = classPathProperty._2$/;"	V
classPathHeaders	ui/env/EnvironmentUI.scala	/^    val classPathHeaders = Seq("Resource", "Source")$/;"	V
classPathProperty	ui/env/EnvironmentUI.scala	/^    val classPathProperty = properties.find { case (k, v) =>$/;"	V
classPathRow	ui/env/EnvironmentUI.scala	/^    def classPathRow(data: (String, String)) = <tr><td>{data._1}<\/td><td>{data._2}<\/td><\/tr>$/;"	m
classPathTable	ui/env/EnvironmentUI.scala	/^    val classPathTable =$/;"	V
classUri	executor/Executor.scala	/^    val classUri = System.getProperty("spark.repl.class.uri")$/;"	V
clazz	SparkContext.scala	/^          val clazz = Class.forName("org.apache.spark.scheduler.cluster.YarnClientClusterScheduler")$/;"	V
clazz	SparkContext.scala	/^          val clazz = Class.forName("org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend")$/;"	V
clazz	SparkContext.scala	/^          val clazz = Class.forName("org.apache.spark.scheduler.cluster.YarnClusterScheduler")$/;"	V
clazz	deploy/master/FileSystemPersistenceEngine.scala	/^    val clazz = m.erasure.asInstanceOf[Class[T]]$/;"	V
clazz	deploy/master/ZooKeeperPersistenceEngine.scala	/^    val clazz = m.erasure.asInstanceOf[Class[T]]$/;"	V
clean	util/ClosureCleaner.scala	/^  def clean(func: AnyRef) {$/;"	m
cleanCombOp	rdd/RDD.scala	/^    val cleanCombOp = sc.clean(combOp)$/;"	V
cleanF	SparkContext.scala	/^    val cleanF = clean(processPartition)$/;"	V
cleanF	api/java/JavaRDDLike.scala	/^    val cleanF = rdd.context.clean(f)$/;"	V
cleanF	rdd/PairRDDFunctions.scala	/^    val cleanF = self.context.clean(f)$/;"	V
cleanF	rdd/RDD.scala	/^    val cleanF = sc.clean(f)$/;"	V
cleanOp	rdd/RDD.scala	/^    val cleanOp = sc.clean(op)$/;"	V
cleanSeqOp	rdd/RDD.scala	/^    val cleanSeqOp = sc.clean(seqOp)$/;"	V
cleanedFunc	SparkContext.scala	/^    val cleanedFunc = clean(func)$/;"	V
cleaner	broadcast/HttpBroadcast.scala	/^  private val cleaner = new MetadataCleaner(MetadataCleanerType.HTTP_BROADCAST, cleanup)$/;"	V
cleanup	broadcast/HttpBroadcast.scala	/^  def cleanup(cleanupTime: Long) {$/;"	m
clearOldValues	util/TimeStampedHashMap.scala	/^  def clearOldValues(threshTime: Long) {$/;"	m
clearOldValues	util/TimeStampedHashSet.scala	/^  def clearOldValues(threshTime: Long) {$/;"	m
client	deploy/client/TestClient.scala	/^    val client = new Client(actorSystem, Array(url), desc, listener)$/;"	V
client	scheduler/cluster/SparkDeploySchedulerBackend.scala	/^  var client: Client = null$/;"	v
clientSocket	broadcast/BitTorrentBroadcast.scala	/^          var clientSocket: Socket = null$/;"	v
clientSocket	broadcast/BitTorrentBroadcast.scala	/^    class GuideSingleRequest(val clientSocket: Socket)$/;"	V
clientSocket	broadcast/BitTorrentBroadcast.scala	/^    class ServeSingleRequest(val clientSocket: Socket)$/;"	V
clientSocket	broadcast/MultiTracker.scala	/^          var clientSocket: Socket = null$/;"	v
clientSocket	broadcast/TreeBroadcast.scala	/^          var clientSocket: Socket = null$/;"	v
clientSocket	broadcast/TreeBroadcast.scala	/^    class GuideSingleRequest(val clientSocket: Socket)$/;"	V
clientSocket	broadcast/TreeBroadcast.scala	/^    class ServeSingleRequest(val clientSocket: Socket)$/;"	V
clientSocketToDriver	broadcast/TreeBroadcast.scala	/^    var clientSocketToDriver: Socket = null$/;"	v
clientSocketToGuide	broadcast/BitTorrentBroadcast.scala	/^      var clientSocketToGuide: Socket = null$/;"	v
clientSocketToSource	broadcast/TreeBroadcast.scala	/^    var clientSocketToSource: Socket = null$/;"	v
clientSocketToTracker	broadcast/MultiTracker.scala	/^    var clientSocketToTracker: Socket = null$/;"	v
clone	util/Utils.scala	/^  def clone[T](value: T, serializer: SerializerInstance): T = {$/;"	m
close	serializer/Serializer.scala	/^  def close(): Unit$/;"	m
closed	deploy/master/SparkZooKeeperSession.scala	/^  private var closed = false$/;"	v
closed	network/Connection.scala	/^  @volatile private var closed = false$/;"	v
closed	util/NextIterator.scala	/^  private var closed = false$/;"	v
closureSerializer	SparkEnv.scala	/^    val closureSerializer = serializerManager.get($/;"	V
closureSerializer	SparkEnv.scala	/^    val closureSerializer: Serializer,$/;"	V
cls	util/SizeEstimator.scala	/^    val cls = obj.getClass$/;"	V
clsLoader	serializer/SerializerManager.scala	/^          val clsLoader = Thread.currentThread.getContextClassLoader$/;"	V
cm	api/java/JavaPairRDD.scala	/^    implicit val cm: ClassManifest[C] =$/;"	V
cm	api/java/JavaPairRDD.scala	/^    implicit val cm: ClassManifest[U] =$/;"	V
cm	api/java/JavaRDDLike.scala	/^    def cm = implicitly[ClassManifest[AnyRef]].asInstanceOf[ClassManifest[Tuple2[K2, V2]]]$/;"	m
cm	api/java/JavaSparkContext.scala	/^    implicit val cm: ClassManifest[(K, V)] = first.classManifest$/;"	V
cm	api/java/JavaSparkContext.scala	/^    implicit val cm: ClassManifest[T] = first.classManifest$/;"	V
cm	api/java/JavaSparkContext.scala	/^    implicit val cm: ClassManifest[T] =$/;"	V
cm	api/python/PythonRDD.scala	/^    implicit val cm : ClassManifest[T] = rdd.elementClassManifest$/;"	V
cmId	storage/BlockFetcherIterator.scala	/^      val cmId = new ConnectionManagerId(req.address.host, req.address.nettyPort)$/;"	V
cmId	storage/BlockFetcherIterator.scala	/^      val cmId = new ConnectionManagerId(req.address.host, req.address.port)$/;"	V
cmd	deploy/FaultToleranceTest.scala	/^    val cmd = "docker run %s %s %s".format(mountCmd, imageTag, args)$/;"	V
cmd	deploy/FaultToleranceTest.scala	/^    val cmd = Docker.makeRunCmd("spark-test-master", mountDir = mountDir)$/;"	V
cmd	deploy/FaultToleranceTest.scala	/^    val cmd = Docker.makeRunCmd("spark-test-worker", args = masters, mountDir = mountDir)$/;"	V
cmk	api/java/JavaPairRDD.scala	/^    implicit val cmk: ClassManifest[K] =$/;"	V
cmtr	SparkHadoopWriter.scala	/^    val cmtr = getOutputCommitter() $/;"	V
cmtr	SparkHadoopWriter.scala	/^    val cmtr = getOutputCommitter()$/;"	V
cmv	api/java/JavaPairRDD.scala	/^    implicit val cmv: ClassManifest[V] =$/;"	V
coalesce	api/java/JavaDoubleRDD.scala	/^  def coalesce(numPartitions: Int): JavaDoubleRDD = fromRDD(srdd.coalesce(numPartitions))$/;"	m
coalesce	api/java/JavaDoubleRDD.scala	/^  def coalesce(numPartitions: Int, shuffle: Boolean): JavaDoubleRDD =$/;"	m
coalesce	api/java/JavaPairRDD.scala	/^  def coalesce(numPartitions: Int): JavaPairRDD[K, V] = fromRDD(rdd.coalesce(numPartitions))$/;"	m
coalesce	api/java/JavaPairRDD.scala	/^  def coalesce(numPartitions: Int, shuffle: Boolean): JavaPairRDD[K, V] =$/;"	m
coalesce	api/java/JavaRDD.scala	/^  def coalesce(numPartitions: Int): JavaRDD[T] = rdd.coalesce(numPartitions)$/;"	m
coalesce	api/java/JavaRDD.scala	/^  def coalesce(numPartitions: Int, shuffle: Boolean): JavaRDD[T] =$/;"	m
coalesce	rdd/RDD.scala	/^  def coalesce(numPartitions: Int, shuffle: Boolean = false): RDD[T] = {$/;"	m
coarseGrained	SparkContext.scala	/^        val coarseGrained = System.getProperty("spark.mesos.coarse", "false").toBoolean$/;"	V
cogroup	api/java/JavaPairRDD.scala	/^  def cogroup[W1, W2](other1: JavaPairRDD[K, W1], other2: JavaPairRDD[K, W2])$/;"	m
cogroup	api/java/JavaPairRDD.scala	/^  def cogroup[W1, W2](other1: JavaPairRDD[K, W1], other2: JavaPairRDD[K, W2], numPartitions: Int)$/;"	m
cogroup	api/java/JavaPairRDD.scala	/^  def cogroup[W1, W2](other1: JavaPairRDD[K, W1], other2: JavaPairRDD[K, W2], partitioner: Partitioner)$/;"	m
cogroup	api/java/JavaPairRDD.scala	/^  def cogroup[W](other: JavaPairRDD[K, W]): JavaPairRDD[K, (JList[V], JList[W])] =$/;"	m
cogroup	api/java/JavaPairRDD.scala	/^  def cogroup[W](other: JavaPairRDD[K, W], numPartitions: Int): JavaPairRDD[K, (JList[V], JList[W])]$/;"	m
cogroup	api/java/JavaPairRDD.scala	/^  def cogroup[W](other: JavaPairRDD[K, W], partitioner: Partitioner)$/;"	m
cogroup	rdd/PairRDDFunctions.scala	/^  def cogroup[W1, W2](other1: RDD[(K, W1)], other2: RDD[(K, W2)])$/;"	m
cogroup	rdd/PairRDDFunctions.scala	/^  def cogroup[W1, W2](other1: RDD[(K, W1)], other2: RDD[(K, W2)], numPartitions: Int)$/;"	m
cogroup	rdd/PairRDDFunctions.scala	/^  def cogroup[W1, W2](other1: RDD[(K, W1)], other2: RDD[(K, W2)], partitioner: Partitioner)$/;"	m
cogroup	rdd/PairRDDFunctions.scala	/^  def cogroup[W](other: RDD[(K, W)]): RDD[(K, (Seq[V], Seq[W]))] = {$/;"	m
cogroup	rdd/PairRDDFunctions.scala	/^  def cogroup[W](other: RDD[(K, W)], numPartitions: Int): RDD[(K, (Seq[V], Seq[W]))] = {$/;"	m
cogroup	rdd/PairRDDFunctions.scala	/^  def cogroup[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (Seq[V], Seq[W]))] = {$/;"	m
cogroupResult2ToJava	api/java/JavaPairRDD.scala	/^  def cogroupResult2ToJava[W1, W2, K, V](rdd: RDD[(K, (Seq[V], Seq[W1],$/;"	m
cogroupResultToJava	api/java/JavaPairRDD.scala	/^  def cogroupResultToJava[W, K, V](rdd: RDD[(K, (Seq[V], Seq[W]))])(implicit kcm: ClassManifest[K],$/;"	m
colWidth	ui/UIUtils.scala	/^    val colWidth = 100.toDouble \/ headers.size$/;"	V
colWidthAttr	ui/UIUtils.scala	/^    val colWidthAttr = if (fixedWidth) colWidth + "%" else ""$/;"	V
collect	api/java/JavaRDDLike.scala	/^  def collect(): JList[T] = {$/;"	m
collect	rdd/RDD.scala	/^  def collect(): Array[T] = {$/;"	m
collect	rdd/RDD.scala	/^  def collect[U: ClassManifest](f: PartialFunction[T, U]): RDD[U] = {$/;"	m
collectAsMap	api/java/JavaPairRDD.scala	/^  def collectAsMap(): java.util.Map[K, V] = mapAsJavaMap(rdd.collectAsMap())$/;"	m
collectAsMap	rdd/PairRDDFunctions.scala	/^  def collectAsMap(): Map[K, V] = {$/;"	m
collectAsync	rdd/AsyncRDDActions.scala	/^  def collectAsync(): FutureAction[Seq[T]] = {$/;"	m
collection.JavaConverters._	storage/BlockManagerMasterActor.scala	/^      import collection.JavaConverters._$/;"	i
collection.mutable	SparkEnv.scala	/^import collection.mutable$/;"	i
collection.mutable	ui/jobs/JobProgressUI.scala	/^import collection.mutable$/;"	i
collection.mutable.ArrayBuffer	scheduler/SplitInfo.scala	/^import collection.mutable.ArrayBuffer$/;"	i
collection.mutable.ArrayBuffer	storage/MemoryStore.scala	/^import collection.mutable.ArrayBuffer$/;"	i
com.codahale.metrics.MetricRegistry	metrics/sink/GangliaSink.scala	/^import com.codahale.metrics.MetricRegistry$/;"	i
com.codahale.metrics.MetricRegistry	metrics/sink/MetricsServlet.scala	/^import com.codahale.metrics.MetricRegistry$/;"	i
com.codahale.metrics.MetricRegistry	metrics/source/JvmSource.scala	/^import com.codahale.metrics.MetricRegistry$/;"	i
com.codahale.metrics.MetricRegistry	metrics/source/Source.scala	/^import com.codahale.metrics.MetricRegistry$/;"	i
com.codahale.metrics.ganglia.GangliaReporter	metrics/sink/GangliaSink.scala	/^import com.codahale.metrics.ganglia.GangliaReporter$/;"	i
com.codahale.metrics.json.MetricsModule	metrics/sink/MetricsServlet.scala	/^import com.codahale.metrics.json.MetricsModule$/;"	i
com.codahale.metrics.jvm.{GarbageCollectorMetricSet, MemoryUsageGaugeSet}	metrics/source/JvmSource.scala	/^import com.codahale.metrics.jvm.{GarbageCollectorMetricSet, MemoryUsageGaugeSet}$/;"	i
com.codahale.metrics.{ConsoleReporter, MetricRegistry}	metrics/sink/ConsoleSink.scala	/^import com.codahale.metrics.{ConsoleReporter, MetricRegistry}$/;"	i
com.codahale.metrics.{CsvReporter, MetricRegistry}	metrics/sink/CsvSink.scala	/^import com.codahale.metrics.{CsvReporter, MetricRegistry}$/;"	i
com.codahale.metrics.{Gauge, MetricRegistry}	deploy/master/ApplicationSource.scala	/^import com.codahale.metrics.{Gauge, MetricRegistry}$/;"	i
com.codahale.metrics.{Gauge, MetricRegistry}	deploy/master/MasterSource.scala	/^import com.codahale.metrics.{Gauge, MetricRegistry}$/;"	i
com.codahale.metrics.{Gauge, MetricRegistry}	deploy/worker/WorkerSource.scala	/^import com.codahale.metrics.{Gauge, MetricRegistry}$/;"	i
com.codahale.metrics.{Gauge, MetricRegistry}	executor/ExecutorSource.scala	/^import com.codahale.metrics.{Gauge, MetricRegistry}$/;"	i
com.codahale.metrics.{Gauge,MetricRegistry}	scheduler/DAGSchedulerSource.scala	/^import com.codahale.metrics.{Gauge,MetricRegistry}$/;"	i
com.codahale.metrics.{Gauge,MetricRegistry}	storage/BlockManagerSource.scala	/^import com.codahale.metrics.{Gauge,MetricRegistry}$/;"	i
com.codahale.metrics.{JmxReporter, MetricRegistry}	metrics/sink/JmxSink.scala	/^import com.codahale.metrics.{JmxReporter, MetricRegistry}$/;"	i
com.codahale.metrics.{Metric, MetricFilter, MetricRegistry}	metrics/MetricsSystem.scala	/^import com.codahale.metrics.{Metric, MetricFilter, MetricRegistry}$/;"	i
com.esotericsoftware.kryo.io.{Input => KryoInput, Output => KryoOutput}	serializer/KryoSerializer.scala	/^import com.esotericsoftware.kryo.io.{Input => KryoInput, Output => KryoOutput}$/;"	i
com.esotericsoftware.kryo.serializers.{JavaSerializer => KryoJavaSerializer}	serializer/KryoSerializer.scala	/^import com.esotericsoftware.kryo.serializers.{JavaSerializer => KryoJavaSerializer}$/;"	i
com.esotericsoftware.kryo.{KryoException, Kryo}	serializer/KryoSerializer.scala	/^import com.esotericsoftware.kryo.{KryoException, Kryo}$/;"	i
com.fasterxml.jackson.databind.ObjectMapper	metrics/sink/MetricsServlet.scala	/^import com.fasterxml.jackson.databind.ObjectMapper$/;"	i
com.google.common.base.Charsets	deploy/worker/ExecutorRunner.scala	/^import com.google.common.base.Charsets$/;"	i
com.google.common.base.Optional	api/java/JavaPairRDD.scala	/^import com.google.common.base.Optional$/;"	i
com.google.common.base.Optional	api/java/JavaRDDLike.scala	/^import com.google.common.base.Optional$/;"	i
com.google.common.base.Optional	api/java/JavaSparkContext.scala	/^import com.google.common.base.Optional$/;"	i
com.google.common.base.Optional	api/java/JavaUtils.scala	/^import com.google.common.base.Optional$/;"	i
com.google.common.collect.MapMaker	SparkEnv.scala	/^import com.google.common.collect.MapMaker$/;"	i
com.google.common.io.Files	HttpFileServer.scala	/^import com.google.common.io.Files$/;"	i
com.google.common.io.Files	deploy/worker/ExecutorRunner.scala	/^import com.google.common.io.Files$/;"	i
com.google.common.io.Files	util/Utils.scala	/^import com.google.common.io.Files$/;"	i
com.google.common.util.concurrent.ThreadFactoryBuilder	util/Utils.scala	/^import com.google.common.util.concurrent.ThreadFactoryBuilder$/;"	i
com.google.protobuf.ByteString	executor/MesosExecutorBackend.scala	/^import com.google.protobuf.ByteString$/;"	i
com.google.protobuf.ByteString	scheduler/cluster/mesos/CoarseMesosSchedulerBackend.scala	/^import com.google.protobuf.ByteString$/;"	i
com.google.protobuf.ByteString	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^import com.google.protobuf.ByteString$/;"	i
com.ning.compress.lzf.{LZFInputStream, LZFOutputStream}	io/CompressionCodec.scala	/^import com.ning.compress.lzf.{LZFInputStream, LZFOutputStream}$/;"	i
com.twitter.chill.{EmptyScalaKryoInstantiator, AllScalaRegistrar}	serializer/KryoSerializer.scala	/^import com.twitter.chill.{EmptyScalaKryoInstantiator, AllScalaRegistrar}$/;"	i
com.typesafe.config.ConfigFactory	util/AkkaUtils.scala	/^import com.typesafe.config.ConfigFactory$/;"	i
combineByKey	api/java/JavaPairRDD.scala	/^  def combineByKey[C](createCombiner: JFunction[V, C],$/;"	m
combineByKey	rdd/PairRDDFunctions.scala	/^  def combineByKey[C](createCombiner: V => C, mergeValue: (C, V) => C, mergeCombiners: (C, C) => C)$/;"	m
combineByKey	rdd/PairRDDFunctions.scala	/^  def combineByKey[C](createCombiner: V => C,$/;"	m
combineCombinersByKey	Aggregator.scala	/^  def combineCombinersByKey(iter: Iterator[(K, C)]) : Iterator[(K, C)] = {$/;"	m
combineValuesByKey	Aggregator.scala	/^  def combineValuesByKey(iter: Iterator[_ <: Product2[K, V]]) : Iterator[(K, C)] = {$/;"	m
combined	rdd/PairRDDFunctions.scala	/^      val combined = self.mapPartitions(aggregator.combineValuesByKey, preservesPartitioning = true)$/;"	V
combiners	Aggregator.scala	/^    val combiners = new AppendOnlyMap[K, C]$/;"	V
command	deploy/ApplicationDescription.scala	/^    val command: Command,$/;"	V
command	deploy/worker/ExecutorRunner.scala	/^      val command = buildCommandSeq()$/;"	V
command	deploy/worker/ExecutorRunner.scala	/^    val command = appDesc.command$/;"	V
command	scheduler/cluster/SparkDeploySchedulerBackend.scala	/^    val command = Command($/;"	V
command	scheduler/cluster/mesos/CoarseMesosSchedulerBackend.scala	/^    val command = CommandInfo.newBuilder()$/;"	V
command	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^    val command = CommandInfo.newBuilder()$/;"	V
commit	storage/BlockObjectWriter.scala	/^  def commit(): Long$/;"	m
committer	SparkHadoopWriter.scala	/^  @transient private var committer: OutputCommitter = null$/;"	v
committer	rdd/PairRDDFunctions.scala	/^      val committer = format.getOutputCommitter(hadoopContext)$/;"	V
communicate	MapOutputTracker.scala	/^  def communicate(message: Any) {$/;"	m
comp	api/java/JavaPairRDD.scala	/^    val comp = com.google.common.collect.Ordering.natural().asInstanceOf[Comparator[K]]$/;"	V
comp	api/java/JavaRDDLike.scala	/^    val comp = com.google.common.collect.Ordering.natural().asInstanceOf[Comparator[T]]$/;"	V
comparator	scheduler/SchedulingAlgorithm.scala	/^  def comparator(s1: Schedulable, s2: Schedulable): Boolean$/;"	m
compare	rdd/CoalescedRDD.scala	/^  def compare(o1: Option[PartitionGroup], o2: Option[PartitionGroup]): Boolean =$/;"	m
compare	rdd/CoalescedRDD.scala	/^  def compare(o1: PartitionGroup, o2: PartitionGroup): Boolean = o1.size < o2.size$/;"	m
compare	scheduler/SchedulingAlgorithm.scala	/^    var compare:Int = 0$/;"	v
compareTo	broadcast/SourceInfo.scala	/^  def compareTo (o: SourceInfo): Int = (currentLeechers - o.currentLeechers)$/;"	m
completeWidth	ui/jobs/StageTable.scala	/^    val completeWidth = "width: %s%%".format((completed.toDouble\/total)*100)$/;"	V
completedApps	deploy/master/Master.scala	/^  val completedApps = new ArrayBuffer[ApplicationInfo]$/;"	V
completedApps	deploy/master/ui/IndexPage.scala	/^    val completedApps = state.completedApps.sortBy(_.endTime).reverse$/;"	V
completedAppsTable	deploy/master/ui/IndexPage.scala	/^    val completedAppsTable = UIUtils.listingTable(appHeaders, appRow, completedApps)$/;"	V
completedStages	ui/jobs/IndexPage.scala	/^      val completedStages = listener.completedStages.reverse.toSeq$/;"	V
completedStages	ui/jobs/JobProgressListener.scala	/^  val completedStages = ListBuffer[StageInfo]()$/;"	V
completedStagesTable	ui/jobs/IndexPage.scala	/^      val completedStagesTable = new StageTable(completedStages.sortBy(_.submissionTime).reverse, parent)$/;"	V
completedTasks	ui/exec/ExecutorsUI.scala	/^    val completedTasks = listener.executorToTasksComplete.getOrElse(execId, 0)$/;"	V
completedTasks	ui/jobs/StageTable.scala	/^    val completedTasks = listener.stageIdToTasksComplete.getOrElse(s.stageId, 0)$/;"	V
completion	util/CompletionIterator.scala	/^      def completion() = completionFunction$/;"	m
completionHandler	partial/PartialResult.scala	/^  private var completionHandler: Option[R => Unit] = None$/;"	v
completionIter	BlockStoreShuffleFetcher.scala	/^    val completionIter = CompletionIterator[T, Iterator[T]](itr, {$/;"	V
completionTime	scheduler/StageInfo.scala	/^  var completionTime: Option[Long] = None$/;"	v
compress	broadcast/HttpBroadcast.scala	/^  private var compress: Boolean = false$/;"	v
compressBroadcast	storage/BlockManager.scala	/^  val compressBroadcast = System.getProperty("spark.broadcast.compress", "true").toBoolean$/;"	V
compressRdds	storage/BlockManager.scala	/^  val compressRdds = System.getProperty("spark.rdd.compress", "false").toBoolean$/;"	V
compressShuffle	storage/BlockManager.scala	/^  val compressShuffle = System.getProperty("spark.shuffle.compress", "true").toBoolean$/;"	V
compressSize	MapOutputTracker.scala	/^  def compressSize(size: Long): Byte = {$/;"	m
compressStream	storage/BlockManager.scala	/^    val compressStream: OutputStream => OutputStream = wrapForCompression(blockId, _)$/;"	V
compressedInputStream	io/CompressionCodec.scala	/^  def compressedInputStream(s: InputStream): InputStream$/;"	m
compressedOutputStream	io/CompressionCodec.scala	/^  def compressedOutputStream(s: OutputStream): OutputStream$/;"	m
compressedSizes	scheduler/ShuffleMapTask.scala	/^      val compressedSizes: Array[Byte] = shuffle.writers.map { writer: BlockObjectWriter =>$/;"	V
compressionCodec	broadcast/HttpBroadcast.scala	/^  private lazy val compressionCodec = CompressionCodec.createCodec()$/;"	V
compressionCodec	storage/BlockManager.scala	/^  private lazy val compressionCodec: CompressionCodec = CompressionCodec.createCodec()$/;"	V
compute	rdd/RDD.scala	/^  def compute(split: Partition, context: TaskContext): Iterator[T]$/;"	m
computeNextPair	util/collection/OpenHashMap.scala	/^    def computeNextPair(): (K, V) = {$/;"	m
computeNextPair	util/collection/PrimitiveKeyOpenHashMap.scala	/^    def computeNextPair(): (K, V) = {$/;"	m
computePreferredLocations	scheduler/InputFormatInfo.scala	/^  def computePreferredLocations(formats: Seq[InputFormatInfo]): HashMap[String, HashSet[SplitInfo]] = {$/;"	m
computedValues	CacheManager.scala	/^          val computedValues = rdd.computeOrReadCheckpoint(split, context)$/;"	V
conf	SparkContext.scala	/^    val conf = SparkHadoopUtil.get.newConfiguration()$/;"	V
conf	SparkHadoopWriter.scala	/^  private val conf = new SerializableWritable(jobConf)$/;"	V
conf	deploy/SparkHadoopUtil.scala	/^  val conf = newConfiguration()$/;"	V
conf	rdd/HadoopRDD.scala	/^    val conf: Configuration = broadcastedConf.value.value$/;"	V
conf	rdd/NewHadoopRDD.scala	/^      val conf = confBroadcast.value.value$/;"	V
conf	scheduler/InputFormatInfo.scala	/^    val conf = new JobConf(configuration)$/;"	V
conf	scheduler/cluster/SimrSchedulerBackend.scala	/^    val conf = new Configuration()$/;"	V
conf	util/Utils.scala	/^        val conf = SparkHadoopUtil.get.newConfiguration()$/;"	V
confBroadcast	SparkContext.scala	/^    val confBroadcast = broadcast(new SerializableWritable(hadoopConfiguration))$/;"	V
confBroadcast	rdd/NewHadoopRDD.scala	/^  private val confBroadcast = sc.broadcast(new SerializableWritable(conf))$/;"	V
confFactor	partial/CountEvaluator.scala	/^      val confFactor = Probability.normalInverse(1 - (1 - confidence) \/ 2)$/;"	V
confFactor	partial/GroupedCountEvaluator.scala	/^      val confFactor = Probability.normalInverse(1 - (1 - confidence) \/ 2)$/;"	V
confFactor	partial/GroupedMeanEvaluator.scala	/^        val confFactor = studentTCacher.get(counter.count)$/;"	V
confFactor	partial/GroupedSumEvaluator.scala	/^        val confFactor = studentTCacher.get(counter.count)$/;"	V
confFactor	partial/MeanEvaluator.scala	/^      val confFactor = {$/;"	V
confFactor	partial/SumEvaluator.scala	/^      val confFactor = {$/;"	V
confFile	metrics/MetricsSystem.scala	/^  val confFile = System.getProperty("spark.metrics.conf")$/;"	V
configFile	metrics/MetricsConfig.scala	/^private[spark] class MetricsConfig(val configFile: Option[String]) extends Logging {$/;"	V
configuration	scheduler/InputFormatInfo.scala	/^class InputFormatInfo(val configuration: Configuration, val inputFormatClazz: Class[_], $/;"	V
conn	network/ConnectionManager.scala	/^          val conn: SendingConnection = registerRequests.dequeue$/;"	V
conn	network/ConnectionManager.scala	/^    val conn = connectionsByKey.getOrElse(key, null)$/;"	V
conn	network/ConnectionManager.scala	/^    val conn = connectionsByKey.getOrElse(key, null).asInstanceOf[SendingConnection]$/;"	V
conn	rdd/JdbcRDD.scala	/^    val conn = getConnection()$/;"	V
connManager	network/ConnectionManagerTest.scala	/^        val connManager = SparkEnv.get.connectionManager$/;"	V
connect	ui/JettyUtils.scala	/^    def connect(currentPort: Int): (Server, Int) = {$/;"	m
connectTimeout	network/netty/ShuffleCopier.scala	/^    val connectTimeout = System.getProperty("spark.shuffle.netty.connect.timeout", "60000").toInt$/;"	V
connected	deploy/client/ClientListener.scala	/^  def connected(appId: String): Unit$/;"	m
connected	deploy/client/TestClient.scala	/^    def connected(id: String) {$/;"	m
connected	deploy/worker/Worker.scala	/^  @volatile var connected = false$/;"	v
connected	network/Connection.scala	/^      val connected = channel.finishConnect$/;"	V
connection	network/ConnectionManager.scala	/^              val connection = connectionsByKey.getOrElse(key, null)$/;"	V
connection	network/ConnectionManager.scala	/^    val connection = connectionsById.getOrElseUpdate(connectionManagerId, startNewConnection())$/;"	V
connectionManager	SparkEnv.scala	/^    val connectionManager = blockManager.connectionManager$/;"	V
connectionManager	SparkEnv.scala	/^    val connectionManager: ConnectionManager,$/;"	V
connectionManager	storage/BlockManager.scala	/^  val connectionManager = new ConnectionManager(0)$/;"	V
connectionManager	storage/BlockManagerWorker.scala	/^    val connectionManager = blockManager.connectionManager$/;"	V
connectionManagerId	network/ConnectionManager.scala	/^      val connectionManagerId: ConnectionManagerId,$/;"	V
connectionManagerId	network/ConnectionManager.scala	/^    val connectionManagerId = ConnectionManagerId.fromSocketAddress(message.senderAddress)$/;"	V
connections	network/ConnectionManager.scala	/^    val connections = connectionsByKey.values$/;"	V
connectionsById	network/ConnectionManager.scala	/^  private val connectionsById = new HashMap[ConnectionManagerId, SendingConnection] with SynchronizedMap[ConnectionManagerId, SendingConnection]$/;"	V
connectionsByKey	network/ConnectionManager.scala	/^  private val connectionsByKey = new HashMap[SelectionKey, Connection] with SynchronizedMap[SelectionKey, Connection]$/;"	V
connector	HttpServer.scala	/^      val connector = new SocketConnector$/;"	V
cons	SparkContext.scala	/^          val cons = clazz.getConstructor(classOf[ClusterScheduler], classOf[SparkContext])$/;"	V
cons	SparkContext.scala	/^          val cons = clazz.getConstructor(classOf[SparkContext])$/;"	V
cons	util/ClosureCleaner.scala	/^      val cons = cls.getConstructors()(0)$/;"	V
consolidateShuffleFiles	storage/ShuffleBlockManager.scala	/^  val consolidateShuffleFiles =$/;"	V
constructor	executor/Executor.scala	/^        val constructor = klass.getConstructor(classOf[String], classOf[ClassLoader])$/;"	V
consumers	storage/ThreadingTest.scala	/^    val consumers = producers.map(p => new ConsumerThread(blockManager, p.queue))$/;"	V
containerList	scheduler/cluster/ClusterScheduler.scala	/^        val containerList: ArrayBuffer[T] = map.get(key).getOrElse(null)$/;"	V
containerSparkHome	deploy/FaultToleranceTest.scala	/^  val containerSparkHome = "\/opt\/spark"$/;"	V
contains	storage/BlockStore.scala	/^  def contains(blockId: BlockId): Boolean$/;"	m
contains	util/TimeStampedHashSet.scala	/^  def contains(key: A): Boolean = {$/;"	m
contains	util/collection/OpenHashSet.scala	/^  def contains(k: T): Boolean = getPos(k) != INVALID_POS$/;"	m
containsCachedMetadata	rdd/HadoopRDD.scala	/^  def containsCachedMetadata(key: String) = SparkEnv.get.hadoopJobMetadata.containsKey(key)$/;"	m
content	deploy/master/ui/ApplicationPage.scala	/^    val content =$/;"	V
content	deploy/master/ui/IndexPage.scala	/^    val content =$/;"	V
content	deploy/worker/ui/IndexPage.scala	/^    val content =$/;"	V
content	deploy/worker/ui/WorkerWebUI.scala	/^    val content =$/;"	V
content	ui/env/EnvironmentUI.scala	/^    val content =$/;"	V
content	ui/exec/ExecutorsUI.scala	/^    val content =$/;"	V
content	ui/jobs/IndexPage.scala	/^      val content = summary ++$/;"	V
content	ui/jobs/PoolPage.scala	/^      val content = <h4>Summary <\/h4> ++ poolTable.toNodeSeq() ++$/;"	V
content	ui/jobs/StagePage.scala	/^        val content =$/;"	V
content	ui/jobs/StagePage.scala	/^      val content =$/;"	V
content	ui/storage/IndexPage.scala	/^    val content = listingTable(rddHeaders, rddRow, rdds)$/;"	V
content	ui/storage/RDDPage.scala	/^    val content =$/;"	V
context	InterruptibleIterator.scala	/^class InterruptibleIterator[+T](val context: TaskContext, val delegate: Iterator[T])$/;"	V
context	api/java/JavaRDDLike.scala	/^  def context: SparkContext = rdd.context$/;"	m
context	rdd/RDD.scala	/^  def context = sc$/;"	m
context	scheduler/Task.scala	/^  @transient protected var context: TaskContext = _$/;"	v
context.dispatcher	storage/BlockManagerMasterActor.scala	/^    import context.dispatcher$/;"	i
contextHandler	ui/JettyUtils.scala	/^      val contextHandler = new ContextHandler(path)$/;"	V
convert	SparkContext.scala	/^    val convert: Writable => T)$/;"	V
convertKey	rdd/SequenceFileRDDFunctions.scala	/^    val convertKey = !classOf[Writable].isAssignableFrom(self.getKeyClass)$/;"	V
convertValue	rdd/SequenceFileRDDFunctions.scala	/^    val convertValue = !classOf[Writable].isAssignableFrom(self.getValueClass)$/;"	V
copier	network/netty/ShuffleCopier.scala	/^          val copier = new ShuffleCopier()$/;"	V
copier	storage/BlockFetcherIterator.scala	/^        val copier = new Thread {$/;"	V
copiers	network/netty/ShuffleCopier.scala	/^    val copiers = Executors.newFixedThreadPool(80)$/;"	V
copiers	storage/BlockFetcherIterator.scala	/^    private var copiers: List[_ <: Thread] = null$/;"	v
copiesRunning	scheduler/cluster/ClusterTaskSetManager.scala	/^  val copiesRunning = new Array[Int](numTasks)$/;"	V
copiesRunning	scheduler/local/LocalTaskSetManager.scala	/^  val copiesRunning = new Array[Int](numTasks)$/;"	V
copy	Accumulators.scala	/^    val copy = ser.deserialize[R](ser.serialize(initialValue))$/;"	V
copy	util/StatCounter.scala	/^  def copy(): StatCounter = {$/;"	m
copyForMemory	storage/BlockManager.scala	/^              val copyForMemory = ByteBuffer.allocate(bytes.limit)$/;"	V
copyState	deploy/master/ExecutorInfo.scala	/^  def copyState(execDesc: ExecutorDescription) {$/;"	m
copyStream	util/Utils.scala	/^  def copyStream(in: InputStream,$/;"	m
cores	deploy/ExecutorDescription.scala	/^    val cores: Int,$/;"	V
cores	deploy/master/ExecutorInfo.scala	/^    val cores: Int,$/;"	V
cores	deploy/master/WorkerInfo.scala	/^    val cores: Int,$/;"	V
cores	deploy/worker/ExecutorRunner.scala	/^    val cores: Int,$/;"	V
cores	deploy/worker/WorkerArguments.scala	/^  var cores = inferDefaultCores()$/;"	v
coresByTaskId	scheduler/cluster/mesos/CoarseMesosSchedulerBackend.scala	/^  val coresByTaskId = new HashMap[Int, Int]$/;"	V
coresFree	deploy/master/WorkerInfo.scala	/^  def coresFree: Int = cores - coresUsed$/;"	m
coresFree	deploy/worker/Worker.scala	/^  def coresFree: Int = cores - coresUsed$/;"	m
coresGranted	deploy/master/ApplicationInfo.scala	/^  @transient var coresGranted: Int = _$/;"	v
coresLeft	deploy/master/ApplicationInfo.scala	/^  def coresLeft: Int = desc.maxCores - coresGranted$/;"	m
coresToUse	deploy/master/Master.scala	/^            val coresToUse = math.min(worker.coresFree, app.coresLeft)$/;"	V
coresUsed	deploy/master/WorkerInfo.scala	/^  @transient var coresUsed: Int = _$/;"	v
coresUsed	deploy/worker/Worker.scala	/^  var coresUsed = 0$/;"	v
count	api/java/JavaRDDLike.scala	/^  def count(): Long = rdd.count()$/;"	m
count	network/ConnectionManager.scala	/^    val count = 10$/;"	V
count	network/ConnectionManagerTest.scala	/^    val count = if (args.length > 4) args(4).toInt else 3$/;"	V
count	network/SenderTest.scala	/^    val count = 100$/;"	V
count	rdd/PairRDDFunctions.scala	/^      var count = 0$/;"	v
count	rdd/PairRDDFunctions.scala	/^    val count = self.context.runJob(self, writeShard _).sum$/;"	V
count	rdd/RDD.scala	/^  def count(): Long = {$/;"	m
count	rdd/SampledRDD.scala	/^        val count = poisson.nextInt()$/;"	V
count	util/StatCounter.scala	/^  def count: Long = n$/;"	m
countApprox	api/java/JavaRDDLike.scala	/^  def countApprox(timeout: Long): PartialResult[BoundedDouble] =$/;"	m
countApprox	api/java/JavaRDDLike.scala	/^  def countApprox(timeout: Long, confidence: Double): PartialResult[BoundedDouble] =$/;"	m
countApprox	rdd/RDD.scala	/^  def countApprox(timeout: Long, confidence: Double = 0.95): PartialResult[BoundedDouble] = {$/;"	m
countAsync	rdd/AsyncRDDActions.scala	/^  def countAsync(): FutureAction[Long] = {$/;"	m
countByKey	api/java/JavaPairRDD.scala	/^  def countByKey(): java.util.Map[K, Long] = mapAsJavaMap(rdd.countByKey())$/;"	m
countByKey	rdd/PairRDDFunctions.scala	/^  def countByKey(): Map[K, Long] = self.map(_._1).countByValue()$/;"	m
countByKeyApprox	api/java/JavaPairRDD.scala	/^  def countByKeyApprox(timeout: Long): PartialResult[java.util.Map[K, BoundedDouble]] =$/;"	m
countByKeyApprox	api/java/JavaPairRDD.scala	/^  def countByKeyApprox(timeout: Long, confidence: Double = 0.95)$/;"	m
countByKeyApprox	rdd/PairRDDFunctions.scala	/^  def countByKeyApprox(timeout: Long, confidence: Double = 0.95)$/;"	m
countByValue	api/java/JavaRDDLike.scala	/^  def countByValue(): java.util.Map[T, java.lang.Long] =$/;"	m
countByValue	rdd/RDD.scala	/^  def countByValue(): Map[T, Long] = {$/;"	m
countByValueApprox	api/java/JavaRDDLike.scala	/^  def countByValueApprox(timeout: Long): PartialResult[java.util.Map[T, BoundedDouble]] =$/;"	m
countElements	rdd/RDD.scala	/^    val countElements: (TaskContext, Iterator[T]) => Long = { (ctx, iter) =>$/;"	V
countEstimate	partial/GroupedSumEvaluator.scala	/^        val countEstimate = (counter.count + 1 - p) \/ p$/;"	V
countEstimate	partial/SumEvaluator.scala	/^      val countEstimate = (counter.count + 1 - p) \/ p$/;"	V
countPartition	rdd/RDD.scala	/^    def countPartition(iter: Iterator[T]): Iterator[OLMap[T]] = {$/;"	m
countPartition	rdd/RDD.scala	/^    val countPartition: (TaskContext, Iterator[T]) => OLMap[T] = { (ctx, iter) =>$/;"	V
countVar	partial/GroupedSumEvaluator.scala	/^        val countVar = (counter.count + 1) * (1 - p) \/ (p * p)$/;"	V
countVar	partial/SumEvaluator.scala	/^      val countVar = (counter.count + 1) * (1 - p) \/ (p * p)$/;"	V
counter	partial/GroupedMeanEvaluator.scala	/^        val counter = entry.getValue$/;"	V
counter	partial/GroupedSumEvaluator.scala	/^        val counter = entry.getValue$/;"	V
counter	partial/MeanEvaluator.scala	/^  var counter = new StatCounter$/;"	v
counter	partial/SumEvaluator.scala	/^  var counter = new StatCounter$/;"	v
cpFile	rdd/RDDCheckpointData.scala	/^  @transient var cpFile: Option[String] = None$/;"	v
cpRDD	rdd/CheckpointRDD.scala	/^    val cpRDD = new CheckpointRDD[Int](sc, path.toString)$/;"	V
cpRDD	rdd/RDDCheckpointData.scala	/^  var cpRDD: Option[RDD[T]] = None$/;"	v
cpState	rdd/RDDCheckpointData.scala	/^  var cpState = Initialized$/;"	v
cpath	rdd/CheckpointRDD.scala	/^    val cpath = new Path(checkpointPath)$/;"	V
cpier	storage/BlockFetcherIterator.scala	/^      val cpier = new ShuffleCopier$/;"	V
cpuResource	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^    val cpuResource = Resource.newBuilder()$/;"	V
cpus	scheduler/cluster/mesos/CoarseMesosSchedulerBackend.scala	/^        val cpus = getResource(offer.getResourcesList, "cpus").toInt$/;"	V
cpusToUse	scheduler/cluster/mesos/CoarseMesosSchedulerBackend.scala	/^          val cpusToUse = math.min(cpus, maxCores - totalCoresAcquired)$/;"	V
cr	util/ClosureCleaner.scala	/^      val cr = getClassReader(stack.head)$/;"	V
create	api/java/StorageLevels.java	/^  public static StorageLevel create(boolean useDisk, boolean useMemory, boolean deserialized, int replication) {$/;"	m	class:StorageLevels
create	api/python/PythonWorkerFactory.scala	/^  def create(): Socket = {$/;"	m
create	deploy/master/SparkZooKeeperSession.scala	/^  def create(path: String, bytes: Array[Byte], createMode: CreateMode): String = {$/;"	m
create	network/Message.scala	/^  def create(header: MessageChunkHeader): Message = {$/;"	m
create	network/MessageChunkHeader.scala	/^  def create(buffer: ByteBuffer): MessageChunkHeader = {$/;"	m
create	network/netty/FileHeader.scala	/^  def create(buf: ByteBuf): FileHeader = {$/;"	m
create	rdd/PartitionPruningRDD.scala	/^  def create[T](rdd: RDD[T], partitionFilterFunc: Int => Boolean) = {$/;"	m
createActorSystem	util/AkkaUtils.scala	/^  def createActorSystem(name: String, host: String, port: Int): (ActorSystem, Int) = {$/;"	m
createApplication	deploy/master/Master.scala	/^  def createApplication(desc: ApplicationDescription, driver: ActorRef): ApplicationInfo = {$/;"	m
createBufferMessage	network/Message.scala	/^  def createBufferMessage(ackId: Int): BufferMessage = {$/;"	m
createBufferMessage	network/Message.scala	/^  def createBufferMessage(dataBuffer: ByteBuffer): BufferMessage =$/;"	m
createBufferMessage	network/Message.scala	/^  def createBufferMessage(dataBuffer: ByteBuffer, ackId: Int): BufferMessage = {$/;"	m
createBufferMessage	network/Message.scala	/^  def createBufferMessage(dataBuffers: Seq[ByteBuffer]): BufferMessage =$/;"	m
createBufferMessage	network/Message.scala	/^  def createBufferMessage(dataBuffers: Seq[ByteBuffer], ackId: Int): BufferMessage = {$/;"	m
createClient	deploy/FaultToleranceTest.scala	/^  def createClient() = {$/;"	m
createCodec	io/CompressionCodec.scala	/^  def createCodec(): CompressionCodec = {$/;"	m
createCodec	io/CompressionCodec.scala	/^  def createCodec(codecName: String): CompressionCodec = {$/;"	m
createCombiner	rdd/PairRDDFunctions.scala	/^    def createCombiner(v: V) = ArrayBuffer(v)$/;"	m
createCommand	scheduler/cluster/mesos/CoarseMesosSchedulerBackend.scala	/^  def createCommand(offer: Offer, numCores: Int): CommandInfo = {$/;"	m
createExecutorInfo	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^  def createExecutorInfo(execId: String): ExecutorInfo = {$/;"	m
createHandler	ui/JettyUtils.scala	/^  def createHandler[T <% AnyRef](responder: Responder[T], contentType: String,$/;"	m
createJobID	SparkHadoopWriter.scala	/^  def createJobID(time: Date, id: Int): JobID = {$/;"	m
createMesosTask	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^  def createMesosTask(task: TaskDescription, slaveId: String): MesosTaskInfo = {$/;"	m
createMetricsSystem	metrics/MetricsSystem.scala	/^  def createMetricsSystem(instance: String): MetricsSystem = new MetricsSystem(instance)$/;"	m
createNewMessage	network/Connection.scala	/^      def createNewMessage: BufferMessage = {$/;"	m
createPathFromString	SparkHadoopWriter.scala	/^  def createPathFromString(path: String, conf: JobConf): Path = {$/;"	m
createPythonWorker	SparkEnv.scala	/^  def createPythonWorker(pythonExec: String, envVars: Map[String, String]): java.net.Socket = {$/;"	m
createRedirectHandler	ui/JettyUtils.scala	/^  def createRedirectHandler(newPath: String): Handler = {$/;"	m
createStaticHandler	ui/JettyUtils.scala	/^  def createStaticHandler(resourceBase: String): ResourceHandler = {$/;"	m
createTempDir	util/Utils.scala	/^  def createTempDir(root: String = System.getProperty("java.io.tmpdir")): File = {$/;"	m
createZero	rdd/PairRDDFunctions.scala	/^    def createZero() = cachedSerializer.deserialize[V](ByteBuffer.wrap(zeroArray))$/;"	m
created	deploy/master/FileSystemPersistenceEngine.scala	/^    val created = file.createNewFile()$/;"	V
creationTime	network/ConnectionManager.scala	/^      val creationTime = System.currentTimeMillis$/;"	V
curBlocks	storage/BlockFetcherIterator.scala	/^          var curBlocks = new ArrayBuffer[(BlockId, Long)]$/;"	v
curKey	util/AppendOnlyMap.scala	/^          val curKey = newData(2 * newPos)$/;"	V
curKey	util/AppendOnlyMap.scala	/^      val curKey = data(2 * pos)$/;"	V
curMax	broadcast/BitTorrentBroadcast.scala	/^      var curMax = 0$/;"	v
curPeer	broadcast/BitTorrentBroadcast.scala	/^              var curPeer = peerIter.next$/;"	v
curPeer	broadcast/BitTorrentBroadcast.scala	/^      var curPeer: SourceInfo = null$/;"	v
curPosition	network/Connection.scala	/^    val curPosition = buffer.position$/;"	V
curRequestSize	storage/BlockFetcherIterator.scala	/^          var curRequestSize = 0L$/;"	v
curSize	util/AppendOnlyMap.scala	/^  private var curSize = 0$/;"	v
curTime	broadcast/BitTorrentBroadcast.scala	/^          var curTime = startTime$/;"	v
curTime	scheduler/cluster/ClusterTaskSetManager.scala	/^      val curTime = clock.getTime()$/;"	V
curWord	util/Utils.scala	/^    var curWord = new StringBuilder$/;"	v
currId	network/Connection.scala	/^    val currId = inferredRemoteManagerId$/;"	V
currPrefLocs	rdd/CoalescedRDD.scala	/^  def currPrefLocs(part: Partition): Seq[String] = {$/;"	m
currSplit	rdd/CartesianRDD.scala	/^    val currSplit = split.asInstanceOf[CartesianPartition]$/;"	V
currentBuffers	network/Connection.scala	/^  val currentBuffers = new ArrayBuffer[ByteBuffer]()$/;"	V
currentChunk	network/Connection.scala	/^  var currentChunk: MessageChunk = null$/;"	v
currentEnvVars	rdd/PipedRDD.scala	/^    val currentEnvVars = pb.environment()$/;"	V
currentEpoch	scheduler/DAGScheduler.scala	/^    val currentEpoch = maybeEpoch.getOrElse(mapOutputTracker.getEpoch)$/;"	V
currentFiles	executor/Executor.scala	/^  private val currentFiles: HashMap[String, Long] = new HashMap[String, Long]()$/;"	V
currentFiles	scheduler/local/LocalScheduler.scala	/^  val currentFiles: HashMap[String, Long] = new HashMap[String, Long]()$/;"	V
currentJars	executor/Executor.scala	/^  private val currentJars: HashMap[String, Long] = new HashMap[String, Long]()$/;"	V
currentJars	scheduler/local/LocalScheduler.scala	/^  val currentJars: HashMap[String, Long] = new HashMap[String, Long]()$/;"	V
currentLeechers	broadcast/SourceInfo.scala	/^  var currentLeechers = 0$/;"	v
currentLocalityIndex	scheduler/cluster/ClusterTaskSetManager.scala	/^  var currentLocalityIndex = 0    \/\/ Index of our current locality level in validLocalityLevels$/;"	v
currentMemory	storage/MemoryStore.scala	/^  @volatile private var currentMemory = 0L$/;"	v
currentResult	partial/ApproximateEvaluator.scala	/^  def currentResult(): R$/;"	m
currentSize	network/BufferMessage.scala	/^  def currentSize() = {$/;"	m
currentTime	deploy/master/Master.scala	/^    val currentTime = System.currentTimeMillis()$/;"	V
currentUser	deploy/SparkHadoopUtil.scala	/^    val currentUser = Option(System.getProperty("user.name")).$/;"	V
customHostname	util/Utils.scala	/^  private var customHostname: Option[String] = None$/;"	v
daemon	api/python/PythonWorkerFactory.scala	/^  var daemon: Process = null$/;"	v
daemonHost	api/python/PythonWorkerFactory.scala	/^  val daemonHost = InetAddress.getByAddress(Array(127, 0, 0, 1))$/;"	V
daemonPort	api/python/PythonWorkerFactory.scala	/^  var daemonPort: Int = 0$/;"	v
daemonThreadFactoryBuilder	util/Utils.scala	/^  private val daemonThreadFactoryBuilder: ThreadFactoryBuilder =$/;"	V
dagScheduler	SparkContext.scala	/^  @volatile private[spark] var dagScheduler = new DAGScheduler(taskScheduler)$/;"	v
dagScheduler	scheduler/DAGSchedulerSource.scala	/^private[spark] class DAGSchedulerSource(val dagScheduler: DAGScheduler, sc: SparkContext)$/;"	V
dagScheduler	scheduler/cluster/ClusterScheduler.scala	/^  var dagScheduler: DAGScheduler = null$/;"	v
dagScheduler	scheduler/local/LocalScheduler.scala	/^  var dagScheduler: DAGScheduler = null$/;"	v
dagSchedulerCopy	SparkContext.scala	/^    val dagSchedulerCopy = dagScheduler$/;"	V
dagSchedulerSource	SparkContext.scala	/^  private val dagSchedulerSource = new DAGSchedulerSource(this.dagScheduler, this)$/;"	V
data	rdd/PairRDDFunctions.scala	/^    val data = self.toArray()$/;"	V
data	storage/BlockManager.scala	/^      val data = BlockManagerWorker.syncGetBlock($/;"	V
data	storage/BlockMessage.scala	/^  private var data: ByteBuffer = null$/;"	v
data	storage/MemoryStore.scala	/^            val data = if (entry.deserialized) {$/;"	V
data	storage/MemoryStore.scala	/^        val data = if (deserialized) {$/;"	V
data	util/AppendOnlyMap.scala	/^  private var data = new Array[AnyRef](2 * capacity)$/;"	v
data	util/Distribution.scala	/^class Distribution(val data: Array[Double], val startIdx: Int, val endIdx: Int) {$/;"	V
dataIn	scheduler/Task.scala	/^    val dataIn = new DataInputStream(in)$/;"	V
dataLength	storage/BlockMessage.scala	/^      val dataLength = buffer.getInt()$/;"	V
dataMessage	network/SenderTest.scala	/^      val dataMessage = Message.createBufferMessage(buffer.duplicate)$/;"	V
dataOut	api/python/PythonRDD.scala	/^          val dataOut = new DataOutputStream(stream)$/;"	V
dataOut	scheduler/Task.scala	/^    val dataOut = new DataOutputStream(out)$/;"	V
dataSizeMb	storage/StoragePerfTester.scala	/^    val dataSizeMb = Utils.memoryStringToMb(sys.env.getOrElse("OUTPUT_DATA", "1g"))$/;"	V
date	deploy/master/Master.scala	/^    val date = new Date(now)$/;"	V
date	scheduler/JobLogger.scala	/^      val date = new Date(System.currentTimeMillis())$/;"	V
dateFmt	ui/jobs/JobProgressUI.scala	/^  val dateFmt = new SimpleDateFormat("yyyy\/MM\/dd HH:mm:ss")$/;"	V
dateFmt	ui/jobs/StagePage.scala	/^  val dateFmt = parent.dateFmt$/;"	V
dateFmt	ui/jobs/StageTable.scala	/^  val dateFmt = parent.dateFmt$/;"	V
dateFormat	storage/DiskBlockManager.scala	/^    val dateFormat = new SimpleDateFormat("yyyyMMddHHmmss")$/;"	V
dead	deploy/client/ClientListener.scala	/^  def dead(): Unit$/;"	m
debugString	rdd/RDD.scala	/^    def debugString(rdd: RDD[_], prefix: String = ""): Seq[String] = {$/;"	m
decompressSize	MapOutputTracker.scala	/^  def decompressSize(compressedSize: Byte): Long = {$/;"	m
decreaseRunningTasks	scheduler/Pool.scala	/^  def decreaseRunningTasks(taskNum: Int) {$/;"	m
decreaseRunningTasks	scheduler/local/LocalTaskSetManager.scala	/^  def decreaseRunningTasks(taskNum: Int): Unit = {$/;"	m
default	serializer/SerializerManager.scala	/^  def default = _default$/;"	m
defaultBytes	deploy/worker/ui/WorkerWebUI.scala	/^    val defaultBytes = 100 * 1024$/;"	V
defaultChunkSize	network/Connection.scala	/^    val defaultChunkSize = 65536  \/\/32768 \/\/16384$/;"	V
defaultIpOverride	util/Utils.scala	/^    val defaultIpOverride = System.getenv("SPARK_LOCAL_IP")$/;"	V
defaultMinSplits	SparkContext.scala	/^  def defaultMinSplits: Int = math.min(defaultParallelism, 2)$/;"	m
defaultParallelism	SparkContext.scala	/^  def defaultParallelism: Int = taskScheduler.defaultParallelism$/;"	m
defaultParallelism	scheduler/TaskScheduler.scala	/^  def defaultParallelism(): Int$/;"	m
defaultParallelism	scheduler/cluster/SchedulerBackend.scala	/^  def defaultParallelism(): Int$/;"	m
defaultPartitioner	Partitioner.scala	/^  def defaultPartitioner(rdd: RDD[_], others: RDD[_]*): Partitioner = {$/;"	m
defaultProbabilities	util/Distribution.scala	/^  val defaultProbabilities = Array(0,0.25,0.5,0.75,1.0)$/;"	V
defaultProperty	metrics/MetricsConfig.scala	/^      val defaultProperty = propertyCategories(DEFAULT_PREFIX)$/;"	V
defaultSerializer	storage/BlockManager.scala	/^    val defaultSerializer: Serializer,$/;"	V
defaultWait	scheduler/cluster/ClusterTaskSetManager.scala	/^    val defaultWait = System.getProperty("spark.locality.wait", "3000")$/;"	V
delay	deploy/FaultToleranceTest.scala	/^  def delay(secs: Duration = 5.seconds) = Thread.sleep(secs.toMillis)$/;"	m
delaySeconds	util/MetadataCleaner.scala	/^  private val delaySeconds = MetadataCleaner.getDelaySeconds$/;"	V
delete	deploy/master/SparkZooKeeperSession.scala	/^  def delete(path: String, version: Int = -1): Unit = {$/;"	m
deleteRecursively	util/Utils.scala	/^  def deleteRecursively(file: File) {$/;"	m
delta	util/AppendOnlyMap.scala	/^            val delta = i$/;"	V
delta	util/AppendOnlyMap.scala	/^        val delta = i$/;"	V
delta	util/StatCounter.scala	/^        val delta = other.mu - mu$/;"	V
delta	util/StatCounter.scala	/^    val delta = value - mu$/;"	V
delta	util/collection/OpenHashSet.scala	/^        val delta = i$/;"	V
denom	scheduler/SparkListener.scala	/^    val denom = totalTime.toDouble$/;"	V
dep	scheduler/ShuffleMapTask.scala	/^      val dep = objIn.readObject().asInstanceOf[ShuffleDependency[_,_]]$/;"	V
dep	scheduler/ShuffleMapTask.scala	/^    var dep: ShuffleDependency[_,_],$/;"	v
depInfo	scheduler/JobLogger.scala	/^        val depInfo = "SHUFFLE_ID=" + shufDep.shuffleId$/;"	V
depRddDesc	scheduler/JobLogger.scala	/^        var depRddDesc: String = ""$/;"	v
depStageDesc	scheduler/JobLogger.scala	/^        var depStageDesc: String = ""$/;"	v
dependencies_	rdd/RDD.scala	/^  private var dependencies_ : Seq[Dependency[_]] = null$/;"	v
dependentStages	scheduler/DAGScheduler.scala	/^    val dependentStages = resultStageToJob.keys.filter(x => stageDependsOn(x, failedStage)).toSeq$/;"	V
deps	rdd/CoGroupedRDD.scala	/^class CoGroupPartition(idx: Int, val deps: Array[CoGroupSplitDep])$/;"	V
deps	rdd/RDD.scala	/^    @transient private var deps: Seq[Dependency[_]]$/;"	v
deps	rdd/UnionRDD.scala	/^    val deps = new ArrayBuffer[Dependency[_]]$/;"	V
dequeue	util/SizeEstimator.scala	/^    def dequeue(): AnyRef = {$/;"	m
desc	deploy/client/TestClient.scala	/^    val desc = new ApplicationDescription($/;"	V
desc	deploy/master/ApplicationInfo.scala	/^    val desc: ApplicationDescription,$/;"	V
description	scheduler/JobLogger.scala	/^      val description = properties.getProperty(SparkContext.SPARK_JOB_DESCRIPTION, "")$/;"	V
description	storage/StorageLevel.scala	/^  def description : String = {$/;"	m
description	ui/jobs/JobProgressListener.scala	/^    val description = Option(stageSubmitted.properties).flatMap {$/;"	V
description	ui/jobs/StageTable.scala	/^    val description = listener.stageIdToDescription.get(s.stageId)$/;"	V
deserialize	serializer/JavaSerializer.scala	/^  def deserialize[T](bytes: ByteBuffer): T = {$/;"	m
deserialize	serializer/JavaSerializer.scala	/^  def deserialize[T](bytes: ByteBuffer, loader: ClassLoader): T = {$/;"	m
deserialize	serializer/KryoSerializer.scala	/^  def deserialize[T](bytes: ByteBuffer): T = {$/;"	m
deserialize	serializer/KryoSerializer.scala	/^  def deserialize[T](bytes: ByteBuffer, loader: ClassLoader): T = {$/;"	m
deserialize	serializer/Serializer.scala	/^  def deserialize[T](bytes: ByteBuffer): T$/;"	m
deserialize	serializer/Serializer.scala	/^  def deserialize[T](bytes: ByteBuffer, loader: ClassLoader): T$/;"	m
deserialize	util/Utils.scala	/^  def deserialize[T](bytes: Array[Byte]): T = {$/;"	m
deserialize	util/Utils.scala	/^  def deserialize[T](bytes: Array[Byte], loader: ClassLoader): T = {$/;"	m
deserializeFileSet	scheduler/ShuffleMapTask.scala	/^  def deserializeFileSet(bytes: Array[Byte]) : HashMap[String, Long] = {$/;"	m
deserializeFromFile	deploy/master/FileSystemPersistenceEngine.scala	/^  def deserializeFromFile[T <: Serializable](file: File)(implicit m: Manifest[T]): T = {$/;"	m
deserializeFromFile	deploy/master/ZooKeeperPersistenceEngine.scala	/^  def deserializeFromFile[T <: Serializable](filename: String)(implicit m: Manifest[T]): T = {$/;"	m
deserializeInfo	scheduler/ResultTask.scala	/^  def deserializeInfo(stageId: Int, bytes: Array[Byte]): (RDD[_], (TaskContext, Iterator[_]) => _) = {$/;"	m
deserializeInfo	scheduler/ShuffleMapTask.scala	/^  def deserializeInfo(stageId: Int, bytes: Array[Byte]): (RDD[_], ShuffleDependency[_,_]) = {$/;"	m
deserializeLongValue	util/Utils.scala	/^  def deserializeLongValue(bytes: Array[Byte]) : Long = {$/;"	m
deserializeMany	serializer/Serializer.scala	/^  def deserializeMany(buffer: ByteBuffer): Iterator[Any] = {$/;"	m
deserializeStatuses	MapOutputTracker.scala	/^  def deserializeStatuses(bytes: Array[Byte]): Array[MapStatus] = {$/;"	m
deserializeStream	rdd/CheckpointRDD.scala	/^    val deserializeStream = serializer.deserializeStream(fileInputStream)$/;"	V
deserializeStream	serializer/JavaSerializer.scala	/^  def deserializeStream(s: InputStream): DeserializationStream = {$/;"	m
deserializeStream	serializer/JavaSerializer.scala	/^  def deserializeStream(s: InputStream, loader: ClassLoader): DeserializationStream = {$/;"	m
deserializeStream	serializer/KryoSerializer.scala	/^  def deserializeStream(s: InputStream): DeserializationStream = {$/;"	m
deserializeStream	serializer/Serializer.scala	/^  def deserializeStream(s: InputStream): DeserializationStream$/;"	m
deserializeViaNestedStream	util/Utils.scala	/^  def deserializeViaNestedStream(is: InputStream, ser: SerializerInstance)(f: DeserializationStream => Unit) = {$/;"	m
deserializeWithDependencies	scheduler/Task.scala	/^  def deserializeWithDependencies(serializedTask: ByteBuffer)$/;"	m
deserialized	Accumulators.scala	/^  var deserialized = false$/;"	v
deserialized	storage/StorageLevel.scala	/^  def deserialized = deserialized_$/;"	m
deserializedResult	scheduler/cluster/TaskResultGetter.scala	/^              val deserializedResult = serializer.get().deserialize[DirectTaskResult[_]]($/;"	V
deserialized_	storage/StorageLevel.scala	/^    private var deserialized_ : Boolean,$/;"	v
dir	deploy/master/FileSystemPersistenceEngine.scala	/^    val dir: String,$/;"	V
dir	scheduler/JobLogger.scala	/^    val dir = new File(logDir + "\/" + logDirName + "\/")$/;"	V
dir	util/Utils.scala	/^    var dir: File = null$/;"	v
dirContents	rdd/CheckpointRDD.scala	/^      val dirContents = fs.listStatus(cpath)$/;"	V
dirId	network/netty/ShuffleSender.scala	/^        val dirId = hash % localDirs.length$/;"	V
dirId	storage/DiskBlockManager.scala	/^    val dirId = hash % localDirs.length$/;"	V
directResult	executor/Executor.scala	/^        val directResult = new DirectTaskResult(value, accumUpdates, task.metrics.getOrElse(null))$/;"	V
dis	deploy/master/FileSystemPersistenceEngine.scala	/^    val dis = new DataInputStream(new FileInputStream(file))$/;"	V
disconnected	deploy/client/ClientListener.scala	/^  def disconnected(): Unit$/;"	m
diskBlockManager	storage/BlockManager.scala	/^  val diskBlockManager = new DiskBlockManager(shuffleBlockManager,$/;"	V
diskSize	storage/BlockManager.scala	/^          val diskSize = if (onDisk) diskStore.getSize(blockId) else 0L$/;"	V
diskSize	storage/BlockManagerMessages.scala	/^      var diskSize: Long)$/;"	v
diskSize	storage/StorageUtils.scala	/^      val diskSize = rddBlocks.map(_.diskSize).reduce(_ + _)$/;"	V
diskSpaceUsed	storage/BlockManagerSource.scala	/^      val diskSpaceUsed = storageStatusList$/;"	V
diskSpaceUsed	ui/exec/ExecutorsUI.scala	/^    val diskSpaceUsed = storageStatusList.flatMap(_.blocks.values.map(_.diskSize)).fold(0L)(_+_)$/;"	V
diskStore	storage/BlockManager.scala	/^  private[storage] val diskStore = new DiskStore(this, diskBlockManager)$/;"	V
diskUsed	storage/StorageUtils.scala	/^  def diskUsed() = blocks.values.map(_.diskSize).reduceOption(_+_).getOrElse(0L)$/;"	m
diskUsed	ui/exec/ExecutorsUI.scala	/^    val diskUsed = status.diskUsed().toString$/;"	V
diskUsedByRDD	storage/StorageUtils.scala	/^  def diskUsedByRDD(rddId: Int) =$/;"	m
dispose	storage/BlockManager.scala	/^  def dispose(buffer: ByteBuffer) {$/;"	m
dist	util/Vector.scala	/^  def dist(other: Vector): Double = math.sqrt(squaredDist(other))$/;"	m
distinct	api/java/JavaDoubleRDD.scala	/^  def distinct(): JavaDoubleRDD = fromRDD(srdd.distinct())$/;"	m
distinct	api/java/JavaDoubleRDD.scala	/^  def distinct(numPartitions: Int): JavaDoubleRDD = fromRDD(srdd.distinct(numPartitions))$/;"	m
distinct	api/java/JavaPairRDD.scala	/^  def distinct(): JavaPairRDD[K, V] = new JavaPairRDD[K, V](rdd.distinct())$/;"	m
distinct	api/java/JavaPairRDD.scala	/^  def distinct(numPartitions: Int): JavaPairRDD[K, V] = new JavaPairRDD[K, V](rdd.distinct(numPartitions))$/;"	m
distinct	api/java/JavaRDD.scala	/^  def distinct(): JavaRDD[T] = wrapRDD(rdd.distinct())$/;"	m
distinct	api/java/JavaRDD.scala	/^  def distinct(numPartitions: Int): JavaRDD[T] = wrapRDD(rdd.distinct(numPartitions))$/;"	m
distinct	rdd/RDD.scala	/^  def distinct(): RDD[T] = distinct(partitions.size)$/;"	m
distinct	rdd/RDD.scala	/^  def distinct(numPartitions: Int): RDD[T] =$/;"	m
divide	util/Vector.scala	/^  def divide (d: Double) = this \/ d$/;"	m
doCheckpointCalled	rdd/RDD.scala	/^  @transient private var doCheckpointCalled = false$/;"	v
dockerId	deploy/FaultToleranceTest.scala	/^    val dockerId = Docker.getLastProcessId$/;"	V
dockerMountDir	deploy/FaultToleranceTest.scala	/^  val dockerMountDir = "%s:%s".format(sparkHome, containerSparkHome)$/;"	V
dot	util/Vector.scala	/^  def dot(other: Vector): Double = {$/;"	m
doubleAccumulator	api/java/JavaSparkContext.scala	/^  def doubleAccumulator(initialValue: Double): Accumulator[java.lang.Double] =$/;"	m
drawn	util/SizeEstimator.scala	/^        val drawn = new IntOpenHashSet(ARRAY_SAMPLE_SIZE)$/;"	V
driver	deploy/master/ApplicationInfo.scala	/^    val driver: ActorRef,$/;"	V
driver	executor/CoarseGrainedExecutorBackend.scala	/^  var driver: ActorRef = null$/;"	v
driver	executor/MesosExecutorBackend.scala	/^  var driver: ExecutorDriver = null$/;"	v
driver	scheduler/cluster/mesos/CoarseMesosSchedulerBackend.scala	/^  var driver: SchedulerDriver = null$/;"	v
driver	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^  var driver: SchedulerDriver = null$/;"	v
driverActor	scheduler/cluster/CoarseGrainedSchedulerBackend.scala	/^  var driverActor: ActorRef = null$/;"	v
driverActor	storage/BlockManagerMaster.scala	/^private[spark] class BlockManagerMaster(var driverActor: ActorRef) extends Logging {$/;"	v
driverHost	SparkEnv.scala	/^        val driverHost: String = System.getProperty("spark.driver.host", "localhost")$/;"	V
driverPort	SparkEnv.scala	/^        val driverPort: Int = System.getProperty("spark.driver.port", "7077").toInt$/;"	V
driverSource	broadcast/BitTorrentBroadcast.scala	/^    val driverSource =$/;"	V
driverUrl	scheduler/cluster/SimrSchedulerBackend.scala	/^    val driverUrl = "akka:\/\/spark@%s:%s\/user\/%s".format($/;"	V
driverUrl	scheduler/cluster/SparkDeploySchedulerBackend.scala	/^    val driverUrl = "akka:\/\/spark@%s:%s\/user\/%s".format($/;"	V
driverUrl	scheduler/cluster/mesos/CoarseMesosSchedulerBackend.scala	/^    val driverUrl = "akka:\/\/spark@%s:%s\/user\/%s".format($/;"	V
dropFromMemory	storage/BlockManager.scala	/^  def dropFromMemory(blockId: BlockId, data: Either[ArrayBuffer[Any], ByteBuffer]) {$/;"	m
droppedMemorySize	storage/BlockManager.scala	/^        val droppedMemorySize = if (memoryStore.contains(blockId)) memoryStore.getSize(blockId) else 0L$/;"	V
duration	deploy/master/ApplicationInfo.scala	/^  def duration: Long = {$/;"	m
duration	scheduler/TaskInfo.scala	/^  def duration: Long = {$/;"	m
duration	ui/jobs/StagePage.scala	/^    val duration = if (info.status == "RUNNING") info.timeRunning(System.currentTimeMillis())$/;"	V
duration	ui/jobs/StageTable.scala	/^    val duration =  s.submissionTime.map(t => finishTime - t)$/;"	V
durations	scheduler/cluster/ClusterTaskSetManager.scala	/^      val durations = taskInfos.values.filter(_.successful).map(_.duration).toArray$/;"	V
echoResultCollectCallBack	network/netty/ShuffleCopier.scala	/^  def echoResultCollectCallBack(blockId: BlockId, size: Long, content: ByteBuf) {$/;"	m
effectiveEnd	util/Utils.scala	/^    val effectiveEnd = math.min(length, end)$/;"	V
effectiveStart	util/Utils.scala	/^    val effectiveStart = math.max(0, start)$/;"	V
eid	ui/exec/ExecutorsUI.scala	/^      val eid = taskEnd.taskInfo.executorId$/;"	V
eid	ui/exec/ExecutorsUI.scala	/^      val eid = taskStart.taskInfo.executorId$/;"	V
elapsedSecs	util/RateLimitedOutputStream.scala	/^    val elapsedSecs = SECONDS.convert(math.max(now - lastSyncTime, 1), NANOSECONDS)$/;"	V
elem	util/SizeEstimator.scala	/^          val elem = JArray.get(array, index)$/;"	V
elem	util/SizeEstimator.scala	/^      val elem = stack.last$/;"	V
elementClass	util/SizeEstimator.scala	/^    val elementClass = cls.getComponentType$/;"	V
elementType	api/java/function/FlatMapFunction.scala	/^  def elementType() : ClassManifest[R] = ClassManifest.Any.asInstanceOf[ClassManifest[R]]$/;"	m
elementType	api/java/function/FlatMapFunction2.scala	/^  def elementType() : ClassManifest[C] = ClassManifest.Any.asInstanceOf[ClassManifest[C]]$/;"	m
elements	CacheManager.scala	/^          val elements = new ArrayBuffer[Any]$/;"	V
elements	storage/BlockManager.scala	/^    val elements = new ArrayBuffer[Any]$/;"	V
elements	storage/MemoryStore.scala	/^      val elements = new ArrayBuffer[Any]$/;"	V
elements	util/Vector.scala	/^    val elements: Array[Double] = Array.tabulate(length)(initializer)$/;"	V
elements	util/Vector.scala	/^class Vector(val elements: Array[Double]) extends Serializable {$/;"	V
end	rdd/JdbcRDD.scala	/^      val end = lowerBound + (((i + 1) * length) \/ numPartitions).toLong - 1$/;"	V
end	rdd/ParallelCollectionRDD.scala	/^          val end = (((i + 1) * array.length.toLong) \/ numSlices).toInt$/;"	V
end	rdd/ParallelCollectionRDD.scala	/^          val end = (((i + 1) * r.length.toLong) \/ numSlices).toInt$/;"	V
end	storage/StoragePerfTester.scala	/^    val end = System.currentTimeMillis()$/;"	V
endByte	deploy/worker/ui/WorkerWebUI.scala	/^    val endByte = math.min(startByte+logPageLength, logLength)$/;"	V
endTime	deploy/master/ApplicationInfo.scala	/^  @transient var endTime: Long = _$/;"	v
enoughMemory	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^        def enoughMemory(o: Offer) = {$/;"	m
enqueue	util/SizeEstimator.scala	/^    def enqueue(obj: AnyRef) {$/;"	m
enqueueFailedTask	scheduler/cluster/TaskResultGetter.scala	/^  def enqueueFailedTask(taskSetManager: ClusterTaskSetManager, tid: Long, taskState: TaskState,$/;"	m
entries	storage/MemoryStore.scala	/^  private val entries = new LinkedHashMap[BlockId, Entry](32, 0.75f, true)$/;"	V
entry	broadcast/HttpBroadcast.scala	/^      val entry = iterator.next()$/;"	V
entry	partial/GroupedCountEvaluator.scala	/^        val entry = iter.next()$/;"	V
entry	partial/GroupedCountEvaluator.scala	/^      val entry = iter.next()$/;"	V
entry	partial/GroupedMeanEvaluator.scala	/^        val entry = iter.next()$/;"	V
entry	partial/GroupedMeanEvaluator.scala	/^      val entry = iter.next()$/;"	V
entry	partial/GroupedSumEvaluator.scala	/^        val entry = iter.next()$/;"	V
entry	partial/GroupedSumEvaluator.scala	/^      val entry = iter.next()$/;"	V
entry	rdd/RDD.scala	/^        val entry = iter.next()$/;"	V
entry	scheduler/cluster/CoarseGrainedSchedulerBackend.scala	/^      val entry = iterator.next$/;"	V
entry	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^        val entry = iterator.next$/;"	V
entry	storage/BlockManager.scala	/^      val entry = iterator.next()$/;"	V
entry	storage/MemoryStore.scala	/^          val entry = entries.synchronized { entries.get(blockId) }$/;"	V
entry	storage/MemoryStore.scala	/^        val entry = new Entry(value, size, deserialized)$/;"	V
entry	storage/MemoryStore.scala	/^      val entry = entries.remove(blockId)$/;"	V
entry	storage/MemoryStore.scala	/^    val entry = entries.synchronized {$/;"	V
entry	util/TimeStampedHashMap.scala	/^      val entry = iterator.next()$/;"	V
entry	util/TimeStampedHashSet.scala	/^      val entry = iterator.next()$/;"	V
env	SparkContext.scala	/^    val env = SparkEnv.get$/;"	V
env	SparkContext.scala	/^  private[spark] val env = SparkEnv.createFromSystemProperties($/;"	V
env	SparkEnv.scala	/^  private val env = new ThreadLocal[SparkEnv]$/;"	V
env	api/java/JavaSparkContext.scala	/^  private[spark] val env = sc.env$/;"	V
env	api/python/PythonRDD.scala	/^    val env = SparkEnv.get$/;"	V
env	deploy/worker/ExecutorRunner.scala	/^      val env = builder.environment()$/;"	V
env	executor/Executor.scala	/^  private val env = {$/;"	V
env	rdd/CheckpointRDD.scala	/^    val env = SparkEnv.get$/;"	V
env	rdd/PipedRDD.scala	/^    val env = SparkEnv.get$/;"	V
env	scheduler/cluster/ClusterTaskSetManager.scala	/^  val env = SparkEnv.get$/;"	V
env	scheduler/local/LocalScheduler.scala	/^  val env = SparkEnv.get$/;"	V
env	scheduler/local/LocalTaskSetManager.scala	/^  val env = SparkEnv.get$/;"	V
env	ui/SparkUI.scala	/^  val env = new EnvironmentUI(sc)$/;"	V
envDetails	ui/env/EnvironmentUI.scala	/^  def envDetails(request: HttpServletRequest): Seq[Node] = {$/;"	m
envVar	deploy/master/Master.scala	/^    val envVar = System.getenv("SPARK_PUBLIC_DNS")$/;"	V
envVar	deploy/worker/Worker.scala	/^    val envVar = System.getenv("SPARK_PUBLIC_DNS")$/;"	V
environment	SparkContext.scala	/^    val environment: Map[String, String] = Map(),$/;"	V
environment	scheduler/cluster/mesos/CoarseMesosSchedulerBackend.scala	/^    val environment = Environment.newBuilder()$/;"	V
environment	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^    val environment = Environment.newBuilder()$/;"	V
environment	ui/UIUtils.scala	/^    val environment = page match {$/;"	V
environment	util/Utils.scala	/^    val environment = builder.environment()$/;"	V
epoch	MapOutputTracker.scala	/^  private var epoch: Long = 0$/;"	v
epoch	scheduler/Task.scala	/^  var epoch: Long = -1$/;"	v
epoch	scheduler/cluster/ClusterTaskSetManager.scala	/^  val epoch = sched.mapOutputTracker.getEpoch$/;"	V
epochGotten	MapOutputTracker.scala	/^    var epochGotten: Long = -1$/;"	v
epochLock	MapOutputTracker.scala	/^  private val epochLock = new java.lang.Object$/;"	V
error	scheduler/DAGScheduler.scala	/^          val error = new SparkException("Job cancelled because SparkContext was shut down")$/;"	V
error	scheduler/DAGScheduler.scala	/^      val error = new SparkException("Job aborted: " + reason)$/;"	V
error	scheduler/TaskSetManager.scala	/^  def error(message: String)$/;"	m
error	scheduler/cluster/ClusterScheduler.scala	/^  def error(message: String) {$/;"	m
errorMessage	scheduler/local/LocalTaskSetManager.scala	/^        val errorMessage = "Task %s:%d failed more than %d times; aborting job %s".format($/;"	V
estimate	util/SizeEstimator.scala	/^  def estimate(obj: AnyRef): Long = estimate(obj, new IdentityHashMap[AnyRef, AnyRef])$/;"	m
evaluator	rdd/DoubleRDDFunctions.scala	/^    val evaluator = new MeanEvaluator(self.partitions.size, confidence)$/;"	V
evaluator	rdd/DoubleRDDFunctions.scala	/^    val evaluator = new SumEvaluator(self.partitions.size, confidence)$/;"	V
evaluator	rdd/RDD.scala	/^    val evaluator = new CountEvaluator(partitions.size, confidence)$/;"	V
evaluator	rdd/RDD.scala	/^    val evaluator = new GroupedCountEvaluator[T](partitions.size, confidence)$/;"	V
event	scheduler/DAGScheduler.scala	/^      val event = eventQueue.poll(POLL_TIMEOUT, TimeUnit.MILLISECONDS)$/;"	V
event	scheduler/SparkListenerBus.scala	/^        val event = eventQueue.take$/;"	V
eventAdded	scheduler/SparkListenerBus.scala	/^    val eventAdded = eventQueue.offer(event)$/;"	V
eventQueue	scheduler/DAGScheduler.scala	/^  private val eventQueue = new LinkedBlockingQueue[DAGSchedulerEvent]$/;"	V
eventQueue	scheduler/JobLogger.scala	/^  private val eventQueue = new LinkedBlockingQueue[SparkListenerEvents]$/;"	V
eventQueue	scheduler/SparkListenerBus.scala	/^  private val eventQueue = new LinkedBlockingQueue[SparkListenerEvents](EVENT_QUEUE_CAPACITY)$/;"	V
exLength	api/python/PythonRDD.scala	/^              val exLength = stream.readInt()$/;"	V
exactMatchLocations	rdd/ZippedPartitionsRDD.scala	/^    val exactMatchLocations = prefs.reduce((x, y) => x.intersect(y))$/;"	V
exactMatchLocations	rdd/ZippedRDD.scala	/^    val exactMatchLocations = pref1.intersect(pref2)$/;"	V
exec	deploy/master/ApplicationInfo.scala	/^    val exec = new ExecutorInfo(newExecutorId(useID), this, worker, cores, desc.memoryPerSlave)$/;"	V
exec	deploy/master/Master.scala	/^              val exec = app.addExecutor(worker, coresToUse)$/;"	V
exec	deploy/master/Master.scala	/^            val exec = app.addExecutor(usableWorkers(pos), assigned(pos))$/;"	V
exec	scheduler/SparkListener.scala	/^    val exec = (metrics.executorRunTime - fetchTime.getOrElse(0l)) \/ denom$/;"	V
exec	ui/SparkUI.scala	/^  val exec = new ExecutorsUI(sc)$/;"	V
execArgs	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^  var execArgs: Array[Byte] = null$/;"	v
execHead	ui/exec/ExecutorsUI.scala	/^    val execHead = Seq("Executor ID", "Address", "RDD blocks", "Memory used", "Disk used",$/;"	V
execId	deploy/ExecutorDescription.scala	/^    val execId: Int,$/;"	V
execId	deploy/worker/ExecutorRunner.scala	/^    val execId: Int,$/;"	V
execId	scheduler/DAGScheduler.scala	/^            val execId = status.location.executorId$/;"	V
execId	scheduler/cluster/ClusterScheduler.scala	/^          val execId = offers(i).executorId$/;"	V
execId	scheduler/cluster/ClusterScheduler.scala	/^          val execId = taskIdToExecutorId(tid)$/;"	V
execId	ui/exec/ExecutorsUI.scala	/^    val execId = status.blockManagerId.executorId$/;"	V
execInfo	deploy/master/Master.scala	/^            val execInfo = app.addExecutor(worker, exec.cores, Some(exec.execId))$/;"	V
execInfo	ui/exec/ExecutorsUI.scala	/^    val execInfo = for (statusId <- 0 until storageStatusList.size) yield getExecInfo(statusId)$/;"	V
execOption	deploy/master/Master.scala	/^      val execOption = idToApp.get(appId).flatMap(app => app.executors.get(execId))$/;"	V
execRow	ui/exec/ExecutorsUI.scala	/^    def execRow(kv: Seq[String]) = {$/;"	m
execTable	ui/exec/ExecutorsUI.scala	/^    val execTable = UIUtils.listingTable(execHead, execRow, execInfo)$/;"	V
execs	deploy/worker/Worker.scala	/^      val execs = executors.values.$/;"	V
execs	scheduler/cluster/ClusterScheduler.scala	/^    val execs = executorsByHost.getOrElse(host, new HashSet)$/;"	V
execute	util/Utils.scala	/^  def execute(command: Seq[String]) {$/;"	m
execute	util/Utils.scala	/^  def execute(command: Seq[String], workingDir: File) {$/;"	m
executeAndGetOutput	util/Utils.scala	/^  def executeAndGetOutput(command: Seq[String], workingDir: File = new File("."),$/;"	m
executor	deploy/worker/Worker.scala	/^        val executor = executors(fullId)$/;"	V
executor	executor/CoarseGrainedExecutorBackend.scala	/^  var executor: Executor = null$/;"	v
executor	executor/ExecutorSource.scala	/^class ExecutorSource(val executor: Executor, executorId: String) extends Source {$/;"	V
executor	executor/MesosExecutorBackend.scala	/^  var executor: Executor = null$/;"	v
executor	scheduler/local/LocalScheduler.scala	/^  val executor = new Executor("localhost", "localhost", Seq.empty, isLocal = true)$/;"	V
executor	storage/StoragePerfTester.scala	/^    val executor = Executors.newFixedThreadPool(numMaps)$/;"	V
executorActor	scheduler/cluster/CoarseGrainedSchedulerBackend.scala	/^    private val executorActor = new HashMap[String, ActorRef]$/;"	V
executorAdded	deploy/client/ClientListener.scala	/^  def executorAdded(fullId: String, workerId: String, hostPort: String, cores: Int, memory: Int): Unit$/;"	m
executorAdded	deploy/client/TestClient.scala	/^    def executorAdded(id: String, workerId: String, hostPort: String, cores: Int, memory: Int) {}$/;"	m
executorAddress	scheduler/cluster/CoarseGrainedSchedulerBackend.scala	/^    private val executorAddress = new HashMap[String, Address]$/;"	V
executorDeserializeTime	executor/TaskMetrics.scala	/^  var executorDeserializeTime: Int = _$/;"	v
executorDir	deploy/worker/ExecutorRunner.scala	/^      val executorDir = new File(workDir, appId + "\/" + execId)$/;"	V
executorEnvs	SparkContext.scala	/^  private[spark] val executorEnvs = HashMap[String, String]()$/;"	V
executorGained	scheduler/DAGScheduler.scala	/^  def executorGained(execId: String, host: String) {$/;"	m
executorGained	scheduler/cluster/ClusterScheduler.scala	/^  def executorGained(execId: String, host: String) {$/;"	m
executorHeaders	deploy/master/ui/ApplicationPage.scala	/^    val executorHeaders = Seq("ExecutorID", "Worker", "Cores", "Memory", "State", "Logs")$/;"	V
executorHeaders	deploy/worker/ui/IndexPage.scala	/^    val executorHeaders = Seq("ExecutorID", "Cores", "Memory", "Job Details", "Logs")$/;"	V
executorHost	scheduler/cluster/CoarseGrainedSchedulerBackend.scala	/^    private val executorHost = new HashMap[String, String]$/;"	V
executorId	SparkEnv.scala	/^    val executorId: String,$/;"	V
executorId	deploy/worker/ui/WorkerWebUI.scala	/^    val executorId = request.getParameter("executorId")$/;"	V
executorId	scheduler/TaskDescription.scala	/^    val executorId: String,$/;"	V
executorId	scheduler/TaskInfo.scala	/^    val executorId: String,$/;"	V
executorId	scheduler/cluster/WorkerOffer.scala	/^class WorkerOffer(val executorId: String, val host: String, val cores: Int)$/;"	V
executorId	storage/BlockManagerId.scala	/^  def executorId: String = executorId_$/;"	m
executorIdToHost	scheduler/cluster/ClusterScheduler.scala	/^  private val executorIdToHost = new HashMap[String, String]$/;"	V
executorId_	storage/BlockManagerId.scala	/^    private var executorId_ : String,$/;"	v
executorLost	scheduler/DAGScheduler.scala	/^  def executorLost(execId: String) {$/;"	m
executorLost	scheduler/Schedulable.scala	/^  def executorLost(executorId: String, host: String): Unit$/;"	m
executorLost	scheduler/cluster/ClusterScheduler.scala	/^  def executorLost(executorId: String, reason: ExecutorLossReason) {$/;"	m
executorMemory	scheduler/cluster/SchedulerBackend.scala	/^  protected val executorMemory: Int = SparkContext.executorMemoryRequested$/;"	V
executorMemoryRequested	SparkContext.scala	/^  private[spark] val executorMemoryRequested = {$/;"	V
executorRemoved	deploy/client/ClientListener.scala	/^  def executorRemoved(fullId: String, message: String, exitStatus: Option[Int]): Unit$/;"	m
executorRemoved	deploy/client/TestClient.scala	/^    def executorRemoved(id: String, message: String, exitStatus: Option[Int]) {}$/;"	m
executorRow	deploy/master/ui/ApplicationPage.scala	/^  def executorRow(executor: ExecutorInfo): Seq[Node] = {$/;"	m
executorRow	deploy/worker/ui/IndexPage.scala	/^  def executorRow(executor: ExecutorRunner): Seq[Node] = {$/;"	m
executorRunTime	executor/TaskMetrics.scala	/^  var executorRunTime: Int = _$/;"	v
executorRunTime	scheduler/JobLogger.scala	/^    val executorRunTime = " EXECUTOR_RUN_TIME=" + taskMetrics.executorRunTime$/;"	V
executorSource	executor/Executor.scala	/^  val executorSource = new ExecutorSource(this, executorId)$/;"	V
executorTable	deploy/master/ui/ApplicationPage.scala	/^    val executorTable = UIUtils.listingTable(executorHeaders, executorRow, executors)$/;"	V
executorToTasksActive	ui/exec/ExecutorsUI.scala	/^    val executorToTasksActive = HashMap[String, HashSet[TaskInfo]]()$/;"	V
executorToTasksComplete	ui/exec/ExecutorsUI.scala	/^    val executorToTasksComplete = HashMap[String, Int]()$/;"	V
executorToTasksFailed	ui/exec/ExecutorsUI.scala	/^    val executorToTasksFailed = HashMap[String, Int]()$/;"	V
executors	deploy/master/ApplicationInfo.scala	/^  @transient var executors: mutable.HashMap[Int, ExecutorInfo] = _$/;"	v
executors	deploy/master/WorkerInfo.scala	/^  @transient var executors: mutable.HashMap[String, ExecutorInfo] = _ \/\/ fullId => info$/;"	v
executors	deploy/master/ui/ApplicationPage.scala	/^    val executors = app.executors.values.toSeq$/;"	V
executors	deploy/worker/Worker.scala	/^  val executors = new HashMap[String, ExecutorRunner]$/;"	V
executors	scheduler/cluster/ClusterTaskSetManager.scala	/^        val executors = prefs.flatMap(_.executorId)$/;"	V
executors	ui/UIUtils.scala	/^    val executors = page match {$/;"	V
executorsByHost	scheduler/cluster/ClusterScheduler.scala	/^  private val executorsByHost = new HashMap[String, HashSet[String]]$/;"	V
exists	deploy/master/SparkZooKeeperSession.scala	/^  def exists(path: String, watcher: Watcher = null): Stat = {$/;"	m
exitCode	deploy/worker/ExecutorRunner.scala	/^      val exitCode = process.waitFor()$/;"	V
exitCode	scheduler/cluster/ExecutorLossReason.scala	/^case class ExecutorExited(val exitCode: Int)$/;"	V
exitCode	util/Utils.scala	/^    val exitCode = process.waitFor()$/;"	V
exitStatus	rdd/PipedRDD.scala	/^          val exitStatus = proc.waitFor()$/;"	V
expectedCoupons2	rdd/CoalescedRDD.scala	/^    val expectedCoupons2 = 2 * (math.log(targetLen)*targetLen + targetLen + 0.5).toInt$/;"	V
explainExitCode	executor/ExecutorExitCode.scala	/^  def explainExitCode(exitCode: Int): String = {$/;"	m
ext	deploy/worker/ExecutorRunner.scala	/^    val ext = if (System.getProperty("os.name").startsWith("Windows")) ".cmd" else ".sh"$/;"	V
extraCoresPerSlave	scheduler/cluster/mesos/CoarseMesosSchedulerBackend.scala	/^  val extraCoresPerSlave = System.getProperty("spark.mesos.extra.cores", "0").toInt$/;"	V
extractDoubleDistribution	scheduler/SparkListener.scala	/^  def extractDoubleDistribution(stage:StageCompleted, getMetric: (TaskInfo,TaskMetrics) => Option[Double]): Option[Distribution] = {$/;"	m
extractLongDistribution	scheduler/SparkListener.scala	/^  def extractLongDistribution(stage:StageCompleted, getMetric: (TaskInfo,TaskMetrics) => Option[Long]): Option[Distribution] = {$/;"	m
f	deploy/FaultToleranceTest.scala	/^    val f = future {$/;"	V
f	rdd/AsyncRDDActions.scala	/^    val f = new ComplexFutureAction[Seq[T]]$/;"	V
f	scheduler/SparkListener.scala	/^    def f(d:Double) = format.format(d)$/;"	m
failCount	scheduler/local/LocalTaskSetManager.scala	/^  var failCount = new Array[Int](taskSet.tasks.size)$/;"	v
failed	scheduler/DAGScheduler.scala	/^  val failed = new HashSet[Stage]  \/\/ Stages that must be resubmitted due to fetch failures$/;"	V
failed	scheduler/TaskInfo.scala	/^  var failed = false$/;"	v
failed	scheduler/cluster/ClusterTaskSetManager.scala	/^  var failed = false$/;"	v
failed	storage/BlockFetcherIterator.scala	/^    def failed: Boolean = size == -1$/;"	m
failed2	scheduler/DAGScheduler.scala	/^    val failed2 = failed.toArray$/;"	V
failedEpoch	scheduler/DAGScheduler.scala	/^  val failedEpoch = new HashMap[String, Long]$/;"	V
failedExecutor	scheduler/cluster/ClusterScheduler.scala	/^    var failedExecutor: Option[String] = None$/;"	v
failedStage	scheduler/DAGScheduler.scala	/^        val failedStage = stageIdToStage(task.stageId)$/;"	V
failedStages	ui/jobs/IndexPage.scala	/^      val failedStages = listener.failedStages.reverse.toSeq$/;"	V
failedStages	ui/jobs/JobProgressListener.scala	/^  val failedStages = ListBuffer[StageInfo]()$/;"	V
failedStagesTable	ui/jobs/IndexPage.scala	/^      val failedStagesTable = new StageTable(failedStages.sortBy(_.submissionTime).reverse, parent)$/;"	V
failedTasks	ui/exec/ExecutorsUI.scala	/^    val failedTasks = listener.executorToTasksFailed.getOrElse(execId, 0)$/;"	V
failedTasks	ui/jobs/StageTable.scala	/^    val failedTasks = listener.stageIdToTasksFailed.getOrElse(s.stageId, 0) match {$/;"	V
failure	partial/ApproximateActionListener.scala	/^  var failure: Option[Exception] = None             \/\/ Set if the job has failed (permanently)$/;"	v
failure	partial/PartialResult.scala	/^  private var failure: Option[Exception] = None$/;"	v
failureHandler	partial/PartialResult.scala	/^  private var failureHandler: Option[Exception => Unit] = None$/;"	v
failuresBySlaveId	scheduler/cluster/mesos/CoarseMesosSchedulerBackend.scala	/^  val failuresBySlaveId = new HashMap[String, Int] \/\/ How many times tasks on each slave failed$/;"	V
fc	network/netty/ShuffleCopier.scala	/^    val fc = new FileClient(handler, connectTimeout)$/;"	V
fetch	scheduler/SparkListener.scala	/^    val fetch = fetchTime.map{_ \/ denom}$/;"	V
fetchDone	storage/BlockFetcherIterator.scala	/^          val fetchDone = System.currentTimeMillis()$/;"	V
fetchFile	util/Utils.scala	/^  def fetchFile(url: String, targetDir: File) {$/;"	m
fetchRequests	storage/BlockFetcherIterator.scala	/^    private val fetchRequests = new Queue[FetchRequest]$/;"	V
fetchRequestsSync	storage/BlockFetcherIterator.scala	/^    val fetchRequestsSync = new LinkedBlockingQueue[FetchRequest]$/;"	V
fetchResult	storage/BlockFetcherIterator.scala	/^        val fetchResult = new FetchResult(blockId, blockSize,$/;"	V
fetchStart	storage/BlockFetcherIterator.scala	/^      val fetchStart = System.currentTimeMillis()$/;"	V
fetchTime	scheduler/SparkListener.scala	/^    val fetchTime = metrics.shuffleReadMetrics.map{_.fetchWaitTime}$/;"	V
fetchWaitTime	executor/TaskMetrics.scala	/^  var fetchWaitTime: Long = _$/;"	v
fetchWaitTime	storage/BlockFetchTracker.scala	/^  def fetchWaitTime: Long$/;"	m
fetchedBytes	MapOutputTracker.scala	/^          val fetchedBytes =$/;"	V
fetchedStatuses	MapOutputTracker.scala	/^      var fetchedStatuses: Array[MapStatus] = null$/;"	v
fetcher	rdd/CoGroupedRDD.scala	/^        val fetcher = SparkEnv.get.shuffleFetcher$/;"	V
fetching	MapOutputTracker.scala	/^  private val fetching = new HashSet[Int]$/;"	V
field	util/ClosureCleaner.scala	/^        val field = cls.getDeclaredField("$outer")$/;"	V
field	util/ClosureCleaner.scala	/^        val field = cls.getDeclaredField(fieldName)$/;"	V
field	util/ClosureCleaner.scala	/^      val field = func.getClass.getDeclaredField("$outer")$/;"	V
fieldClass	util/SizeEstimator.scala	/^        val fieldClass = field.getType$/;"	V
file	SparkContext.scala	/^   * val file = sparkContext.hadoopFile[LongWritable, Text, TextInputFormat](path)$/;"	V
file	SparkContext.scala	/^   * val file = sparkContext.hadoopFile[LongWritable, Text, TextInputFormat](path, minSplits)$/;"	V
file	api/python/PythonRDD.scala	/^    val file = new DataInputStream(new FileInputStream(filename))$/;"	V
file	api/python/PythonRDD.scala	/^    val file = new DataOutputStream(new FileOutputStream(filename))$/;"	V
file	broadcast/HttpBroadcast.scala	/^    val file = new File(broadcastDir, BroadcastBlockId(id).name)$/;"	V
file	deploy/worker/ui/WorkerWebUI.scala	/^    val file = new File(path)$/;"	V
file	network/netty/ShuffleSender.scala	/^        val file = new File(subDir, blockId.name)$/;"	V
file	rdd/CheckpointRDD.scala	/^    val file = new Path(checkpointPath, CheckpointRDD.splitIdToFile(split.index))$/;"	V
file	storage/DiskBlockManager.scala	/^      val file = getFile(blockId.name)$/;"	V
file	storage/DiskStore.scala	/^    val file = diskManager.getBlockLocation(blockId).file$/;"	V
file	storage/DiskStore.scala	/^    val file = diskManager.getFile(blockId)$/;"	V
file	storage/DiskStore.scala	/^    val file = fileSegment.file$/;"	V
file	storage/FileSegment.scala	/^private[spark] class FileSegment(val file: File, val offset: Long, val length : Long) {$/;"	V
file	storage/ShuffleBlockManager.scala	/^      val file = files(reducerId)$/;"	V
file	util/Utils.scala	/^    val file = new File(path)$/;"	V
fileData	deploy/master/FileSystemPersistenceEngine.scala	/^    val fileData = new Array[Byte](file.length().asInstanceOf[Int])$/;"	V
fileData	deploy/master/ZooKeeperPersistenceEngine.scala	/^    val fileData = zk.getData("\/spark\/master_status\/" + filename)$/;"	V
fileDir	HttpFileServer.scala	/^  var fileDir : File = null$/;"	v
fileGroup	storage/ShuffleBlockManager.scala	/^        val fileGroup = new ShuffleFileGroup(fileId, shuffleId, files)$/;"	V
fileGroup	storage/ShuffleBlockManager.scala	/^        val fileGroup = shuffleState.unusedFileGroups.poll()$/;"	V
fileGroup	storage/ShuffleBlockManager.scala	/^      private var fileGroup: ShuffleFileGroup = null$/;"	v
fileId	storage/ShuffleBlockManager.scala	/^        val fileId = shuffleState.nextFileId.getAndIncrement()$/;"	V
fileInputStream	rdd/CheckpointRDD.scala	/^    val fileInputStream = fs.open(path, bufferSize)$/;"	V
fileLen	network/netty/FileHeader.scala	/^  val fileLen: Int,$/;"	V
fileName	SparkContext.scala	/^              val fileName = new Path(uri.getPath).getName()$/;"	V
fileOutputStream	rdd/CheckpointRDD.scala	/^    val fileOutputStream = if (blockSize < 0) {$/;"	V
filePath	scheduler/cluster/SimrSchedulerBackend.scala	/^  val filePath = new Path(driverFilePath)$/;"	V
fileSegment	storage/BlockObjectWriter.scala	/^  def fileSegment(): FileSegment$/;"	m
fileSegment	storage/DiskStore.scala	/^    val fileSegment = diskManager.getBlockLocation(blockId)$/;"	V
fileWriter	scheduler/JobLogger.scala	/^      val fileWriter = new PrintWriter(logDir + "\/" + logDirName + "\/" + jobID)$/;"	V
filename	storage/ShuffleBlockManager.scala	/^          val filename = physicalFileName(shuffleId, bucketId, fileId)$/;"	V
filename	util/Utils.scala	/^    val filename = url.split("\/").last$/;"	V
files	broadcast/HttpBroadcast.scala	/^  private val files = new TimeStampedHashSet[String]$/;"	V
files	storage/ShuffleBlockManager.scala	/^        val files = Array.tabulate[File](numBuckets) { bucketId =>$/;"	V
files	util/Utils.scala	/^    val files = file.listFiles()$/;"	V
filter	api/java/JavaDoubleRDD.scala	/^  def filter(f: JFunction[Double, java.lang.Boolean]): JavaDoubleRDD =$/;"	m
filter	api/java/JavaPairRDD.scala	/^  def filter(f: JFunction[(K, V), java.lang.Boolean]): JavaPairRDD[K, V] =$/;"	m
filter	api/java/JavaRDD.scala	/^  def filter(f: JFunction[T, java.lang.Boolean]): JavaRDD[T] =$/;"	m
filter	rdd/RDD.scala	/^  def filter(f: T => Boolean): RDD[T] = new FilteredRDD(this, sc.clean(f))$/;"	m
filterStorageStatusByRDD	storage/StorageUtils.scala	/^  def filterStorageStatusByRDD(storageStatusList: Array[StorageStatus], rddId: Int)$/;"	m
filterWith	rdd/RDD.scala	/^  def filterWith[A: ClassManifest](constructA: Int => A)(p: (T, A) => Boolean): RDD[T] = {$/;"	m
filteredStorageStatusList	ui/storage/RDDPage.scala	/^    val filteredStorageStatusList = StorageUtils.filterStorageStatusByRDD(storageStatusList, id)$/;"	V
filters	scheduler/cluster/mesos/CoarseMesosSchedulerBackend.scala	/^      val filters = Filters.newBuilder().setRefuseSeconds(-1).build()$/;"	V
filters	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^        val filters = Filters.newBuilder().setRefuseSeconds(1).build() \/\/ TODO: lower timeout?$/;"	V
finalOutputName	rdd/CheckpointRDD.scala	/^    val finalOutputName = splitIdToFile(ctx.partitionId)$/;"	V
finalOutputPath	rdd/CheckpointRDD.scala	/^    val finalOutputPath = new Path(outputDir, finalOutputName)$/;"	V
finalStage	scheduler/ActiveJob.scala	/^    val finalStage: Stage,$/;"	V
finalStage	scheduler/DAGScheduler.scala	/^        var finalStage: Stage = null$/;"	v
finalValue	partial/PartialResult.scala	/^  private var finalValue: Option[R] = if (isFinal) Some(initialVal) else None$/;"	v
findIpAndLog	deploy/FaultToleranceTest.scala	/^    def findIpAndLog(line: String): Unit = {$/;"	m
findTask	scheduler/local/LocalTaskSetManager.scala	/^  def findTask(): Option[Int] = {$/;"	m
finish	api/python/PythonRDD.scala	/^              val finish = finishTime - initTime$/;"	V
finishApplication	deploy/master/Master.scala	/^  def finishApplication(app: ApplicationInfo) {$/;"	m
finishConnect	network/Connection.scala	/^  def finishConnect(force: Boolean): Boolean = {$/;"	m
finishTime	FutureAction.scala	/^      val finishTime = System.currentTimeMillis() + atMost.toMillis$/;"	V
finishTime	api/python/PythonRDD.scala	/^              val finishTime = stream.readLong()$/;"	V
finishTime	network/ConnectionManager.scala	/^      val finishTime = System.currentTimeMillis$/;"	V
finishTime	network/ConnectionManager.scala	/^    val finishTime = System.currentTimeMillis$/;"	V
finishTime	network/ConnectionManagerTest.scala	/^        val finishTime = System.currentTimeMillis$/;"	V
finishTime	network/Message.scala	/^  var finishTime = -1L$/;"	v
finishTime	network/SenderTest.scala	/^      val finishTime = System.currentTimeMillis$/;"	V
finishTime	partial/ApproximateActionListener.scala	/^    val finishTime = startTime + timeout$/;"	V
finishTime	scheduler/SparkListenerBus.scala	/^    val finishTime = System.currentTimeMillis + timeoutMillis$/;"	V
finishTime	scheduler/TaskInfo.scala	/^  var finishTime: Long = 0$/;"	v
finishTime	storage/BlockMessage.scala	/^    val finishTime = System.currentTimeMillis$/;"	V
finishTime	storage/BlockMessageArray.scala	/^    val finishTime = System.currentTimeMillis$/;"	V
finishTime	storage/DiskStore.scala	/^    val finishTime = System.currentTimeMillis$/;"	V
finishTime	ui/jobs/StageTable.scala	/^    val finishTime = s.completionTime.getOrElse(System.currentTimeMillis())$/;"	V
finished	rdd/NewHadoopRDD.scala	/^      var finished = false$/;"	v
finished	scheduler/ActiveJob.scala	/^  val finished = Array.fill[Boolean](numPartitions)(false)$/;"	V
finished	scheduler/TaskInfo.scala	/^  def finished: Boolean = finishTime != 0$/;"	m
finished	scheduler/local/LocalTaskSetManager.scala	/^  val finished = new Array[Boolean](numTasks)$/;"	V
finished	util/NextIterator.scala	/^  protected var finished = false$/;"	v
finished	util/Utils.scala	/^    var finished = false$/;"	v
finishedExecutorTable	deploy/worker/ui/IndexPage.scala	/^    val finishedExecutorTable =$/;"	V
finishedExecutors	deploy/worker/Worker.scala	/^  val finishedExecutors = new HashMap[String, ExecutorRunner]$/;"	V
finishedTasks	partial/ApproximateActionListener.scala	/^  var finishedTasks = 0$/;"	v
finishedTasks	scheduler/JobWaiter.scala	/^  private var finishedTasks = 0$/;"	v
first	api/java/JavaRDDLike.scala	/^  def first(): T = rdd.first()$/;"	m
first	rdd/RDD.scala	/^  def first(): T = take(1) match {$/;"	m
firstApp	deploy/master/Master.scala	/^  var firstApp: Option[ApplicationInfo] = None$/;"	v
firstUserClass	util/Utils.scala	/^    var firstUserClass = "<unknown>"$/;"	v
firstUserFile	util/Utils.scala	/^    var firstUserFile = "<unknown>"$/;"	v
firstUserLine	util/Utils.scala	/^                                    val firstUserLine: Int, val firstUserClass: String)$/;"	V
firstUserLine	util/Utils.scala	/^    var firstUserLine = 0$/;"	v
flags	storage/StorageLevel.scala	/^    val flags = in.readByte()$/;"	V
flatMap	api/java/JavaRDDLike.scala	/^  def flatMap(f: DoubleFlatMapFunction[T]): JavaDoubleRDD = {$/;"	m
flatMap	api/java/JavaRDDLike.scala	/^  def flatMap[K2, V2](f: PairFlatMapFunction[T, K2, V2]): JavaPairRDD[K2, V2] = {$/;"	m
flatMap	api/java/JavaRDDLike.scala	/^  def flatMap[U](f: FlatMapFunction[T, U]): JavaRDD[U] = {$/;"	m
flatMap	rdd/RDD.scala	/^  def flatMap[U: ClassManifest](f: T => TraversableOnce[U]): RDD[U] =$/;"	m
flatMapValues	api/java/JavaPairRDD.scala	/^  def flatMapValues[U](f: JFunction[V, java.lang.Iterable[U]]): JavaPairRDD[K, U] = {$/;"	m
flatMapValues	rdd/PairRDDFunctions.scala	/^  def flatMapValues[U](f: V => TraversableOnce[U]): RDD[(K, U)] = {$/;"	m
flatMapWith	rdd/RDD.scala	/^  def flatMapWith[A: ClassManifest, U: ClassManifest]$/;"	m
flush	serializer/Serializer.scala	/^  def flush(): Unit$/;"	m
fmtStackTrace	ui/jobs/StagePage.scala	/^    def fmtStackTrace(trace: Seq[StackTraceElement]): Seq[Node] =$/;"	m
fn	api/java/JavaPairRDD.scala	/^    def fn = (x: V) => f.apply(x).asScala$/;"	m
fn	api/java/JavaRDDLike.scala	/^    def fn = (x: Iterator[T]) => asScalaIterator(f.apply(asJavaIterator(x)).iterator())$/;"	m
fn	api/java/JavaRDDLike.scala	/^    def fn = (x: Iterator[T], y: Iterator[U]) => asScalaIterator($/;"	m
fn	api/java/JavaRDDLike.scala	/^    def fn = (x: T) => f.apply(x).asScala$/;"	m
fold	api/java/JavaRDDLike.scala	/^  def fold(zeroValue: T)(f: JFunction2[T, T, T]): T =$/;"	m
fold	rdd/RDD.scala	/^  def fold(zeroValue: T)(op: (T, T) => T): T = {$/;"	m
foldByKey	api/java/JavaPairRDD.scala	/^  def foldByKey(zeroValue: V, func: JFunction2[V, V, V]): JavaPairRDD[K, V] =$/;"	m
foldByKey	api/java/JavaPairRDD.scala	/^  def foldByKey(zeroValue: V, numPartitions: Int, func: JFunction2[V, V, V]): JavaPairRDD[K, V] =$/;"	m
foldByKey	api/java/JavaPairRDD.scala	/^  def foldByKey(zeroValue: V, partitioner: Partitioner, func: JFunction2[V, V, V]): JavaPairRDD[K, V] =$/;"	m
foldByKey	rdd/PairRDDFunctions.scala	/^  def foldByKey(zeroValue: V)(func: (V, V) => V): RDD[(K, V)] = {$/;"	m
foldByKey	rdd/PairRDDFunctions.scala	/^  def foldByKey(zeroValue: V, numPartitions: Int)(func: (V, V) => V): RDD[(K, V)] = {$/;"	m
foldByKey	rdd/PairRDDFunctions.scala	/^  def foldByKey(zeroValue: V, partitioner: Partitioner)(func: (V, V) => V): RDD[(K, V)] = {$/;"	m
foldPartition	rdd/RDD.scala	/^    val foldPartition = (iter: Iterator[T]) => iter.fold(zeroValue)(cleanOp)$/;"	V
forMapTask	storage/ShuffleBlockManager.scala	/^  def forMapTask(shuffleId: Int, mapId: Int, numBuckets: Int, serializer: Serializer) = {$/;"	m
foreach	api/java/JavaRDDLike.scala	/^  def foreach(f: VoidFunction[T]) {$/;"	m
foreach	rdd/RDD.scala	/^  def foreach(f: T => Unit) {$/;"	m
foreachAsync	rdd/AsyncRDDActions.scala	/^  def foreachAsync(f: T => Unit): FutureAction[Unit] = {$/;"	m
foreachPartition	rdd/RDD.scala	/^  def foreachPartition(f: Iterator[T] => Unit) {$/;"	m
foreachPartitionAsync	rdd/AsyncRDDActions.scala	/^  def foreachPartitionAsync(f: Iterator[T] => Unit): FutureAction[Unit] = {$/;"	m
foreachWith	rdd/RDD.scala	/^  def foreachWith[A: ClassManifest](constructA: Int => A)(f: (T, A) => Unit) {$/;"	m
format	SparkContext.scala	/^    val format = classOf[SequenceFileInputFormat[Writable, Writable]]$/;"	V
format	SparkHadoopWriter.scala	/^  @transient private var format: OutputFormat[AnyRef,AnyRef] = null$/;"	v
format	rdd/NewHadoopRDD.scala	/^      val format = inputFormatClass.newInstance$/;"	V
format	rdd/PairRDDFunctions.scala	/^      val format = outputFormatClass.newInstance$/;"	V
format	rdd/SequenceFileRDDFunctions.scala	/^    val format = classOf[SequenceFileOutputFormat[Writable, Writable]]$/;"	V
formatDate	deploy/WebUI.scala	/^  def formatDate(date: Date): String = DATE_FORMAT.format(date)$/;"	m
formatDate	deploy/WebUI.scala	/^  def formatDate(timestamp: Long): String = DATE_FORMAT.format(new Date(timestamp))$/;"	m
formatDuration	deploy/WebUI.scala	/^  def formatDuration(milliseconds: Long): String = {$/;"	m
formatDuration	ui/jobs/JobProgressUI.scala	/^  def formatDuration(ms: Long) = Utils.msDurationToString(ms)$/;"	m
formatDuration	ui/jobs/StagePage.scala	/^    val formatDuration = if (info.status == "RUNNING") parent.formatDuration(duration)$/;"	V
formatSparkCallSite	util/Utils.scala	/^  def formatSparkCallSite = {$/;"	m
formats	deploy/FaultToleranceTest.scala	/^  implicit val formats = net.liftweb.json.DefaultFormats$/;"	V
formatter	SparkHadoopWriter.scala	/^    val formatter = new SimpleDateFormat("yyyyMMddHHmm")$/;"	V
formatter	rdd/NewHadoopRDD.scala	/^    val formatter = new SimpleDateFormat("yyyyMMddHHmm")$/;"	V
formatter	rdd/PairRDDFunctions.scala	/^    val formatter = new SimpleDateFormat("yyyyMMddHHmm")$/;"	V
fos	storage/BlockObjectWriter.scala	/^  private var fos: FileOutputStream = null$/;"	v
found	scheduler/cluster/ClusterScheduler.scala	/^    var found = true$/;"	v
foundLocalDir	storage/DiskBlockManager.scala	/^      var foundLocalDir = false$/;"	v
foundTasks	scheduler/cluster/ClusterTaskSetManager.scala	/^    var foundTasks = false$/;"	v
frac	Partitioner.scala	/^      val frac = math.min(maxSampleSize \/ math.max(rddSize, 1), 1.0)$/;"	V
fraction	rdd/RDD.scala	/^    var fraction = 0.0$/;"	v
freeCores	scheduler/cluster/CoarseGrainedSchedulerBackend.scala	/^    private val freeCores = new HashMap[String, Int]$/;"	V
freeCores	scheduler/local/LocalScheduler.scala	/^class LocalActor(localScheduler: LocalScheduler, private var freeCores: Int)$/;"	v
freeCpuCores	scheduler/local/LocalScheduler.scala	/^      var freeCpuCores = freeCores$/;"	v
freeMemory	storage/MemoryStore.scala	/^  def freeMemory: Long = maxMemory - currentMemory$/;"	m
fromBufferMessage	storage/BlockMessage.scala	/^  def fromBufferMessage(bufferMessage: BufferMessage): BlockMessage = {$/;"	m
fromBufferMessage	storage/BlockMessageArray.scala	/^  def fromBufferMessage(bufferMessage: BufferMessage): BlockMessageArray = {$/;"	m
fromByteBuffer	storage/BlockMessage.scala	/^  def fromByteBuffer(buffer: ByteBuffer): BlockMessage = {$/;"	m
fromGetBlock	storage/BlockMessage.scala	/^  def fromGetBlock(getBlock: GetBlock): BlockMessage = {$/;"	m
fromGotBlock	storage/BlockMessage.scala	/^  def fromGotBlock(gotBlock: GotBlock): BlockMessage = {$/;"	m
fromJavaRDD	api/java/JavaPairRDD.scala	/^  def fromJavaRDD[K, V](rdd: JavaRDD[(K, V)]): JavaPairRDD[K, V] = {$/;"	m
fromMesos	TaskState.scala	/^  def fromMesos(mesosState: MesosTaskState): TaskState = mesosState match {$/;"	m
fromPutBlock	storage/BlockMessage.scala	/^  def fromPutBlock(putBlock: PutBlock): BlockMessage = {$/;"	m
fromRDD	api/java/JavaDoubleRDD.scala	/^  def fromRDD(rdd: RDD[scala.Double]): JavaDoubleRDD = new JavaDoubleRDD(rdd)$/;"	m
fromRDD	api/java/JavaPairRDD.scala	/^  def fromRDD[K: ClassManifest, V: ClassManifest](rdd: RDD[(K, V)]): JavaPairRDD[K, V] =$/;"	m
fromSocketAddress	network/ConnectionManagerId.scala	/^  def fromSocketAddress(socketAddress: InetSocketAddress): ConnectionManagerId = {$/;"	m
fs	SparkContext.scala	/^    val fs = path.getFileSystem(SparkHadoopUtil.get.newConfiguration())$/;"	V
fs	SparkHadoopWriter.scala	/^    val fs = outputPath.getFileSystem(conf)$/;"	V
fs	SparkHadoopWriter.scala	/^    val fs: FileSystem = {$/;"	V
fs	rdd/CheckpointRDD.scala	/^    val fs = outputDir.getFileSystem(SparkHadoopUtil.get.newConfiguration())$/;"	V
fs	rdd/CheckpointRDD.scala	/^    val fs = path.getFileSystem(SparkHadoopUtil.get.newConfiguration())$/;"	V
fs	rdd/CheckpointRDD.scala	/^  @transient val fs = new Path(checkpointPath).getFileSystem(sc.hadoopConfiguration)$/;"	V
fs	rdd/RDDCheckpointData.scala	/^    val fs = path.getFileSystem(new Configuration())$/;"	V
fs	scheduler/cluster/SimrSchedulerBackend.scala	/^    val fs = FileSystem.get(conf)$/;"	V
fs	util/Utils.scala	/^        val fs = FileSystem.get(uri, conf)$/;"	V
fullDir	deploy/master/SparkZooKeeperSession.scala	/^    var fullDir = ""$/;"	v
fullId	deploy/client/Client.scala	/^        val fullId = appId + "\/" + id$/;"	V
fullId	deploy/master/ExecutorInfo.scala	/^  def fullId: String = application.id + "\/" + id$/;"	m
fullId	deploy/worker/ExecutorRunner.scala	/^  val fullId = appId + "\/" + execId$/;"	V
fullId	deploy/worker/Worker.scala	/^        val fullId = appId + "\/" + execId$/;"	V
fullId	deploy/worker/Worker.scala	/^      val fullId = appId + "\/" + execId$/;"	V
func	rdd/RDD.scala	/^    val func = (context: TaskContext, index: Int, iter: Iterator[T]) => f(context, iter)$/;"	V
func	rdd/RDD.scala	/^    val func = (context: TaskContext, index: Int, iter: Iterator[T]) => f(index, iter)$/;"	V
func	rdd/RDD.scala	/^    val func = (context: TaskContext, index: Int, iter: Iterator[T]) => f(iter)$/;"	V
func	scheduler/ActiveJob.scala	/^    val func: (TaskContext, Iterator[_]) => _,$/;"	V
func	scheduler/ResultTask.scala	/^    val func = objIn.readObject().asInstanceOf[(TaskContext, Iterator[_]) => _]$/;"	V
func	scheduler/ResultTask.scala	/^    var func: (TaskContext, Iterator[T]) => U,$/;"	v
func2	scheduler/DAGScheduler.scala	/^    val func2 = func.asInstanceOf[(TaskContext, Iterator[_]) => _]$/;"	V
future	MapOutputTracker.scala	/^      val future = trackerActor.ask(message)(timeout)$/;"	V
future	deploy/client/Client.scala	/^        val future = actor.ask(StopClient)(timeout)$/;"	V
future	scheduler/cluster/CoarseGrainedSchedulerBackend.scala	/^        val future = driverActor.ask(StopDriver)(timeout)$/;"	V
future	scheduler/cluster/CoarseGrainedSchedulerBackend.scala	/^        val future = driverActor.ask(StopExecutors)(timeout)$/;"	V
future	scheduler/cluster/CoarseGrainedSchedulerBackend.scala	/^      val future = driverActor.ask(RemoveExecutor(executorId, reason))(timeout)$/;"	V
future	storage/BlockFetcherIterator.scala	/^      val future = connectionManager.sendMessageReliably(cmId, blockMessageArray.toBufferMessage)$/;"	V
future	storage/BlockManagerMaster.scala	/^        val future = driverActor.ask(message)(timeout)$/;"	V
future	storage/BlockManagerMaster.scala	/^    val future = askDriverWithReply[Future[Seq[Int]]](RemoveRdd(rddId))$/;"	V
futureExecContext	network/ConnectionManager.scala	/^  implicit val futureExecContext = ExecutionContext.fromExecutor($/;"	V
futureExecContext	storage/BlockManager.scala	/^  implicit val futureExecContext = connectionManager.futureExecContext$/;"	V
futures	network/ConnectionManagerTest.scala	/^        val futures = slaveConnManagerIds.filter(_ != thisConnManagerId).map(slaveConnManagerId => {$/;"	V
fwInfo	scheduler/cluster/mesos/CoarseMesosSchedulerBackend.scala	/^          val fwInfo = FrameworkInfo.newBuilder().setUser("").setName(appName).build()$/;"	V
fwInfo	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^          val fwInfo = FrameworkInfo.newBuilder().setUser("").setName(appName).build()$/;"	V
g	network/ConnectionManager.scala	/^          val g = Await.result(f, 1 second)$/;"	V
g	network/ConnectionManager.scala	/^      val g = Await.result(f, 1 second)$/;"	V
gB	storage/BlockManagerWorker.scala	/^        val gB = new GetBlock(blockMessage.getId)$/;"	V
gB	storage/BlockMessageArray.scala	/^          val gB = new GetBlock(blockMessage.getId)$/;"	V
gInfo	broadcast/BitTorrentBroadcast.scala	/^    val gInfo = MultiTracker.getGuideInfo(variableID)$/;"	V
gInfo	broadcast/MultiTracker.scala	/^                      val gInfo = ois.readObject.asInstanceOf[SourceInfo]$/;"	V
gInfo	broadcast/MultiTracker.scala	/^                      var gInfo =$/;"	v
gInfo	broadcast/MultiTracker.scala	/^    var gInfo: SourceInfo = SourceInfo("", SourceInfo.TxNotStartedRetry)$/;"	v
gInfo	broadcast/TreeBroadcast.scala	/^    val gInfo = MultiTracker.getGuideInfo(variableID)$/;"	V
ganglia	metrics/sink/GangliaSink.scala	/^  val ganglia = new GMetric(host, port, mode, ttl)$/;"	V
gcMetricSet	metrics/source/JvmSource.scala	/^  val gcMetricSet = new GarbageCollectorMetricSet$/;"	V
gcTime	executor/Executor.scala	/^      def gcTime = ManagementFactory.getGarbageCollectorMXBeans.map(_.getCollectionTime).sum$/;"	m
gcTime	ui/jobs/StagePage.scala	/^    val gcTime = metrics.map(m => m.jvmGCTime).getOrElse(0L)$/;"	V
generateWorkerId	deploy/worker/Worker.scala	/^  def generateWorkerId(): String = {$/;"	m
generator	rdd/RDD.scala	/^  @transient var generator = Utils.getCallSiteInfo.firstUserClass$/;"	v
get	FutureAction.scala	/^  def get(): T = Await.result(this, Duration.Inf)$/;"	m
get	SparkEnv.scala	/^  def get: SparkEnv = {$/;"	m
get	SparkFiles.java	/^  public static String get(String filename) {$/;"	m	class:SparkFiles
get	deploy/SparkHadoopUtil.scala	/^  def get: SparkHadoopUtil = {$/;"	m
get	partial/StudentTCacher.scala	/^  def get(sampleSize: Long): Double = {$/;"	m
get	serializer/SerializerManager.scala	/^  def get(clsName: String): Serializer = {$/;"	m
get	storage/BlockManager.scala	/^  def get(blockId: BlockId): Option[Iterator[Any]] = {$/;"	m
get	util/TimeStampedHashMap.scala	/^  def get(key: A): Option[B] = {$/;"	m
get	util/collection/BitSet.scala	/^  def get(index: Int): Boolean = {$/;"	m
getAddressHostName	util/Utils.scala	/^  def getAddressHostName(address: String): String = {$/;"	m
getAllPools	SparkContext.scala	/^  def getAllPools: ArrayBuffer[Schedulable] = {$/;"	m
getBlock	network/netty/ShuffleCopier.scala	/^  def getBlock(cmId: ConnectionManagerId, blockId: BlockId,$/;"	m
getBlock	network/netty/ShuffleCopier.scala	/^  def getBlock(host: String, port: Int, blockId: BlockId,$/;"	m
getBlockLocation	storage/DiskBlockManager.scala	/^  def getBlockLocation(blockId: BlockId): FileSegment = {$/;"	m
getBlockLocation	storage/ShuffleBlockManager.scala	/^  def getBlockLocation(id: ShuffleBlockId): FileSegment = {$/;"	m
getBlocks	network/netty/ShuffleCopier.scala	/^  def getBlocks(cmId: ConnectionManagerId,$/;"	m
getByteRange	deploy/worker/ui/WorkerWebUI.scala	/^  def getByteRange(path: String, offset: Option[Long], byteLength: Int)$/;"	m
getBytes	storage/BlockStore.scala	/^  def getBytes(blockId: BlockId): Option[ByteBuffer]$/;"	m
getCachedBlockManagerId	storage/BlockManagerId.scala	/^  def getCachedBlockManagerId(id: BlockManagerId): BlockManagerId = {$/;"	m
getCachedMetadata	rdd/HadoopRDD.scala	/^  def getCachedMetadata(key: String) = SparkEnv.get.hadoopJobMetadata.get(key)$/;"	m
getCallSiteInfo	util/Utils.scala	/^  def getCallSiteInfo: CallSiteInfo = {$/;"	m
getCheckpointFile	api/java/JavaRDDLike.scala	/^  def getCheckpointFile(): Optional[String] = {$/;"	m
getCheckpointFile	rdd/RDD.scala	/^  def getCheckpointFile: Option[String] = {$/;"	m
getCheckpointFile	rdd/RDDCheckpointData.scala	/^  def getCheckpointFile: Option[String] = {$/;"	m
getChildren	deploy/master/SparkZooKeeperSession.scala	/^  def getChildren(path: String, watcher: Watcher = null): List[String] = {$/;"	m
getChunk	network/Connection.scala	/^    def getChunk(): Option[MessageChunk] = {$/;"	m
getChunk	network/Connection.scala	/^    def getChunk(header: MessageChunkHeader): Option[MessageChunk] = {$/;"	m
getChunkForReceiving	network/BufferMessage.scala	/^  def getChunkForReceiving(chunkSize: Int): Option[MessageChunk] = {$/;"	m
getChunkForReceiving	network/Message.scala	/^  def getChunkForReceiving(chunkSize: Int): Option[MessageChunk]$/;"	m
getChunkForSending	network/BufferMessage.scala	/^  def getChunkForSending(maxChunkSize: Int): Option[MessageChunk] = {$/;"	m
getChunkForSending	network/Message.scala	/^  def getChunkForSending(maxChunkSize: Int): Option[MessageChunk]$/;"	m
getConf	rdd/HadoopRDD.scala	/^  def getConf: Configuration = getJobConf()$/;"	m
getConf	rdd/NewHadoopRDD.scala	/^  def getConf: Configuration = confBroadcast.value.value$/;"	m
getData	deploy/master/SparkZooKeeperSession.scala	/^  def getData(path: String): Array[Byte] = {$/;"	m
getData	storage/BlockMessage.scala	/^  def getData: ByteBuffer = data$/;"	m
getDelaySeconds	util/MetadataCleaner.scala	/^  def getDelaySeconds = System.getProperty("spark.cleaner.ttl", "-1").toInt$/;"	m
getDelaySeconds	util/MetadataCleaner.scala	/^  def getDelaySeconds(cleanerType: MetadataCleanerType.MetadataCleanerType): Int = {$/;"	m
getDisableHeartBeatsForTesting	storage/BlockManager.scala	/^  def getDisableHeartBeatsForTesting: Boolean =$/;"	m
getDiskWriter	storage/BlockManager.scala	/^  def getDiskWriter(blockId: BlockId, file: File, serializer: Serializer, bufferSize: Int)$/;"	m
getEpoch	MapOutputTracker.scala	/^  def getEpoch: Long = {$/;"	m
getExecInfo	ui/exec/ExecutorsUI.scala	/^  def getExecInfo(statusId: Int): Seq[String] = {$/;"	m
getExecutorMemoryStatus	SparkContext.scala	/^  def getExecutorMemoryStatus: Map[String, (Long, Long)] = {$/;"	m
getExecutorStorageStatus	SparkContext.scala	/^  def getExecutorStorageStatus: Array[StorageStatus] = {$/;"	m
getExecutorsAliveOnHost	scheduler/cluster/ClusterScheduler.scala	/^  def getExecutorsAliveOnHost(host: String): Option[Set[String]] = synchronized {$/;"	m
getFile	storage/DiskBlockManager.scala	/^  def getFile(blockId: BlockId): File = getFile(blockId.name)$/;"	m
getFile	storage/DiskBlockManager.scala	/^  def getFile(filename: String): File = {$/;"	m
getFileLenOffset	network/netty/FileHeader.scala	/^  def getFileLenOffset = 0$/;"	m
getFileLenSize	network/netty/FileHeader.scala	/^  def getFileLenSize = Integer.SIZE\/8$/;"	m
getFileSegmentFor	storage/ShuffleBlockManager.scala	/^    def getFileSegmentFor(mapId: Int, reducerId: Int): Option[FileSegment] = {$/;"	m
getFinalValue	partial/PartialResult.scala	/^  def getFinalValue(): R = synchronized {$/;"	m
getFinalValueInternal	partial/PartialResult.scala	/^      def getFinalValueInternal() = PartialResult.this.getFinalValueInternal().map(f)$/;"	m
getGuideInfo	broadcast/MultiTracker.scala	/^  def getGuideInfo(variableLong: Long): SourceInfo = {$/;"	m
getHandlers	metrics/sink/MetricsServlet.scala	/^  def getHandlers = Array[(String, Handler)]($/;"	m
getHandlers	ui/env/EnvironmentUI.scala	/^  def getHandlers = Seq[(String, Handler)]($/;"	m
getHandlers	ui/exec/ExecutorsUI.scala	/^  def getHandlers = Seq[(String, Handler)]($/;"	m
getHandlers	ui/jobs/JobProgressUI.scala	/^  def getHandlers = Seq[(String, Handler)]($/;"	m
getHandlers	ui/storage/BlockManagerUI.scala	/^  def getHandlers = Seq[(String, Handler)]($/;"	m
getHeartBeatFrequencyFromSystemProperties	storage/BlockManager.scala	/^  def getHeartBeatFrequencyFromSystemProperties: Long =$/;"	m
getId	storage/BlockMessage.scala	/^  def getId: BlockId = id$/;"	m
getInstance	metrics/MetricsConfig.scala	/^  def getInstance(inst: String): Properties = {$/;"	m
getLastProcessId	deploy/FaultToleranceTest.scala	/^  def getLastProcessId: DockerId = {$/;"	m
getLeader	deploy/FaultToleranceTest.scala	/^  def getLeader: TestMasterInfo = {$/;"	m
getLeastGroupHash	rdd/CoalescedRDD.scala	/^  def getLeastGroupHash(key: String): Option[PartitionGroup] = {$/;"	m
getLevel	storage/BlockManager.scala	/^  def getLevel(blockId: BlockId): StorageLevel = blockInfo.get(blockId).map(_.level).orNull$/;"	m
getLevel	storage/BlockMessage.scala	/^  def getLevel: StorageLevel =  level$/;"	m
getLocal	storage/BlockManager.scala	/^  def getLocal(blockId: BlockId): Option[Iterator[Any]] = {$/;"	m
getLocalBytes	storage/BlockManager.scala	/^  def getLocalBytes(blockId: BlockId): Option[ByteBuffer] = {$/;"	m
getLocalDir	util/Utils.scala	/^  def getLocalDir: String = {$/;"	m
getLocalFromDisk	storage/BlockManager.scala	/^  def getLocalFromDisk(blockId: BlockId, serializer: Serializer): Option[Iterator[Any]] = {$/;"	m
getLocalProperty	SparkContext.scala	/^  def getLocalProperty(key: String): String =$/;"	m
getLocalityIndex	scheduler/cluster/ClusterTaskSetManager.scala	/^  def getLocalityIndex(locality: TaskLocality.TaskLocality): Int = {$/;"	m
getLocationBlockIds	storage/BlockManager.scala	/^  def getLocationBlockIds(blockIds: Array[BlockId]): Array[Seq[BlockManagerId]] = {$/;"	m
getLocations	storage/BlockManagerMaster.scala	/^  def getLocations(blockId: BlockId): Seq[BlockManagerId] = {$/;"	m
getLocations	storage/BlockManagerMaster.scala	/^  def getLocations(blockIds: Array[BlockId]): Seq[Seq[BlockManagerId]] = {$/;"	m
getMasterUrls	deploy/FaultToleranceTest.scala	/^  def getMasterUrls(masters: Seq[TestMasterInfo]): String = {$/;"	m
getMaxMemoryFromSystemProperties	storage/BlockManager.scala	/^  def getMaxMemoryFromSystemProperties: Long = {$/;"	m
getMemoryStatus	storage/BlockManagerMaster.scala	/^  def getMemoryStatus: Map[BlockManagerId, (Long, Long)] = {$/;"	m
getMessageForChunk	network/Connection.scala	/^    def getMessageForChunk(chunk: MessageChunk): Option[BufferMessage] = {$/;"	m
getMetricsSnapshot	metrics/sink/MetricsServlet.scala	/^  def getMetricsSnapshot(request: HttpServletRequest): String = {$/;"	m
getNewId	network/Message.scala	/^  def getNewId() = synchronized {$/;"	m
getOffset	deploy/worker/ui/WorkerWebUI.scala	/^    val getOffset = offset.getOrElse(logLength-defaultBytes)$/;"	V
getOrCompute	CacheManager.scala	/^  def getOrCompute[T](rdd: RDD[T], split: Partition, context: TaskContext, storageLevel: StorageLevel)$/;"	m
getOrElse	util/collection/PrimitiveKeyOpenHashMap.scala	/^  def getOrElse(k: K, elseValue: V): V = {$/;"	m
getParents	Dependency.scala	/^  def getParents(partitionId: Int): Seq[Int]$/;"	m
getParents	rdd/CartesianRDD.scala	/^      def getParents(id: Int): Seq[Int] = List(id % numPartitionsInRdd2)$/;"	m
getParents	rdd/CartesianRDD.scala	/^      def getParents(id: Int): Seq[Int] = List(id \/ numPartitionsInRdd2)$/;"	m
getParents	rdd/CoalescedRDD.scala	/^      def getParents(id: Int): Seq[Int] =$/;"	m
getPartition	Partitioner.scala	/^  def getPartition(key: Any): Int = key match {$/;"	m
getPartition	Partitioner.scala	/^  def getPartition(key: Any): Int = {$/;"	m
getPartition	Partitioner.scala	/^  def getPartition(key: Any): Int$/;"	m
getPartitions	rdd/CoalescedRDD.scala	/^  def getPartitions: Array[PartitionGroup] = groupArr.filter( pg => pg.size > 0).toArray$/;"	m
getPartitions	rdd/RDDCheckpointData.scala	/^  def getPartitions: Array[Partition] = {$/;"	m
getPeers	storage/BlockManagerMaster.scala	/^  def getPeers(blockManagerId: BlockManagerId, numPeers: Int): Seq[BlockManagerId] = {$/;"	m
getPersistentRDDs	SparkContext.scala	/^  def getPersistentRDDs: Map[Int, RDD[_]] = persistentRdds.toMap$/;"	m
getPoolForName	SparkContext.scala	/^  def getPoolForName(pool: String): Option[Schedulable] = {$/;"	m
getPos	util/collection/OpenHashSet.scala	/^  def getPos(k: T): Int = {$/;"	m
getPreferredLocations	rdd/RDDCheckpointData.scala	/^  def getPreferredLocations(split: Partition): Seq[String] = {$/;"	m
getPreferredLocs	scheduler/DAGScheduler.scala	/^  def getPreferredLocs(rdd: RDD[_], partition: Int): Seq[TaskLocation] = synchronized {$/;"	m
getQuantileCols	ui/jobs/StagePage.scala	/^          def getQuantileCols(data: Seq[Double]) =$/;"	m
getQuantiles	util/Distribution.scala	/^  def getQuantiles(probabilities: Traversable[Double] = defaultProbabilities) = {$/;"	m
getRDDStorageInfo	SparkContext.scala	/^  def getRDDStorageInfo: Array[RDDInfo] = {$/;"	m
getRackForHost	scheduler/cluster/ClusterScheduler.scala	/^  def getRackForHost(value: String): Option[String] = None$/;"	m
getRddsInStage	scheduler/JobLogger.scala	/^    def getRddsInStage(rdd: RDD[_]): ListBuffer[RDD[_]] = {$/;"	m
getRemote	storage/BlockManager.scala	/^  def getRemote(blockId: BlockId): Option[Iterator[Any]] = {$/;"	m
getRemoteAddress	network/Connection.scala	/^  def getRemoteAddress() = channel.socket.getRemoteSocketAddress().asInstanceOf[InetSocketAddress]$/;"	m
getRemoteBytes	storage/BlockManager.scala	/^   def getRemoteBytes(blockId: BlockId): Option[ByteBuffer] = {$/;"	m
getRemoteConnectionManagerId	network/Connection.scala	/^  def getRemoteConnectionManagerId(): ConnectionManagerId = {$/;"	m
getResource	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^  def getResource(res: JList[Resource], name: String): Double = {$/;"	m
getRootDirectory	SparkFiles.java	/^  public static String getRootDirectory() {$/;"	m	class:SparkFiles
getSchedulableByName	scheduler/Schedulable.scala	/^  def getSchedulableByName(name: String): Schedulable$/;"	m
getSchedulingMode	SparkContext.scala	/^  def getSchedulingMode: SchedulingMode.SchedulingMode = {$/;"	m
getSeq	rdd/CoGroupedRDD.scala	/^    val getSeq = (k: K) => {$/;"	V
getSeq	rdd/SubtractedRDD.scala	/^    def getSeq(k: K): ArrayBuffer[V] = {$/;"	m
getSerializedLocations	MapOutputTracker.scala	/^  def getSerializedLocations(shuffleId: Int): Array[Byte] = {$/;"	m
getServerStatuses	MapOutputTracker.scala	/^  def getServerStatuses(shuffleId: Int, reduceId: Int): Array[(BlockManagerId, Long)] = {$/;"	m
getServletHandlers	metrics/MetricsSystem.scala	/^  def getServletHandlers = metricsServlet.map(_.getHandlers).getOrElse(Array())$/;"	m
getSingle	storage/BlockManager.scala	/^  def getSingle(blockId: BlockId): Option[Any] = {$/;"	m
getSize	storage/BlockStore.scala	/^  def getSize(blockId: BlockId): Long$/;"	m
getSortedTaskSetQueue	scheduler/Schedulable.scala	/^  def getSortedTaskSetQueue(): ArrayBuffer[TaskSetManager]$/;"	m
getSparkHome	api/java/JavaSparkContext.scala	/^  def getSparkHome(): Optional[String] = JavaUtils.optionToOptional(sc.getSparkHome())$/;"	m
getStageInfo	SparkContext.scala	/^  def getStageInfo: Map[Stage,StageInfo] = {$/;"	m
getStorageLevel	api/java/JavaRDDLike.scala	/^  def getStorageLevel: StorageLevel = rdd.getStorageLevel$/;"	m
getStorageLevel	rdd/RDD.scala	/^  def getStorageLevel = storageLevel$/;"	m
getStorageStatus	storage/BlockManagerMaster.scala	/^  def getStorageStatus: Array[StorageStatus] = {$/;"	m
getSystemProperties	util/Utils.scala	/^  def getSystemProperties(): Map[String, String] = {$/;"	m
getTaskResultExecutor	scheduler/cluster/TaskResultGetter.scala	/^  private val getTaskResultExecutor = Utils.newDaemonFixedThreadPool($/;"	V
getThreadLocal	SparkEnv.scala	/^  def getThreadLocal : SparkEnv = {$/;"	m
getTime	util/Clock.scala	/^  def getTime(): Long = System.currentTimeMillis()$/;"	m
getTime	util/Clock.scala	/^  def getTime(): Long$/;"	m
getType	storage/BlockMessage.scala	/^  def getType: Int = typ$/;"	m
getUsedTimeMs	util/Utils.scala	/^  def getUsedTimeMs(startTimeMs: Long): String = {$/;"	m
getVMMethod	util/SizeEstimator.scala	/^      val getVMMethod = hotSpotMBeanClass.getDeclaredMethod("getVMOption",$/;"	V
getValue	util/collection/OpenHashSet.scala	/^  def getValue(pos: Int): T = _data(pos)$/;"	m
getValues	storage/BlockStore.scala	/^  def getValues(blockId: BlockId): Option[Iterator[Any]]$/;"	m
getValues	storage/DiskStore.scala	/^  def getValues(blockId: BlockId, serializer: Serializer): Option[Iterator[Any]] = {$/;"	m
gettingResult	scheduler/TaskInfo.scala	/^  def gettingResult: Boolean = gettingResultTime != 0$/;"	m
gettingResultTime	scheduler/TaskInfo.scala	/^  var gettingResultTime: Long = 0$/;"	v
gisSource	broadcast/BitTorrentBroadcast.scala	/^          var gisSource: ObjectInputStream = null$/;"	v
gisSource	broadcast/TreeBroadcast.scala	/^          var gisSource: ObjectInputStream = null$/;"	v
glom	api/java/JavaRDDLike.scala	/^  def glom(): JavaRDD[JList[T]] =$/;"	m
glom	rdd/RDD.scala	/^  def glom(): RDD[Array[T]] = new GlommedRDD(this)$/;"	m
gosSource	broadcast/BitTorrentBroadcast.scala	/^          var gosSource: ObjectOutputStream = null$/;"	v
gosSource	broadcast/TreeBroadcast.scala	/^          var gosSource: ObjectOutputStream = null$/;"	v
gotChunkForSendingOnce	network/BufferMessage.scala	/^  var gotChunkForSendingOnce = false$/;"	v
gotNext	util/NextIterator.scala	/^  private var gotNext = false$/;"	v
groupArr	rdd/CoalescedRDD.scala	/^  val groupArr = ArrayBuffer[PartitionGroup]()$/;"	V
groupBy	api/java/JavaRDDLike.scala	/^  def groupBy[K](f: JFunction[T, K]): JavaPairRDD[K, JList[T]] = {$/;"	m
groupBy	api/java/JavaRDDLike.scala	/^  def groupBy[K](f: JFunction[T, K], numPartitions: Int): JavaPairRDD[K, JList[T]] = {$/;"	m
groupBy	rdd/RDD.scala	/^  def groupBy[K: ClassManifest](f: T => K): RDD[(K, Seq[T])] =$/;"	m
groupBy	rdd/RDD.scala	/^  def groupBy[K: ClassManifest](f: T => K, numPartitions: Int): RDD[(K, Seq[T])] =$/;"	m
groupBy	rdd/RDD.scala	/^  def groupBy[K: ClassManifest](f: T => K, p: Partitioner): RDD[(K, Seq[T])] = {$/;"	m
groupByKey	api/java/JavaPairRDD.scala	/^  def groupByKey(): JavaPairRDD[K, JList[V]] =$/;"	m
groupByKey	api/java/JavaPairRDD.scala	/^  def groupByKey(numPartitions: Int): JavaPairRDD[K, JList[V]] =$/;"	m
groupByKey	api/java/JavaPairRDD.scala	/^  def groupByKey(partitioner: Partitioner): JavaPairRDD[K, JList[V]] =$/;"	m
groupByKey	rdd/PairRDDFunctions.scala	/^  def groupByKey(): RDD[(K, Seq[V])] = {$/;"	m
groupByKey	rdd/PairRDDFunctions.scala	/^  def groupByKey(numPartitions: Int): RDD[(K, Seq[V])] = {$/;"	m
groupByKey	rdd/PairRDDFunctions.scala	/^  def groupByKey(partitioner: Partitioner): RDD[(K, Seq[V])] = {$/;"	m
groupByResultToJava	api/java/JavaPairRDD.scala	/^  def groupByResultToJava[K, T](rdd: RDD[(K, Seq[T])])(implicit kcm: ClassManifest[K],$/;"	m
groupHash	rdd/CoalescedRDD.scala	/^  val groupHash = mutable.Map[String, ArrayBuffer[PartitionGroup]]()$/;"	V
groupWith	api/java/JavaPairRDD.scala	/^  def groupWith[W1, W2](other1: JavaPairRDD[K, W1], other2: JavaPairRDD[K, W2])$/;"	m
groupWith	api/java/JavaPairRDD.scala	/^  def groupWith[W](other: JavaPairRDD[K, W]): JavaPairRDD[K, (JList[V], JList[W])] =$/;"	m
groupWith	rdd/PairRDDFunctions.scala	/^  def groupWith[W1, W2](other1: RDD[(K, W1)], other2: RDD[(K, W2)])$/;"	m
groupWith	rdd/PairRDDFunctions.scala	/^  def groupWith[W](other: RDD[(K, W)]): RDD[(K, (Seq[V], Seq[W]))] = {$/;"	m
groupedRddBlocks	storage/StorageUtils.scala	/^    val groupedRddBlocks = infos.groupBy { case(k, v) => k.rddId }.mapValues(_.values.toArray)$/;"	V
grow	util/collection/OpenHashMap.scala	/^  protected var grow = (newCapacity: Int) => {$/;"	v
grow	util/collection/OpenHashSet.scala	/^  private val grow = grow1 _$/;"	V
grow	util/collection/PrimitiveKeyOpenHashMap.scala	/^  protected var grow = (newCapacity: Int) => {$/;"	v
growThreshold	util/AppendOnlyMap.scala	/^  private var growThreshold = LOAD_FACTOR * capacity$/;"	v
guess	util/SizeEstimator.scala	/^        val guess = Runtime.getRuntime.maxMemory < (32L*1024*1024*1024)$/;"	V
guessInWords	util/SizeEstimator.scala	/^        val guessInWords = if (guess) "yes" else "not"$/;"	V
guideMR	broadcast/BitTorrentBroadcast.scala	/^  @transient var guideMR: GuideMultipleRequests = null$/;"	v
guideMR	broadcast/TreeBroadcast.scala	/^  @transient var guideMR: GuideMultipleRequests = null$/;"	v
guidePort	broadcast/BitTorrentBroadcast.scala	/^  @transient var guidePort = -1$/;"	v
guidePort	broadcast/TreeBroadcast.scala	/^  @transient var guidePort = -1$/;"	v
guidePortLock	broadcast/BitTorrentBroadcast.scala	/^  @transient var guidePortLock = new Object$/;"	v
guidePortLock	broadcast/TreeBroadcast.scala	/^  @transient var guidePortLock = new Object$/;"	v
guideSocketToSource	broadcast/BitTorrentBroadcast.scala	/^          var guideSocketToSource: Socket = null$/;"	v
guideSocketToSource	broadcast/TreeBroadcast.scala	/^          var guideSocketToSource: Socket = null$/;"	v
hadAliveLocations	scheduler/cluster/ClusterTaskSetManager.scala	/^    var hadAliveLocations = false$/;"	v
hadoop	deploy/SparkHadoopUtil.scala	/^  private val hadoop = {$/;"	V
hadoopAttemptContext	rdd/NewHadoopRDD.scala	/^      val hadoopAttemptContext = newTaskAttemptContext(conf, attemptId)$/;"	V
hadoopConfiguration	SparkContext.scala	/^  val hadoopConfiguration = {$/;"	V
hadoopConfiguration	api/java/JavaSparkContext.scala	/^  def hadoopConfiguration(): Configuration = {$/;"	m
hadoopContext	rdd/PairRDDFunctions.scala	/^      val hadoopContext = newTaskAttemptContext(wrappedConf.value, attemptId)$/;"	V
hadoopFile	SparkContext.scala	/^  def hadoopFile[K, V, F <: InputFormat[K, V]](path: String)$/;"	m
hadoopFile	SparkContext.scala	/^  def hadoopFile[K, V, F <: InputFormat[K, V]](path: String, minSplits: Int)$/;"	m
hadoopFile	api/java/JavaSparkContext.scala	/^  def hadoopFile[K, V, F <: InputFormat[K, V]]($/;"	m
hadoopJobMetadata	SparkEnv.scala	/^  private[spark] val hadoopJobMetadata = new MapMaker().softValues().makeMap[String, Any]()$/;"	V
hadoopRDD	api/java/JavaSparkContext.scala	/^  def hadoopRDD[K, V, F <: InputFormat[K, V]]($/;"	m
hadoopSplit	rdd/HadoopRDD.scala	/^    val hadoopSplit = split.asInstanceOf[HadoopPartition]$/;"	V
handle	ui/JettyUtils.scala	/^      def handle(target: String,$/;"	m
handleConnectExecutor	network/ConnectionManager.scala	/^  private val handleConnectExecutor = new ThreadPoolExecutor($/;"	V
handleConnectionError	network/ConnectionManager.scala	/^  def handleConnectionError(connection: Connection, e: Exception) {$/;"	m
handleFailedTask	scheduler/cluster/ClusterTaskSetManager.scala	/^  def handleFailedTask(tid: Long, state: TaskState, reason: Option[TaskEndReason]) {$/;"	m
handleMessageExecutor	network/ConnectionManager.scala	/^  private val handleMessageExecutor = new ThreadPoolExecutor($/;"	V
handleReadWriteExecutor	network/ConnectionManager.scala	/^  private val handleReadWriteExecutor = new ThreadPoolExecutor($/;"	V
handleSuccessfulTask	scheduler/cluster/ClusterTaskSetManager.scala	/^  def handleSuccessfulTask(tid: Long, result: DirectTaskResult[_]) = {$/;"	m
handleTaskGettingResult	scheduler/cluster/ClusterScheduler.scala	/^  def handleTaskGettingResult(taskSetManager: ClusterTaskSetManager, tid: Long) {$/;"	m
handleTaskGettingResult	scheduler/cluster/ClusterTaskSetManager.scala	/^  def handleTaskGettingResult(tid: Long) = {$/;"	m
handler	network/netty/ShuffleCopier.scala	/^    val handler = new ShuffleCopier.ShuffleClientHandler(resultCollectCallback)$/;"	V
handlerList	HttpServer.scala	/^      val handlerList = new HandlerList$/;"	V
handlerList	ui/JettyUtils.scala	/^    val handlerList = new HandlerList$/;"	V
handlers	deploy/master/ui/MasterWebUI.scala	/^  val handlers = metricsHandlers ++ Array[(String, Handler)]($/;"	V
handlers	deploy/worker/ui/WorkerWebUI.scala	/^  val handlers = metricsHandlers ++ Array[(String, Handler)]($/;"	V
handlers	ui/SparkUI.scala	/^  val handlers = Seq[(String, Handler)]($/;"	V
handlersToRegister	ui/JettyUtils.scala	/^    val handlersToRegister = handlers.map { case(path, handler) =>$/;"	V
hasAckId	network/BufferMessage.scala	/^  def hasAckId() = (ackId != 0)$/;"	m
hasBlocks	broadcast/BitTorrentBroadcast.scala	/^  @transient var hasBlocks = new AtomicInteger(0)$/;"	v
hasBlocks	broadcast/MultiTracker.scala	/^ @transient var hasBlocks = 0 $/;"	v
hasBlocks	broadcast/SourceInfo.scala	/^  var hasBlocks = 0$/;"	v
hasBlocks	broadcast/TorrentBroadcast.scala	/^  @transient var hasBlocks = 0 $/;"	v
hasBlocks	broadcast/TorrentBroadcast.scala	/^  @transient var hasBlocks = 0$/;"	v
hasBlocks	broadcast/TreeBroadcast.scala	/^  @transient var hasBlocks = 0$/;"	v
hasBlocksBitVector	broadcast/BitTorrentBroadcast.scala	/^  @transient var hasBlocksBitVector: BitSet = null$/;"	v
hasBlocksBitVector	broadcast/SourceInfo.scala	/^  var hasBlocksBitVector: BitSet = new BitSet (totalBlocks)$/;"	v
hasBlocksLock	broadcast/TreeBroadcast.scala	/^  @transient var hasBlocksLock = new Object$/;"	v
hasExecutor	deploy/master/WorkerInfo.scala	/^  def hasExecutor(app: ApplicationInfo): Boolean = {$/;"	m
hasExecutorsAliveOnHost	scheduler/cluster/ClusterScheduler.scala	/^  def hasExecutorsAliveOnHost(host: String): Boolean = synchronized {$/;"	m
hasLaunchedTask	scheduler/cluster/ClusterScheduler.scala	/^  @volatile private var hasLaunchedTask = false$/;"	v
hasNext	InterruptibleIterator.scala	/^  def hasNext: Boolean = !context.interrupted && delegate.hasNext$/;"	m
hasNext	api/python/PythonRDD.scala	/^      def hasNext = _nextObj.length != 0$/;"	m
hasNext	rdd/CoalescedRDD.scala	/^    def hasNext(): Boolean = { !isEmpty }$/;"	m
hasNext	rdd/PipedRDD.scala	/^      def hasNext = {$/;"	m
hasNext	util/CompletionIterator.scala	/^  def hasNext = {$/;"	m
hasNext	util/collection/OpenHashMap.scala	/^    def hasNext = nextPair != null$/;"	m
hasNext	util/collection/PrimitiveKeyOpenHashMap.scala	/^    def hasNext = nextPair != null$/;"	m
hasPendingTasks	scheduler/Schedulable.scala	/^  def hasPendingTasks(): Boolean$/;"	m
hasPendingTasks	scheduler/cluster/ClusterScheduler.scala	/^  def hasPendingTasks: Boolean = {$/;"	m
hasRareBlocks	broadcast/BitTorrentBroadcast.scala	/^        var hasRareBlocks = 0$/;"	v
hasRead	broadcast/MultiTracker.scala	/^      val hasRead = bais.read(tempByteArray, 0, thisBlockSize)$/;"	V
hasRead	broadcast/TorrentBroadcast.scala	/^      val hasRead = bais.read(tempByteArray, 0, thisBlockSize)$/;"	V
hasReceivedTask	scheduler/cluster/ClusterScheduler.scala	/^  @volatile private var hasReceivedTask = false$/;"	v
hasRootAsShutdownDeleteDir	util/Utils.scala	/^  def hasRootAsShutdownDeleteDir(file: File): Boolean = {$/;"	m
hasShuffleRead	ui/jobs/StagePage.scala	/^      val hasShuffleRead = shuffleReadBytes > 0$/;"	V
hasShuffleWrite	ui/jobs/StagePage.scala	/^      val hasShuffleWrite = shuffleWriteBytes > 0$/;"	V
hasShutdownDeleteDir	util/Utils.scala	/^  def hasShutdownDeleteDir(file: File): Boolean = {$/;"	m
hash	network/netty/ShuffleSender.scala	/^        val hash = Utils.nonNegativeHash(blockId)$/;"	V
hash	storage/DiskBlockManager.scala	/^    val hash = Utils.nonNegativeHash(filename)$/;"	V
hash	util/Utils.scala	/^    val hash = obj.hashCode$/;"	V
hash	util/collection/OpenHashSet.scala	/^    def hash(o: T): Int = o.hashCode()$/;"	m
hashAbs	util/Utils.scala	/^    val hashAbs = if (Int.MinValue != hash) math.abs(hash) else 0$/;"	V
hashCode	scheduler/InputFormatInfo.scala	/^    var hashCode = inputFormatClazz.hashCode$/;"	v
hashCode	scheduler/SplitInfo.scala	/^    var hashCode = inputFormatClazz.hashCode$/;"	v
hasher	util/collection/OpenHashSet.scala	/^  protected val hasher: Hasher[T] = {$/;"	V
haveNullValue	util/AppendOnlyMap.scala	/^  private var haveNullValue = false$/;"	v
haveNullValue	util/collection/OpenHashMap.scala	/^  private var haveNullValue = false$/;"	v
havePair	rdd/NewHadoopRDD.scala	/^      var havePair = false$/;"	v
head	util/BoundedPriorityQueue.scala	/^    val head = underlying.peek()$/;"	V
header	deploy/worker/ExecutorRunner.scala	/^      val header = "Spark Executor Command: %s\\n%s\\n\\n".format($/;"	V
header	network/Connection.scala	/^          val header = MessageChunkHeader.create(headerBuffer)$/;"	V
header	network/MessageChunk.scala	/^class MessageChunk(val header: MessageChunkHeader, val buffer: ByteBuffer) {$/;"	V
header	network/netty/FileHeader.scala	/^    val header = new FileHeader(25, TestBlockId("my_block"))$/;"	V
headerBuffer	network/Connection.scala	/^  val headerBuffer: ByteBuffer = ByteBuffer.allocate(MessageChunkHeader.HEADER_SIZE)$/;"	V
headerBytesRead	network/Connection.scala	/^          val headerBytesRead = channel.read(headerBuffer)$/;"	V
headerSparkPage	ui/UIUtils.scala	/^  def headerSparkPage(content: => Seq[Node], sc: SparkContext, title: String, page: Page.Value)$/;"	m
heartBeatFrequency	storage/BlockManager.scala	/^  val heartBeatFrequency = BlockManager.getHeartBeatFrequencyFromSystemProperties$/;"	V
heartBeatTask	storage/BlockManager.scala	/^  var heartBeatTask: Cancellable = null$/;"	v
high	partial/CountEvaluator.scala	/^      val high = mean + confFactor * stdev$/;"	V
high	partial/GroupedCountEvaluator.scala	/^        val high = mean + confFactor * stdev$/;"	V
high	partial/GroupedMeanEvaluator.scala	/^        val high = mean + confFactor * stdev$/;"	V
high	partial/GroupedSumEvaluator.scala	/^        val high = sumEstimate + confFactor * sumStdev$/;"	V
high	partial/MeanEvaluator.scala	/^      val high = mean + confFactor * stdev$/;"	V
high	partial/SumEvaluator.scala	/^      val high = sumEstimate + confFactor * sumStdev$/;"	V
highBit	util/AppendOnlyMap.scala	/^    val highBit = Integer.highestOneBit(n)$/;"	V
highBit	util/collection/OpenHashSet.scala	/^    val highBit = Integer.highestOneBit(n)$/;"	V
hook	util/Utils.scala	/^      val hook = new Thread {$/;"	V
host	deploy/master/MasterArguments.scala	/^  var host = Utils.localHostName()$/;"	v
host	deploy/master/WorkerInfo.scala	/^    val host: String,$/;"	V
host	deploy/master/ui/MasterWebUI.scala	/^  val host = Utils.localHostName()$/;"	V
host	deploy/worker/ExecutorRunner.scala	/^    val host: String,$/;"	V
host	deploy/worker/WorkerArguments.scala	/^  var host = Utils.localHostName()$/;"	v
host	deploy/worker/ui/WorkerWebUI.scala	/^  val host = Utils.localHostName()$/;"	V
host	metrics/sink/GangliaSink.scala	/^  val host = propertyToOption(GANGLIA_KEY_HOST).get$/;"	V
host	network/netty/ShuffleCopier.scala	/^    val host = args(0)$/;"	V
host	scheduler/TaskInfo.scala	/^    val host: String,$/;"	V
host	scheduler/TaskLocation.scala	/^class TaskLocation private (val host: String, val executorId: Option[String]) extends Serializable {$/;"	V
host	scheduler/cluster/ClusterScheduler.scala	/^          val host = offers(i).host$/;"	V
host	scheduler/cluster/ClusterScheduler.scala	/^    val host = executorIdToHost(executorId)$/;"	V
host	storage/BlockManagerId.scala	/^  def host: String = host_$/;"	m
host	ui/SparkUI.scala	/^  val host = Option(System.getenv("SPARK_PUBLIC_DNS")).getOrElse(Utils.localHostName())$/;"	V
hostAddress	broadcast/BitTorrentBroadcast.scala	/^  @transient var hostAddress = Utils.localIpAddress$/;"	v
hostAddress	broadcast/TreeBroadcast.scala	/^  @transient var hostAddress = Utils.localIpAddress$/;"	v
hostPort	MapOutputTracker.scala	/^        val hostPort = Utils.localHostPort()$/;"	V
hostPort	deploy/master/WorkerInfo.scala	/^  def hostPort: String = {$/;"	m
hostPort	scheduler/cluster/ClusterScheduler.scala	/^        val hostPort = executorIdToHost(executorId)$/;"	V
hostPort	storage/BlockManager.scala	/^  val hostPort = Utils.localHostPort()$/;"	V
hostPort	storage/BlockManagerId.scala	/^  def hostPort: String = {$/;"	m
hostPort	ui/exec/ExecutorsUI.scala	/^    val hostPort = status.blockManagerId.hostPort$/;"	V
hostPortParseResults	util/Utils.scala	/^  private val hostPortParseResults = new ConcurrentHashMap[String, (String, Int)]()$/;"	V
host_	storage/BlockManagerId.scala	/^    private var host_ : String,$/;"	v
hostname	executor/TaskMetrics.scala	/^  var hostname: String = _$/;"	v
hotSpotMBeanClass	util/SizeEstimator.scala	/^      val hotSpotMBeanClass = Class.forName("com.sun.management.HotSpotDiagnosticMXBean")$/;"	V
hotSpotMBeanName	util/SizeEstimator.scala	/^      val hotSpotMBeanName = "com.sun.management:type=HotSpotDiagnostic"$/;"	V
hour	util/Utils.scala	/^    val hour = 60 * minute$/;"	V
hours	deploy/WebUI.scala	/^    val hours = minutes \/ 60$/;"	V
hours	scheduler/SparkListener.scala	/^  val hours = minutes * 60$/;"	V
httpFileServer	SparkEnv.scala	/^    val httpFileServer = new HttpFileServer()$/;"	V
httpFileServer	SparkEnv.scala	/^    val httpFileServer: HttpFileServer,$/;"	V
httpServer	HttpFileServer.scala	/^  var httpServer : HttpServer = null$/;"	v
i	broadcast/BitTorrentBroadcast.scala	/^              var i = -1$/;"	v
i	broadcast/BitTorrentBroadcast.scala	/^            var i = MultiTracker.ranGen.nextInt(minBlocksIndices.size)$/;"	v
i	broadcast/BitTorrentBroadcast.scala	/^          var i = MultiTracker.ranGen.nextInt(needBlocksBitVector.cardinality)$/;"	v
i	broadcast/BitTorrentBroadcast.scala	/^        var i = 0$/;"	v
i	broadcast/BitTorrentBroadcast.scala	/^        var i = MultiTracker.ranGen.nextInt(peersNotInUse.size)$/;"	v
i	util/AppendOnlyMap.scala	/^        var i = 1$/;"	v
i	util/AppendOnlyMap.scala	/^    var i = 1$/;"	v
i	util/Utils.scala	/^    var i = 0$/;"	v
i	util/Vector.scala	/^    var i = 0$/;"	v
i	util/collection/BitSet.scala	/^    var i = 0$/;"	v
i	util/collection/OpenHashSet.scala	/^    var i = 1$/;"	v
iableID	broadcast/BitTorrentBroadcast.scala	/^  def receiveBroadcast(variableID: Long): Boolean = {$/;"	v
iableID	broadcast/TorrentBroadcast.scala	/^  def receiveBroadcast(variableID: Long): Boolean = {$/;"	v
iableID	broadcast/TreeBroadcast.scala	/^  def receiveBroadcast(variableID: Long): Boolean = {$/;"	v
iableLong	broadcast/MultiTracker.scala	/^  def getGuideInfo(variableLong: Long): SourceInfo = {$/;"	v
iables	scheduler/cluster/ClusterTaskSetManager.scala	/^  \/\/ Delay scheduling variables: we keep track of our current locality level and the time we$/;"	v
iance	partial/CountEvaluator.scala	/^      val variance = (sum + 1) * (1 - p) \/ (p * p)$/;"	v
iance	partial/GroupedCountEvaluator.scala	/^        val variance = (sum + 1) * (1 - p) \/ (p * p)$/;"	v
iance	util/StatCounter.scala	/^  def variance: Double = {$/;"	v
ibmVendor	deploy/worker/WorkerArguments.scala	/^    val ibmVendor = System.getProperty("java.vendor").contains("IBM")$/;"	V
id	Accumulators.scala	/^  val id = Accumulators.newId$/;"	V
id	api/java/JavaRDDLike.scala	/^  def id: Int = rdd.id$/;"	m
id	broadcast/Broadcast.scala	/^abstract class Broadcast[T](private[spark] val id: Long) extends Serializable {$/;"	V
id	broadcast/MultiTracker.scala	/^                      val id = ois.readObject.asInstanceOf[Long]$/;"	V
id	deploy/FaultToleranceTest.scala	/^    var id: String = null$/;"	v
id	deploy/FaultToleranceTest.scala	/^private[spark] class DockerId(val id: String) {$/;"	V
id	deploy/master/ApplicationInfo.scala	/^        val id = nextExecutorId$/;"	V
id	deploy/master/ApplicationInfo.scala	/^    val id: String,$/;"	V
id	deploy/master/ExecutorInfo.scala	/^    val id: Int,$/;"	V
id	deploy/master/WorkerInfo.scala	/^    val id: String,$/;"	V
id	network/ConnectionManager.scala	/^  val id = new ConnectionManagerId(Utils.localHostName, serverChannel.socket.getLocalPort)$/;"	V
id	network/MessageChunkHeader.scala	/^    val id = buffer.getInt()$/;"	V
id	network/MessageChunkHeader.scala	/^    val id: Int,$/;"	V
id	rdd/RDD.scala	/^  val id: Int = sc.newRddId()$/;"	V
id	scheduler/DAGScheduler.scala	/^    val id = nextStageId.getAndIncrement()$/;"	V
id	scheduler/Stage.scala	/^    val id = nextAttemptId$/;"	V
id	scheduler/Stage.scala	/^    val id: Int,$/;"	V
id	scheduler/TaskSet.scala	/^    val id: String = stageId + "." + attempt$/;"	V
id	scheduler/cluster/mesos/CoarseMesosSchedulerBackend.scala	/^    val id = nextMesosTaskId$/;"	V
id	storage/BlockMessage.scala	/^  private var id: BlockId = null$/;"	v
id	ui/storage/RDDPage.scala	/^    val id = request.getParameter("id").toInt$/;"	V
id	util/IdGenerator.scala	/^  private var id = new AtomicInteger$/;"	v
idBuilder	network/netty/FileHeader.scala	/^    val idBuilder = new StringBuilder(idLength)$/;"	V
idBuilder	storage/BlockMessage.scala	/^    val idBuilder = new StringBuilder(idLength)$/;"	V
idLength	network/netty/FileHeader.scala	/^    val idLength = buf.readInt$/;"	V
idLength	storage/BlockMessage.scala	/^    val idLength = buffer.getInt()$/;"	V
idToActiveJob	scheduler/DAGScheduler.scala	/^  val idToActiveJob = new HashMap[Int, ActiveJob]$/;"	V
idToApp	deploy/master/Master.scala	/^  val idToApp = new HashMap[String, ApplicationInfo]$/;"	V
idToWorker	deploy/master/Master.scala	/^  val idToWorker = new HashMap[String, WorkerInfo]$/;"	V
ids	rdd/CoalescedRDD.scala	/^        val ids = pg.arr.map(_.index).toArray$/;"	V
idx	rdd/CartesianRDD.scala	/^      val idx = s1.index * numPartitionsInRdd2 + s2.index$/;"	V
idx	rdd/ShuffledRDD.scala	/^private[spark] class ShuffledRDDPartition(val idx: Int) extends Partition {$/;"	V
ign	ui/jobs/StageTable.scala	/^      <td valign="middle">{submissionTime}<\/td>$/;"	V
in	api/python/PythonRDD.scala	/^      val in = socket.getInputStream$/;"	V
in	api/python/PythonWorkerFactory.scala	/^              val in = daemon.getErrorStream$/;"	V
in	api/python/PythonWorkerFactory.scala	/^            val in = worker.getErrorStream$/;"	V
in	api/python/PythonWorkerFactory.scala	/^            val in = worker.getInputStream$/;"	V
in	api/python/PythonWorkerFactory.scala	/^        val in = new DataInputStream(daemon.getInputStream)$/;"	V
in	broadcast/HttpBroadcast.scala	/^    val in = {$/;"	V
in	broadcast/MultiTracker.scala	/^    val in = new ObjectInputStream (new ByteArrayInputStream (bytes)){$/;"	V
in	scheduler/ResultTask.scala	/^    val in = new GZIPInputStream(new ByteArrayInputStream(bytes))$/;"	V
in	scheduler/ShuffleMapTask.scala	/^      val in = new GZIPInputStream(new ByteArrayInputStream(bytes))$/;"	V
in	scheduler/ShuffleMapTask.scala	/^    val in = new GZIPInputStream(new ByteArrayInputStream(bytes))$/;"	V
in	scheduler/Task.scala	/^    val in = new ByteBufferInputStream(serializedTask)$/;"	V
in	serializer/JavaSerializer.scala	/^    val in = deserializeStream(bis)$/;"	V
in	serializer/JavaSerializer.scala	/^    val in = deserializeStream(bis, loader)$/;"	V
in	util/Utils.scala	/^        val in = fs.open(new Path(uri))$/;"	V
in	util/Utils.scala	/^        val in = new URL(url).openStream()$/;"	V
inDoubleQuote	util/Utils.scala	/^    var inDoubleQuote = false$/;"	v
inInterpreter	util/ClosureCleaner.scala	/^    val inInterpreter = {$/;"	V
inMem	storage/BlockManager.scala	/^          val inMem = level.useMemory && memoryStore.contains(blockId)$/;"	V
inShutdown	util/Utils.scala	/^  def inShutdown(): Boolean = {$/;"	m
inSingleQuote	util/Utils.scala	/^    var inSingleQuote = false$/;"	v
inWord	util/Utils.scala	/^    var inWord = false$/;"	v
inbox	network/Connection.scala	/^  val inbox = new Inbox()$/;"	V
increaseRunningTasks	scheduler/Pool.scala	/^  def increaseRunningTasks(taskNum: Int) {$/;"	m
increaseRunningTasks	scheduler/local/LocalTaskSetManager.scala	/^  def increaseRunningTasks(taskNum: Int): Unit = {$/;"	m
incrementRetryCount	deploy/master/ApplicationInfo.scala	/^  def incrementRetryCount = {$/;"	m
index	Partition.scala	/^  def index: Int$/;"	m
index	Partitioner.scala	/^          val index = (rddSample.length - 1) * (i + 1) \/ partitions$/;"	V
index	rdd/BlockRDD.scala	/^  val index = idx$/;"	V
index	rdd/CartesianRDD.scala	/^  override val index: Int = idx$/;"	V
index	rdd/CheckpointRDD.scala	/^private[spark] class CheckpointRDDPartition(val index: Int) extends Partition {}$/;"	V
index	rdd/CoGroupedRDD.scala	/^  override val index: Int = idx$/;"	V
index	rdd/HadoopRDD.scala	/^  override val index: Int = idx$/;"	V
index	rdd/NewHadoopRDD.scala	/^class NewHadoopPartition(rddId: Int, val index: Int, @transient rawSplit: InputSplit with Writable)$/;"	V
index	rdd/PairRDDFunctions.scala	/^        val index = p.getPartition(key)$/;"	V
index	rdd/PartitionPruningRDD.scala	/^  override val index = idx$/;"	V
index	rdd/SampledRDD.scala	/^  override val index: Int = prev.index$/;"	V
index	rdd/ShuffledRDD.scala	/^  override val index = idx$/;"	V
index	rdd/UnionRDD.scala	/^  override val index: Int = idx$/;"	V
index	rdd/ZippedPartitionsRDD.scala	/^  override val index: Int = idx$/;"	V
index	rdd/ZippedRDD.scala	/^  override val index: Int = idx$/;"	V
index	scheduler/TaskDescription.scala	/^    val index: Int,    \/\/ Index within this task's TaskSet$/;"	V
index	scheduler/TaskInfo.scala	/^    val index: Int,$/;"	V
index	scheduler/cluster/ClusterScheduler.scala	/^    var index = 0$/;"	v
index	scheduler/cluster/ClusterTaskSetManager.scala	/^        val index = info.index$/;"	V
index	scheduler/cluster/ClusterTaskSetManager.scala	/^        val index = taskInfos(tid).index$/;"	V
index	scheduler/cluster/ClusterTaskSetManager.scala	/^      val index = list.last$/;"	V
index	scheduler/cluster/ClusterTaskSetManager.scala	/^    val index = info.index$/;"	V
index	scheduler/cluster/ClusterTaskSetManager.scala	/^    var index = 0$/;"	v
index	scheduler/local/LocalTaskSetManager.scala	/^    val index = info.index$/;"	V
index	storage/ShuffleBlockManager.scala	/^      val index = mapIdToIndex.getOrElse(mapId, -1)$/;"	V
index	util/SizeEstimator.scala	/^          var index = 0$/;"	v
indexPage	deploy/master/ui/MasterWebUI.scala	/^  val indexPage = new IndexPage(this)$/;"	V
indexPage	deploy/worker/ui/WorkerWebUI.scala	/^  val indexPage = new IndexPage(this)$/;"	V
indexPage	ui/jobs/JobProgressUI.scala	/^  private val indexPage = new IndexPage(this)$/;"	V
indexPage	ui/storage/BlockManagerUI.scala	/^  val indexPage = new IndexPage(this)$/;"	V
indexToPrefs	SparkContext.scala	/^    val indexToPrefs = seq.zipWithIndex.map(t => (t._2, t._1._2)).toMap$/;"	V
indx	util/Utils.scala	/^    val indx: Int = hostPort.lastIndexOf(':')$/;"	V
inetSocketAddress	network/ConnectionManager.scala	/^      val inetSocketAddress = new InetSocketAddress(connectionManagerId.host, connectionManagerId.port)$/;"	V
inferDefaultCores	deploy/worker/WorkerArguments.scala	/^  def inferDefaultCores(): Int = {$/;"	m
inferDefaultMemory	deploy/worker/WorkerArguments.scala	/^  def inferDefaultMemory(): Int = {$/;"	m
inferredRemoteManagerId	network/Connection.scala	/^  @volatile private var inferredRemoteManagerId: ConnectionManagerId = null$/;"	v
info	scheduler/JobLogger.scala	/^    val info = " TID=" + taskInfo.taskId + " STAGE_ID=" + stageID +$/;"	V
info	scheduler/JobLogger.scala	/^    var info = "JOB_ID=" + job.jobId$/;"	v
info	scheduler/cluster/ClusterTaskSetManager.scala	/^          val info = new TaskInfo(taskId, index, curTime, execId, host, taskLocality)$/;"	V
info	scheduler/cluster/ClusterTaskSetManager.scala	/^    val info = taskInfos(tid)$/;"	V
info	scheduler/local/LocalTaskSetManager.scala	/^          val info = new TaskInfo(taskId, index, System.currentTimeMillis(), "local", "local:1",$/;"	V
info	scheduler/local/LocalTaskSetManager.scala	/^    val info = taskInfos(tid)$/;"	V
info	storage/BlockManager.scala	/^    val info = blockInfo.get(blockId).orNull$/;"	V
info	storage/BlockManagerMasterActor.scala	/^    val info = blockManagerInfo(blockManagerId)$/;"	V
info	util/SizeEstimator.scala	/^    val info = classInfos.get(cls)$/;"	V
info.ganglia.gmetric4j.gmetric.GMetric	metrics/sink/GangliaSink.scala	/^import info.ganglia.gmetric4j.gmetric.GMetric$/;"	i
init	api/python/PythonRDD.scala	/^              val init = initTime - bootTime$/;"	V
initTime	api/python/PythonRDD.scala	/^              val initTime = stream.readLong()$/;"	V
initialCount	rdd/RDD.scala	/^    val initialCount = this.count()$/;"	V
initialHash	rdd/CoalescedRDD.scala	/^  val initialHash = mutable.Set[Partition]()$/;"	V
initialPosition	storage/BlockObjectWriter.scala	/^  private val initialPosition = file.length()$/;"	V
initialSize	network/BufferMessage.scala	/^  val initialSize = currentSize()$/;"	V
initialValue	partial/PartialResult.scala	/^  def initialValue: R = initialVal$/;"	m
initialize	broadcast/BitTorrentBroadcast.scala	/^  def initialize(isDriver: Boolean) { MultiTracker.initialize(isDriver) }$/;"	m
initialize	broadcast/BroadcastFactory.scala	/^  def initialize(isDriver: Boolean): Unit$/;"	m
initialize	broadcast/HttpBroadcast.scala	/^  def initialize(isDriver: Boolean) { HttpBroadcast.initialize(isDriver) }$/;"	m
initialize	broadcast/HttpBroadcast.scala	/^  def initialize(isDriver: Boolean) {$/;"	m
initialize	broadcast/MultiTracker.scala	/^  def initialize(__isDriver: Boolean) {$/;"	m
initialize	broadcast/TorrentBroadcast.scala	/^  def initialize(_isDriver: Boolean) {$/;"	m
initialize	broadcast/TorrentBroadcast.scala	/^  def initialize(isDriver: Boolean) { TorrentBroadcast.initialize(isDriver) }$/;"	m
initialize	broadcast/TreeBroadcast.scala	/^  def initialize(isDriver: Boolean) { MultiTracker.initialize(isDriver) }$/;"	m
initialize	scheduler/cluster/ClusterScheduler.scala	/^  def initialize(context: SchedulerBackend) {$/;"	m
initialized	broadcast/Broadcast.scala	/^  private var initialized = false$/;"	v
initialized	broadcast/HttpBroadcast.scala	/^  private var initialized = false$/;"	v
initialized	broadcast/MultiTracker.scala	/^  private var initialized = false$/;"	v
initialized	broadcast/TorrentBroadcast.scala	/^  private var initialized = false$/;"	v
initialized	storage/BlockObjectWriter.scala	/^  private var initialized = false$/;"	v
innerClasses	util/ClosureCleaner.scala	/^    val innerClasses = getInnerClasses(func)$/;"	V
input	serializer/KryoSerializer.scala	/^  lazy val input = new KryoInput()$/;"	V
input	serializer/KryoSerializer.scala	/^  val input = new KryoInput(inStream)$/;"	V
inputFormat	rdd/HadoopRDD.scala	/^      val inputFormat = getInputFormat(jobConf)$/;"	V
inputFormat	rdd/HadoopRDD.scala	/^    val inputFormat = getInputFormat(jobConf)$/;"	V
inputFormat	rdd/NewHadoopRDD.scala	/^    val inputFormat = inputFormatClass.newInstance$/;"	V
inputFormatCacheKey	rdd/HadoopRDD.scala	/^  protected val inputFormatCacheKey = "rdd_%d_input_format".format(id)$/;"	V
inputFormatClass	SparkContext.scala	/^    val inputFormatClass = classOf[SequenceFileInputFormat[K, V]]$/;"	V
inputFormatClazz	scheduler/SplitInfo.scala	/^class SplitInfo(val inputFormatClazz: Class[_], val hostLocation: String, val path: String,$/;"	V
inputSplit	rdd/HadoopRDD.scala	/^  val inputSplit = new SerializableWritable[InputSplit](s)$/;"	V
inputSplits	rdd/HadoopRDD.scala	/^    val inputSplits = inputFormat.getSplits(jobConf, minSplits)$/;"	V
instConfig	metrics/MetricsSystem.scala	/^    val instConfig = metricsConfig.getInstance(instance)$/;"	V
instance	metrics/MetricsSystem.scala	/^private[spark] class MetricsSystem private (val instance: String) extends Logging {$/;"	V
instance	scheduler/InputFormatInfo.scala	/^    val instance: org.apache.hadoop.mapred.InputFormat[_, _] =$/;"	V
instance	scheduler/InputFormatInfo.scala	/^    val instance: org.apache.hadoop.mapreduce.InputFormat[_, _] =$/;"	V
instantiateClass	SparkEnv.scala	/^    def instantiateClass[T](propertyName: String, defaultClassName: String): T = {$/;"	m
instantiator	serializer/KryoSerializer.scala	/^    val instantiator = new EmptyScalaKryoInstantiator$/;"	V
intAccumulator	api/java/JavaSparkContext.scala	/^  def intAccumulator(initialValue: Int): Accumulator[java.lang.Integer] =$/;"	m
intToOpStr	network/ConnectionManager.scala	/^                  def intToOpStr(op: Int): String = {$/;"	m
integrate	rdd/SubtractedRDD.scala	/^    def integrate(dep: CoGroupSplitDep, op: Product2[K, V] => Unit) = dep match {$/;"	m
internalMap	util/TimeStampedHashMap.scala	/^  val internalMap = new ConcurrentHashMap[A, (B, Long)]()$/;"	V
internalMap	util/TimeStampedHashSet.scala	/^  val internalMap = new ConcurrentHashMap[A, Long]()$/;"	V
interpClass	util/ClosureCleaner.scala	/^        val interpClass = Class.forName("spark.repl.Main")$/;"	V
interrupted	TaskContext.scala	/^  @volatile var interrupted: Boolean = false,$/;"	v
io.netty.buffer.ByteBuf	network/netty/ShuffleCopier.scala	/^import io.netty.buffer.ByteBuf$/;"	i
io.netty.buffer.ByteBuf	storage/BlockFetcherIterator.scala	/^import io.netty.buffer.ByteBuf$/;"	i
io.netty.buffer._	network/netty/FileHeader.scala	/^import io.netty.buffer._$/;"	i
io.netty.channel.ChannelHandlerContext	network/netty/ShuffleCopier.scala	/^import io.netty.channel.ChannelHandlerContext$/;"	i
io.netty.util.CharsetUtil	network/netty/ShuffleCopier.scala	/^import io.netty.util.CharsetUtil$/;"	i
ip	deploy/FaultToleranceTest.scala	/^        val ip = line.split("=")(1)$/;"	V
ip	deploy/FaultToleranceTest.scala	/^    val ip = Await.result(ipPromise.future, 30 seconds)$/;"	V
ip	deploy/FaultToleranceTest.scala	/^private[spark] class TestMasterInfo(val ip: String, val dockerId: DockerId, val logFile: File)$/;"	V
ip	deploy/FaultToleranceTest.scala	/^private[spark] class TestWorkerInfo(val ip: String, val dockerId: DockerId, val logFile: File)$/;"	V
ip	network/MessageChunkHeader.scala	/^    val ip = InetAddress.getByAddress(ipBytes)$/;"	V
ip	network/MessageChunkHeader.scala	/^    val ip = address.getAddress.getAddress()$/;"	V
ipBytes	network/MessageChunkHeader.scala	/^    val ipBytes = new Array[Byte](ipSize)$/;"	V
ipPromise	deploy/FaultToleranceTest.scala	/^    val ipPromise = promise[String]()$/;"	V
ipSize	network/MessageChunkHeader.scala	/^    val ipSize = buffer.getInt()$/;"	V
is	metrics/MetricsConfig.scala	/^    var is: InputStream = null$/;"	v
is	scheduler/SchedulableBuilder.scala	/^    var is: Option[InputStream] = None$/;"	v
is64bit	util/SizeEstimator.scala	/^  private var is64bit = false$/;"	v
isAllowed	scheduler/TaskLocality.scala	/^  def isAllowed(constraint: TaskLocality, condition: TaskLocality): Boolean = {$/;"	m
isAlpha	util/Utils.scala	/^  def isAlpha(c: Char): Boolean = {$/;"	m
isAvailable	scheduler/Stage.scala	/^  def isAvailable: Boolean = {$/;"	m
isBroadcast	storage/BlockId.scala	/^  def isBroadcast = isInstanceOf[BroadcastBlockId] || isInstanceOf[BroadcastHelperBlockId]$/;"	m
isCheckpointed	api/java/JavaRDDLike.scala	/^  def isCheckpointed: Boolean = rdd.isCheckpointed$/;"	m
isCheckpointed	rdd/RDD.scala	/^  def isCheckpointed: Boolean = {$/;"	m
isCheckpointed	rdd/RDDCheckpointData.scala	/^  def isCheckpointed: Boolean = {$/;"	m
isCompletelyReceived	network/BufferMessage.scala	/^  def isCompletelyReceived() = !buffers(0).hasRemaining$/;"	m
isCompressedOops	util/SizeEstimator.scala	/^  private var isCompressedOops = false$/;"	v
isDriver	broadcast/Broadcast.scala	/^  def isDriver = _isDriver$/;"	m
isDriver	broadcast/MultiTracker.scala	/^  def isDriver = _isDriver$/;"	m
isEmpty	rdd/CoalescedRDD.scala	/^    override val isEmpty = !it.hasNext$/;"	V
isExecutorAlive	scheduler/cluster/ClusterScheduler.scala	/^  def isExecutorAlive(execId: String): Boolean = synchronized {$/;"	m
isFairScheduler	ui/jobs/StageTable.scala	/^  val isFairScheduler = listener.sc.getSchedulingMode == SchedulingMode.FAIR$/;"	V
isFinished	TaskState.scala	/^  def isFinished(state: TaskState) = FINISHED_STATES.contains(state)$/;"	m
isFinished	deploy/ExecutorState.scala	/^  def isFinished(state: ExecutorState): Boolean = Seq(KILLED, FAILED, LOST).contains(state)$/;"	m
isFinished	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^  def isFinished(state: MesosTaskState) = {$/;"	m
isFinished	util/SizeEstimator.scala	/^    def isFinished(): Boolean = stack.isEmpty$/;"	m
isInitialValueFinal	partial/PartialResult.scala	/^  def isInitialValueFinal: Boolean = isFinal$/;"	m
isLeader	deploy/master/ZooKeeperLeaderElectionAgent.scala	/^      val isLeader = myLeaderFile == leaderFile$/;"	V
isLocal	SparkContext.scala	/^  val isLocal = (master == "local" || master.startsWith("local["))$/;"	V
isLocal	storage/BlockManagerMasterActor.scala	/^class BlockManagerMasterActor(val isLocal: Boolean) extends Actor with Logging {$/;"	V
isOpen	storage/BlockObjectWriter.scala	/^  def isOpen: Boolean$/;"	m
isRDD	storage/BlockId.scala	/^  def isRDD = isInstanceOf[RDDBlockId]$/;"	m
isRegistered	scheduler/cluster/mesos/CoarseMesosSchedulerBackend.scala	/^  var isRegistered = false$/;"	v
isRegistered	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^  var isRegistered = false$/;"	v
isShuffle	storage/BlockId.scala	/^  def isShuffle = isInstanceOf[ShuffleBlockId]$/;"	m
isShuffleMap	scheduler/Stage.scala	/^  val isShuffleMap = shuffleDep != None$/;"	V
isSpace	util/Utils.scala	/^  def isSpace(c: Char): Boolean = {$/;"	m
isValid	storage/StorageLevel.scala	/^  def isValid = ((useMemory || useDisk) && (replication > 0))$/;"	m
isWrapper	util/Utils.scala	/^    val isWrapper = ser.deserializeStream(new InputStream {$/;"	V
isYarnMode	deploy/SparkHadoopUtil.scala	/^  def isYarnMode(): Boolean = { false }$/;"	m
it	rdd/CoalescedRDD.scala	/^    var it: Iterator[(String, Partition)] = resetIterator()$/;"	v
it.unimi.dsi.fastutil.ints.IntOpenHashSet	util/SizeEstimator.scala	/^import it.unimi.dsi.fastutil.ints.IntOpenHashSet$/;"	i
it.unimi.dsi.fastutil.io.FastBufferedInputStream	broadcast/HttpBroadcast.scala	/^import it.unimi.dsi.fastutil.io.FastBufferedInputStream$/;"	i
it.unimi.dsi.fastutil.io.FastBufferedOutputStream	broadcast/HttpBroadcast.scala	/^import it.unimi.dsi.fastutil.io.FastBufferedOutputStream$/;"	i
it.unimi.dsi.fastutil.io.FastBufferedOutputStream	storage/BlockObjectWriter.scala	/^import it.unimi.dsi.fastutil.io.FastBufferedOutputStream$/;"	i
it.unimi.dsi.fastutil.io.FastByteArrayOutputStream	scheduler/Task.scala	/^import it.unimi.dsi.fastutil.io.FastByteArrayOutputStream$/;"	i
it.unimi.dsi.fastutil.io.FastByteArrayOutputStream	serializer/Serializer.scala	/^import it.unimi.dsi.fastutil.io.FastByteArrayOutputStream$/;"	i
it.unimi.dsi.fastutil.io.{FastBufferedOutputStream, FastByteArrayOutputStream}	storage/BlockManager.scala	/^import it.unimi.dsi.fastutil.io.{FastBufferedOutputStream, FastByteArrayOutputStream}$/;"	i
it.unimi.dsi.fastutil.objects.{Object2LongOpenHashMap => OLMap}	partial/GroupedCountEvaluator.scala	/^import it.unimi.dsi.fastutil.objects.{Object2LongOpenHashMap => OLMap}$/;"	i
it.unimi.dsi.fastutil.objects.{Object2LongOpenHashMap => OLMap}	rdd/RDD.scala	/^import it.unimi.dsi.fastutil.objects.{Object2LongOpenHashMap => OLMap}$/;"	i
iter	partial/GroupedCountEvaluator.scala	/^      val iter = sums.object2LongEntrySet.fastIterator()$/;"	V
iter	partial/GroupedCountEvaluator.scala	/^    val iter = taskResult.object2LongEntrySet.fastIterator()$/;"	V
iter	partial/GroupedMeanEvaluator.scala	/^      val iter = sums.entrySet.iterator()$/;"	V
iter	partial/GroupedMeanEvaluator.scala	/^    val iter = taskResult.entrySet.iterator()$/;"	V
iter	partial/GroupedSumEvaluator.scala	/^      val iter = sums.entrySet.iterator()$/;"	V
iter	partial/GroupedSumEvaluator.scala	/^    val iter = taskResult.entrySet.iterator()$/;"	V
iter	rdd/HadoopRDD.scala	/^    val iter = new NextIterator[(K, V)] {$/;"	V
iter	rdd/NewHadoopRDD.scala	/^    val iter = new Iterator[(K, V)] {$/;"	V
iter	rdd/RDD.scala	/^      val iter = m2.object2LongEntrySet.fastIterator()$/;"	V
iter	rdd/SubtractedRDD.scala	/^        val iter = SparkEnv.get.shuffleFetcher.fetch[Product2[K, V]](shuffleId, partition.index,$/;"	V
iter	storage/BlockManager.scala	/^    val iter =$/;"	V
iterator	api/java/JavaRDDLike.scala	/^  def iterator(split: Partition, taskContext: TaskContext): java.util.Iterator[T] =$/;"	m
iterator	broadcast/HttpBroadcast.scala	/^    val iterator = files.internalMap.entrySet().iterator()$/;"	V
iterator	rdd/ParallelCollectionRDD.scala	/^  def iterator: Iterator[T] = values.iterator$/;"	m
iterator	rdd/UnionRDD.scala	/^  def iterator(context: TaskContext) = rdd.iterator(split, context)$/;"	m
iterator	scheduler/cluster/CoarseGrainedSchedulerBackend.scala	/^    val iterator = System.getProperties.entrySet.iterator$/;"	V
iterator	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^      val iterator = System.getProperties.entrySet.iterator$/;"	V
iterator	storage/BlockFetcherIterator.scala	/^          val iterator = blockInfos.iterator$/;"	V
iterator	storage/BlockManager.scala	/^    val iterator = blockInfo.internalMap.entrySet().iterator()$/;"	V
iterator	storage/BlockManagerMasterActor.scala	/^    val iterator = info.blocks.keySet.iterator$/;"	V
iterator	storage/BlockMessageArray.scala	/^  def iterator = blockMessages.iterator$/;"	m
iterator	storage/MemoryStore.scala	/^        val iterator = entries.entrySet().iterator()$/;"	V
iterator	util/TimeStampedHashMap.scala	/^    val iterator = internalMap.entrySet().iterator()$/;"	V
iterator	util/TimeStampedHashMap.scala	/^  def iterator: Iterator[(A, B)] = {$/;"	m
iterator	util/TimeStampedHashSet.scala	/^    val iterator = internalMap.entrySet().iterator()$/;"	V
iterator	util/TimeStampedHashSet.scala	/^  def iterator: Iterator[A] = {$/;"	m
iterators	rdd/CoalescedRDD.scala	/^      val iterators = (0 to 2).map( x =>$/;"	V
itr	BlockStoreShuffleFetcher.scala	/^    val itr = blockFetcherItr.flatMap(unpackBlock)$/;"	V
j	util/Utils.scala	/^      val j = rand.nextInt(i)$/;"	V
j	util/Utils.scala	/^      var j = i$/;"	v
jCtxt	SparkHadoopWriter.scala	/^    val jCtxt = getJobContext() $/;"	V
jID	SparkHadoopWriter.scala	/^  private var jID: SerializableWritable[JobID] = null$/;"	v
jIterator	util/TimeStampedHashMap.scala	/^    val jIterator = internalMap.entrySet().iterator()$/;"	V
jIterator	util/TimeStampedHashSet.scala	/^    val jIterator = internalMap.entrySet().iterator()$/;"	V
jarDir	HttpFileServer.scala	/^  var jarDir : File = null$/;"	v
jarOfClass	SparkContext.scala	/^  def jarOfClass(cls: Class[_]): Seq[String] = {$/;"	m
jarOfObject	SparkContext.scala	/^  def jarOfObject(obj: AnyRef): Seq[String] = jarOfClass(obj.getClass)$/;"	m
jars	SparkContext.scala	/^    val jars: Seq[String] = Nil,$/;"	V
java.io.EOFException	rdd/HadoopRDD.scala	/^import java.io.EOFException$/;"	i
java.io.File	HttpServer.scala	/^import java.io.File$/;"	i
java.io.File	deploy/worker/Worker.scala	/^import java.io.File$/;"	i
java.io.File	executor/Executor.scala	/^import java.io.File$/;"	i
java.io.File	metrics/sink/CsvSink.scala	/^import java.io.File$/;"	i
java.io.File	network/netty/ShuffleSender.scala	/^import java.io.File$/;"	i
java.io.File	scheduler/cluster/mesos/CoarseMesosSchedulerBackend.scala	/^import java.io.File$/;"	i
java.io.File	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^import java.io.File$/;"	i
java.io.File	storage/DiskBlockManager.scala	/^import java.io.File$/;"	i
java.io.File	storage/FileSegment.scala	/^import java.io.File$/;"	i
java.io.File	storage/ShuffleBlockManager.scala	/^import java.io.File$/;"	i
java.io.IOException	SparkHadoopWriter.scala	/^import java.io.IOException$/;"	i
java.io.InputStream	util/ByteBufferInputStream.scala	/^import java.io.InputStream$/;"	i
java.io.NotSerializableException	scheduler/DAGScheduler.scala	/^import java.io.NotSerializableException$/;"	i
java.io.NotSerializableException	scheduler/cluster/ClusterTaskSetManager.scala	/^import java.io.NotSerializableException$/;"	i
java.io.OutputStream	util/RateLimitedOutputStream.scala	/^import java.io.OutputStream$/;"	i
java.io.PrintStream	util/Distribution.scala	/^import java.io.PrintStream$/;"	i
java.io.PrintWriter	rdd/PipedRDD.scala	/^import java.io.PrintWriter$/;"	i
java.io.Serializable	util/BoundedPriorityQueue.scala	/^import java.io.Serializable$/;"	i
java.io._	Accumulators.scala	/^import java.io._$/;"	i
java.io._	MapOutputTracker.scala	/^import java.io._$/;"	i
java.io._	SerializableWritable.scala	/^import java.io._$/;"	i
java.io._	SparkContext.scala	/^import java.io._$/;"	i
java.io._	api/python/PythonRDD.scala	/^import java.io._$/;"	i
java.io._	broadcast/BitTorrentBroadcast.scala	/^import java.io._$/;"	i
java.io._	broadcast/Broadcast.scala	/^import java.io._$/;"	i
java.io._	broadcast/MultiTracker.scala	/^import java.io._$/;"	i
java.io._	broadcast/TorrentBroadcast.scala	/^import java.io._$/;"	i
java.io._	broadcast/TreeBroadcast.scala	/^import java.io._$/;"	i
java.io._	deploy/FaultToleranceTest.scala	/^import java.io._$/;"	i
java.io._	deploy/master/FileSystemPersistenceEngine.scala	/^import java.io._$/;"	i
java.io._	deploy/worker/ExecutorRunner.scala	/^import java.io._$/;"	i
java.io._	network/Connection.scala	/^import java.io._$/;"	i
java.io._	rdd/ParallelCollectionRDD.scala	/^import java.io._$/;"	i
java.io._	scheduler/ResultTask.scala	/^import java.io._$/;"	i
java.io._	scheduler/ShuffleMapTask.scala	/^import java.io._$/;"	i
java.io._	scheduler/TaskResult.scala	/^import java.io._$/;"	i
java.io._	serializer/JavaSerializer.scala	/^import java.io._$/;"	i
java.io._	util/Utils.scala	/^import java.io._$/;"	i
java.io.{DataInputStream, DataOutputStream}	scheduler/Task.scala	/^import java.io.{DataInputStream, DataOutputStream}$/;"	i
java.io.{EOFException, InputStream, OutputStream}	serializer/KryoSerializer.scala	/^import java.io.{EOFException, InputStream, OutputStream}$/;"	i
java.io.{EOFException, InputStream, OutputStream}	serializer/Serializer.scala	/^import java.io.{EOFException, InputStream, OutputStream}$/;"	i
java.io.{Externalizable, IOException, ObjectInput, ObjectOutput}	storage/BlockManagerId.scala	/^import java.io.{Externalizable, IOException, ObjectInput, ObjectOutput}$/;"	i
java.io.{Externalizable, IOException, ObjectInput, ObjectOutput}	storage/StorageLevel.scala	/^import java.io.{Externalizable, IOException, ObjectInput, ObjectOutput}$/;"	i
java.io.{Externalizable, ObjectInput, ObjectOutput}	storage/BlockManagerMessages.scala	/^import java.io.{Externalizable, ObjectInput, ObjectOutput}$/;"	i
java.io.{File, FileInputStream, InputStream, IOException}	metrics/MetricsConfig.scala	/^import java.io.{File, FileInputStream, InputStream, IOException}$/;"	i
java.io.{File, FileOutputStream, ObjectInputStream, OutputStream}	broadcast/HttpBroadcast.scala	/^import java.io.{File, FileOutputStream, ObjectInputStream, OutputStream}$/;"	i
java.io.{File, IOException, EOFException}	rdd/CheckpointRDD.scala	/^import java.io.{File, IOException, EOFException}$/;"	i
java.io.{File, InputStream, OutputStream}	storage/BlockManager.scala	/^import java.io.{File, InputStream, OutputStream}$/;"	i
java.io.{FileInputStream, File}	deploy/worker/ui/WorkerWebUI.scala	/^import java.io.{FileInputStream, File}$/;"	i
java.io.{FileInputStream, InputStream}	scheduler/SchedulableBuilder.scala	/^import java.io.{FileInputStream, InputStream}$/;"	i
java.io.{FileOutputStream, File, OutputStream}	storage/BlockObjectWriter.scala	/^import java.io.{FileOutputStream, File, OutputStream}$/;"	i
java.io.{FileOutputStream, RandomAccessFile}	storage/DiskStore.scala	/^import java.io.{FileOutputStream, RandomAccessFile}$/;"	i
java.io.{File}	HttpFileServer.scala	/^import java.io.{File}$/;"	i
java.io.{IOException, File, FileNotFoundException, PrintWriter}	scheduler/JobLogger.scala	/^import java.io.{IOException, File, FileNotFoundException, PrintWriter}$/;"	i
java.io.{IOException, ObjectOutputStream, EOFException, ObjectInputStream}	util/SerializableBuffer.scala	/^import java.io.{IOException, ObjectOutputStream, EOFException, ObjectInputStream}$/;"	i
java.io.{InputStream, IOException, ByteArrayOutputStream, ByteArrayInputStream, BufferedInputStream}	util/ClosureCleaner.scala	/^import java.io.{InputStream, IOException, ByteArrayOutputStream, ByteArrayInputStream, BufferedInputStream}$/;"	i
java.io.{InputStream, OutputStream}	io/CompressionCodec.scala	/^import java.io.{InputStream, OutputStream}$/;"	i
java.io.{ObjectOutput, ObjectInput, Externalizable}	scheduler/MapStatus.scala	/^import java.io.{ObjectOutput, ObjectInput, Externalizable}$/;"	i
java.io.{ObjectOutputStream, IOException}	rdd/CartesianRDD.scala	/^import java.io.{ObjectOutputStream, IOException}$/;"	i
java.io.{ObjectOutputStream, IOException}	rdd/CoGroupedRDD.scala	/^import java.io.{ObjectOutputStream, IOException}$/;"	i
java.io.{ObjectOutputStream, IOException}	rdd/CoalescedRDD.scala	/^import java.io.{ObjectOutputStream, IOException}$/;"	i
java.io.{ObjectOutputStream, IOException}	rdd/UnionRDD.scala	/^import java.io.{ObjectOutputStream, IOException}$/;"	i
java.io.{ObjectOutputStream, IOException}	rdd/ZippedPartitionsRDD.scala	/^import java.io.{ObjectOutputStream, IOException}$/;"	i
java.io.{ObjectOutputStream, IOException}	rdd/ZippedRDD.scala	/^import java.io.{ObjectOutputStream, IOException}$/;"	i
java.io.{OutputStreamWriter, File, DataInputStream, IOException}	api/python/PythonWorkerFactory.scala	/^import java.io.{OutputStreamWriter, File, DataInputStream, IOException}$/;"	i
java.lang.Double	api/java/JavaDoubleRDD.scala	/^import java.lang.Double$/;"	i
java.lang.System.getenv	deploy/worker/ExecutorRunner.scala	/^import java.lang.System.getenv$/;"	i
java.lang.management.ManagementFactory	deploy/worker/WorkerArguments.scala	/^import java.lang.management.ManagementFactory$/;"	i
java.lang.management.ManagementFactory	executor/Executor.scala	/^import java.lang.management.ManagementFactory$/;"	i
java.lang.management.ManagementFactory	util/SizeEstimator.scala	/^import java.lang.management.ManagementFactory$/;"	i
java.lang.reflect.Field	util/ClosureCleaner.scala	/^import java.lang.reflect.Field$/;"	i
java.lang.reflect.Field	util/SizeEstimator.scala	/^import java.lang.reflect.Field$/;"	i
java.lang.reflect.Modifier	util/SizeEstimator.scala	/^import java.lang.reflect.Modifier$/;"	i
java.lang.reflect.{Array => JArray}	util/SizeEstimator.scala	/^import java.lang.reflect.{Array => JArray}$/;"	i
java.net.InetAddress	HttpServer.scala	/^import java.net.InetAddress$/;"	i
java.net.InetAddress	network/ConnectionManagerTest.scala	/^import java.net.InetAddress$/;"	i
java.net.InetAddress	network/MessageChunkHeader.scala	/^import java.net.InetAddress$/;"	i
java.net.InetAddress	network/ReceiverTest.scala	/^import java.net.InetAddress$/;"	i
java.net.InetAddress	network/SenderTest.scala	/^import java.net.InetAddress$/;"	i
java.net.InetSocketAddress	network/ConnectionManagerId.scala	/^import java.net.InetSocketAddress$/;"	i
java.net.InetSocketAddress	network/Message.scala	/^import java.net.InetSocketAddress$/;"	i
java.net.InetSocketAddress	network/MessageChunkHeader.scala	/^import java.net.InetSocketAddress$/;"	i
java.net.URI	SparkContext.scala	/^import java.net.URI$/;"	i
java.net.URL	broadcast/HttpBroadcast.scala	/^import java.net.URL$/;"	i
java.net.URL	deploy/FaultToleranceTest.scala	/^import java.net.URL$/;"	i
java.net._	api/python/PythonRDD.scala	/^import java.net._$/;"	i
java.net._	broadcast/BitTorrentBroadcast.scala	/^import java.net._$/;"	i
java.net._	broadcast/MultiTracker.scala	/^import java.net._$/;"	i
java.net._	broadcast/TreeBroadcast.scala	/^import java.net._$/;"	i
java.net._	network/Connection.scala	/^import java.net._$/;"	i
java.net._	network/ConnectionManager.scala	/^import java.net._$/;"	i
java.net.{InetAddress, URL, URI, NetworkInterface, Inet4Address}	util/Utils.scala	/^import java.net.{InetAddress, URL, URI, NetworkInterface, Inet4Address}$/;"	i
java.net.{ServerSocket, Socket, SocketException, InetAddress}	api/python/PythonWorkerFactory.scala	/^import java.net.{ServerSocket, Socket, SocketException, InetAddress}$/;"	i
java.net.{URLClassLoader, URL}	executor/ExecutorURLClassLoader.scala	/^import java.net.{URLClassLoader, URL}$/;"	i
java.nio.ByteBuffer	executor/CoarseGrainedExecutorBackend.scala	/^import java.nio.ByteBuffer$/;"	i
java.nio.ByteBuffer	executor/Executor.scala	/^import java.nio.ByteBuffer$/;"	i
java.nio.ByteBuffer	executor/ExecutorBackend.scala	/^import java.nio.ByteBuffer$/;"	i
java.nio.ByteBuffer	executor/MesosExecutorBackend.scala	/^import java.nio.ByteBuffer$/;"	i
java.nio.ByteBuffer	network/BufferMessage.scala	/^import java.nio.ByteBuffer$/;"	i
java.nio.ByteBuffer	network/ConnectionManagerTest.scala	/^import java.nio.ByteBuffer$/;"	i
java.nio.ByteBuffer	network/Message.scala	/^import java.nio.ByteBuffer$/;"	i
java.nio.ByteBuffer	network/MessageChunk.scala	/^import java.nio.ByteBuffer$/;"	i
java.nio.ByteBuffer	network/MessageChunkHeader.scala	/^import java.nio.ByteBuffer$/;"	i
java.nio.ByteBuffer	network/ReceiverTest.scala	/^import java.nio.ByteBuffer$/;"	i
java.nio.ByteBuffer	network/SenderTest.scala	/^import java.nio.ByteBuffer$/;"	i
java.nio.ByteBuffer	rdd/PairRDDFunctions.scala	/^import java.nio.ByteBuffer$/;"	i
java.nio.ByteBuffer	scheduler/Task.scala	/^import java.nio.ByteBuffer$/;"	i
java.nio.ByteBuffer	scheduler/TaskDescription.scala	/^import java.nio.ByteBuffer$/;"	i
java.nio.ByteBuffer	scheduler/TaskResult.scala	/^import java.nio.ByteBuffer$/;"	i
java.nio.ByteBuffer	scheduler/TaskSetManager.scala	/^import java.nio.ByteBuffer$/;"	i
java.nio.ByteBuffer	scheduler/cluster/ClusterScheduler.scala	/^import java.nio.ByteBuffer$/;"	i
java.nio.ByteBuffer	scheduler/cluster/CoarseGrainedClusterMessage.scala	/^import java.nio.ByteBuffer$/;"	i
java.nio.ByteBuffer	scheduler/cluster/TaskResultGetter.scala	/^import java.nio.ByteBuffer$/;"	i
java.nio.ByteBuffer	scheduler/local/LocalScheduler.scala	/^import java.nio.ByteBuffer$/;"	i
java.nio.ByteBuffer	scheduler/local/LocalTaskSetManager.scala	/^import java.nio.ByteBuffer$/;"	i
java.nio.ByteBuffer	serializer/JavaSerializer.scala	/^import java.nio.ByteBuffer$/;"	i
java.nio.ByteBuffer	serializer/KryoSerializer.scala	/^import java.nio.ByteBuffer$/;"	i
java.nio.ByteBuffer	serializer/Serializer.scala	/^import java.nio.ByteBuffer$/;"	i
java.nio.ByteBuffer	storage/BlockFetcherIterator.scala	/^import java.nio.ByteBuffer$/;"	i
java.nio.ByteBuffer	storage/BlockManagerWorker.scala	/^import java.nio.ByteBuffer$/;"	i
java.nio.ByteBuffer	storage/BlockMessage.scala	/^import java.nio.ByteBuffer$/;"	i
java.nio.ByteBuffer	storage/BlockMessageArray.scala	/^import java.nio.ByteBuffer$/;"	i
java.nio.ByteBuffer	storage/BlockStore.scala	/^import java.nio.ByteBuffer$/;"	i
java.nio.ByteBuffer	storage/DiskStore.scala	/^import java.nio.ByteBuffer$/;"	i
java.nio.ByteBuffer	storage/MemoryStore.scala	/^import java.nio.ByteBuffer$/;"	i
java.nio.ByteBuffer	storage/PutResult.scala	/^import java.nio.ByteBuffer$/;"	i
java.nio.ByteBuffer	util/ByteBufferInputStream.scala	/^import java.nio.ByteBuffer$/;"	i
java.nio.ByteBuffer	util/SerializableBuffer.scala	/^import java.nio.ByteBuffer$/;"	i
java.nio.ByteBuffer	util/Utils.scala	/^import java.nio.ByteBuffer$/;"	i
java.nio._	network/Connection.scala	/^import java.nio._$/;"	i
java.nio._	network/ConnectionManager.scala	/^import java.nio._$/;"	i
java.nio.channels.Channels	util/SerializableBuffer.scala	/^import java.nio.channels.Channels$/;"	i
java.nio.channels.FileChannel	storage/BlockObjectWriter.scala	/^import java.nio.channels.FileChannel$/;"	i
java.nio.channels.FileChannel.MapMode	storage/DiskStore.scala	/^import java.nio.channels.FileChannel.MapMode$/;"	i
java.nio.channels._	network/Connection.scala	/^import java.nio.channels._$/;"	i
java.nio.channels._	network/ConnectionManager.scala	/^import java.nio.channels._$/;"	i
java.nio.channels.spi._	network/Connection.scala	/^import java.nio.channels.spi._$/;"	i
java.nio.channels.spi._	network/ConnectionManager.scala	/^import java.nio.channels.spi._$/;"	i
java.nio.{ByteBuffer, MappedByteBuffer}	storage/BlockManager.scala	/^import java.nio.{ByteBuffer, MappedByteBuffer}$/;"	i
java.security.PrivilegedExceptionAction	deploy/SparkHadoopUtil.scala	/^import java.security.PrivilegedExceptionAction$/;"	i
java.sql.{Connection, ResultSet}	rdd/JdbcRDD.scala	/^import java.sql.{Connection, ResultSet}$/;"	i
java.text.NumberFormat	SparkHadoopWriter.scala	/^import java.text.NumberFormat$/;"	i
java.text.NumberFormat	rdd/CheckpointRDD.scala	/^import java.text.NumberFormat$/;"	i
java.text.SimpleDateFormat	SparkHadoopWriter.scala	/^import java.text.SimpleDateFormat$/;"	i
java.text.SimpleDateFormat	deploy/WebUI.scala	/^import java.text.SimpleDateFormat$/;"	i
java.text.SimpleDateFormat	deploy/master/Master.scala	/^import java.text.SimpleDateFormat$/;"	i
java.text.SimpleDateFormat	deploy/worker/Worker.scala	/^import java.text.SimpleDateFormat$/;"	i
java.text.SimpleDateFormat	rdd/NewHadoopRDD.scala	/^import java.text.SimpleDateFormat$/;"	i
java.text.SimpleDateFormat	rdd/PairRDDFunctions.scala	/^import java.text.SimpleDateFormat$/;"	i
java.text.SimpleDateFormat	scheduler/JobLogger.scala	/^import java.text.SimpleDateFormat$/;"	i
java.text.SimpleDateFormat	storage/DiskBlockManager.scala	/^import java.text.SimpleDateFormat$/;"	i
java.text.SimpleDateFormat	ui/jobs/JobProgressUI.scala	/^import java.text.SimpleDateFormat$/;"	i
java.util.Arrays	api/python/PythonPartitioner.scala	/^import java.util.Arrays$/;"	i
java.util.Arrays	scheduler/cluster/ClusterTaskSetManager.scala	/^import java.util.Arrays$/;"	i
java.util.BitSet	broadcast/SourceInfo.scala	/^import java.util.BitSet$/;"	i
java.util.Collections	scheduler/cluster/mesos/CoarseMesosSchedulerBackend.scala	/^import java.util.Collections$/;"	i
java.util.Collections	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^import java.util.Collections$/;"	i
java.util.Comparator	api/java/JavaPairRDD.scala	/^import java.util.Comparator$/;"	i
java.util.Date	SparkHadoopWriter.scala	/^import java.util.Date$/;"	i
java.util.Date	deploy/WebUI.scala	/^import java.util.Date$/;"	i
java.util.Date	deploy/master/ApplicationInfo.scala	/^import java.util.Date$/;"	i
java.util.Date	deploy/master/Master.scala	/^import java.util.Date$/;"	i
java.util.Date	deploy/worker/Worker.scala	/^import java.util.Date$/;"	i
java.util.Date	rdd/NewHadoopRDD.scala	/^import java.util.Date$/;"	i
java.util.Date	rdd/PairRDDFunctions.scala	/^import java.util.Date$/;"	i
java.util.Date	ui/jobs/StagePage.scala	/^import java.util.Date$/;"	i
java.util.Date	ui/jobs/StageTable.scala	/^import java.util.Date$/;"	i
java.util.IdentityHashMap	util/SizeEstimator.scala	/^import java.util.IdentityHashMap$/;"	i
java.util.LinkedHashMap	storage/MemoryStore.scala	/^import java.util.LinkedHashMap$/;"	i
java.util.Properties	SparkContext.scala	/^import java.util.Properties$/;"	i
java.util.Properties	metrics/MetricsConfig.scala	/^import java.util.Properties$/;"	i
java.util.Properties	metrics/MetricsSystem.scala	/^import java.util.Properties$/;"	i
java.util.Properties	metrics/sink/ConsoleSink.scala	/^import java.util.Properties$/;"	i
java.util.Properties	metrics/sink/GangliaSink.scala	/^import java.util.Properties$/;"	i
java.util.Properties	metrics/sink/JmxSink.scala	/^import java.util.Properties$/;"	i
java.util.Properties	metrics/sink/MetricsServlet.scala	/^import java.util.Properties$/;"	i
java.util.Properties	scheduler/ActiveJob.scala	/^import java.util.Properties$/;"	i
java.util.Properties	scheduler/DAGScheduler.scala	/^import java.util.Properties$/;"	i
java.util.Properties	scheduler/DAGSchedulerEvent.scala	/^import java.util.Properties$/;"	i
java.util.Properties	scheduler/SparkListener.scala	/^import java.util.Properties$/;"	i
java.util.Properties	scheduler/TaskSet.scala	/^import java.util.Properties$/;"	i
java.util.Random	broadcast/MultiTracker.scala	/^import java.util.Random$/;"	i
java.util.Random	rdd/RDD.scala	/^import java.util.Random$/;"	i
java.util.Random	rdd/SampledRDD.scala	/^import java.util.Random$/;"	i
java.util.Random	util/SizeEstimator.scala	/^import java.util.Random$/;"	i
java.util.StringTokenizer	rdd/PipedRDD.scala	/^import java.util.StringTokenizer$/;"	i
java.util.concurrent.ArrayBlockingQueue	storage/MemoryStore.scala	/^import java.util.concurrent.ArrayBlockingQueue$/;"	i
java.util.concurrent.ArrayBlockingQueue	storage/ThreadingTest.scala	/^import java.util.concurrent.ArrayBlockingQueue$/;"	i
java.util.concurrent.ConcurrentHashMap	serializer/SerializerManager.scala	/^import java.util.concurrent.ConcurrentHashMap$/;"	i
java.util.concurrent.ConcurrentHashMap	storage/BlockInfo.scala	/^import java.util.concurrent.ConcurrentHashMap$/;"	i
java.util.concurrent.ConcurrentHashMap	storage/BlockManagerId.scala	/^import java.util.concurrent.ConcurrentHashMap$/;"	i
java.util.concurrent.ConcurrentHashMap	util/SizeEstimator.scala	/^import java.util.concurrent.ConcurrentHashMap$/;"	i
java.util.concurrent.ConcurrentHashMap	util/TimeStampedHashMap.scala	/^import java.util.concurrent.ConcurrentHashMap$/;"	i
java.util.concurrent.ConcurrentHashMap	util/TimeStampedHashSet.scala	/^import java.util.concurrent.ConcurrentHashMap$/;"	i
java.util.concurrent.ConcurrentLinkedQueue	storage/ShuffleBlockManager.scala	/^import java.util.concurrent.ConcurrentLinkedQueue$/;"	i
java.util.concurrent.Executors	network/netty/ShuffleCopier.scala	/^import java.util.concurrent.Executors$/;"	i
java.util.concurrent.LinkedBlockingQueue	scheduler/JobLogger.scala	/^import java.util.concurrent.LinkedBlockingQueue$/;"	i
java.util.concurrent.LinkedBlockingQueue	scheduler/SparkListenerBus.scala	/^import java.util.concurrent.LinkedBlockingQueue$/;"	i
java.util.concurrent.LinkedBlockingQueue	storage/BlockFetcherIterator.scala	/^import java.util.concurrent.LinkedBlockingQueue$/;"	i
java.util.concurrent.TimeUnit	metrics/MetricsSystem.scala	/^import java.util.concurrent.TimeUnit$/;"	i
java.util.concurrent.TimeUnit	metrics/sink/ConsoleSink.scala	/^import java.util.concurrent.TimeUnit$/;"	i
java.util.concurrent.TimeUnit	metrics/sink/CsvSink.scala	/^import java.util.concurrent.TimeUnit$/;"	i
java.util.concurrent.TimeUnit	metrics/sink/GangliaSink.scala	/^import java.util.concurrent.TimeUnit$/;"	i
java.util.concurrent.TimeUnit	metrics/sink/MetricsServlet.scala	/^import java.util.concurrent.TimeUnit$/;"	i
java.util.concurrent.TimeUnit._	util/RateLimitedOutputStream.scala	/^import java.util.concurrent.TimeUnit._$/;"	i
java.util.concurrent.TimeoutException	deploy/FaultToleranceTest.scala	/^import java.util.concurrent.TimeoutException$/;"	i
java.util.concurrent.TimeoutException	deploy/client/Client.scala	/^import java.util.concurrent.TimeoutException$/;"	i
java.util.concurrent._	executor/Executor.scala	/^import java.util.concurrent._$/;"	i
java.util.concurrent.atomic.AtomicInteger	SparkContext.scala	/^import java.util.concurrent.atomic.AtomicInteger$/;"	i
java.util.concurrent.atomic.AtomicInteger	broadcast/BitTorrentBroadcast.scala	/^import java.util.concurrent.atomic.AtomicInteger$/;"	i
java.util.concurrent.atomic.AtomicInteger	scheduler/DAGScheduler.scala	/^import java.util.concurrent.atomic.AtomicInteger$/;"	i
java.util.concurrent.atomic.AtomicInteger	scheduler/cluster/CoarseGrainedSchedulerBackend.scala	/^import java.util.concurrent.atomic.AtomicInteger$/;"	i
java.util.concurrent.atomic.AtomicInteger	scheduler/local/LocalScheduler.scala	/^import java.util.concurrent.atomic.AtomicInteger$/;"	i
java.util.concurrent.atomic.AtomicInteger	storage/ShuffleBlockManager.scala	/^import java.util.concurrent.atomic.AtomicInteger$/;"	i
java.util.concurrent.atomic.AtomicInteger	util/IdGenerator.scala	/^import java.util.concurrent.atomic.AtomicInteger$/;"	i
java.util.concurrent.atomic.AtomicLong	broadcast/Broadcast.scala	/^import java.util.concurrent.atomic.AtomicLong$/;"	i
java.util.concurrent.atomic.AtomicLong	rdd/AsyncRDDActions.scala	/^import java.util.concurrent.atomic.AtomicLong$/;"	i
java.util.concurrent.atomic.AtomicLong	scheduler/cluster/ClusterScheduler.scala	/^import java.util.concurrent.atomic.AtomicLong$/;"	i
java.util.concurrent.atomic.AtomicLong	storage/StoragePerfTester.scala	/^import java.util.concurrent.atomic.AtomicLong$/;"	i
java.util.concurrent.{ConcurrentHashMap, Executors, ThreadPoolExecutor}	util/Utils.scala	/^import java.util.concurrent.{ConcurrentHashMap, Executors, ThreadPoolExecutor}$/;"	i
java.util.concurrent.{CountDownLatch, Executors}	storage/StoragePerfTester.scala	/^import java.util.concurrent.{CountDownLatch, Executors}$/;"	i
java.util.concurrent.{LinkedBlockingDeque, ThreadFactory, ThreadPoolExecutor, TimeUnit}	scheduler/cluster/TaskResultGetter.scala	/^import java.util.concurrent.{LinkedBlockingDeque, ThreadFactory, ThreadPoolExecutor, TimeUnit}$/;"	i
java.util.concurrent.{LinkedBlockingDeque, TimeUnit, ThreadPoolExecutor}	network/ConnectionManager.scala	/^import java.util.concurrent.{LinkedBlockingDeque, TimeUnit, ThreadPoolExecutor}$/;"	i
java.util.concurrent.{LinkedBlockingQueue, TimeUnit}	scheduler/DAGScheduler.scala	/^import java.util.concurrent.{LinkedBlockingQueue, TimeUnit}$/;"	i
java.util.zip.{GZIPInputStream, GZIPOutputStream}	MapOutputTracker.scala	/^import java.util.zip.{GZIPInputStream, GZIPOutputStream}$/;"	i
java.util.zip.{GZIPInputStream, GZIPOutputStream}	scheduler/ResultTask.scala	/^import java.util.zip.{GZIPInputStream, GZIPOutputStream}$/;"	i
java.util.zip.{GZIPInputStream, GZIPOutputStream}	scheduler/ShuffleMapTask.scala	/^import java.util.zip.{GZIPInputStream, GZIPOutputStream}$/;"	i
java.util.{ArrayList => JArrayList, List => JList}	scheduler/cluster/mesos/CoarseMesosSchedulerBackend.scala	/^import java.util.{ArrayList => JArrayList, List => JList}$/;"	i
java.util.{ArrayList => JArrayList, List => JList}	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^import java.util.{ArrayList => JArrayList, List => JList}$/;"	i
java.util.{BitSet, Comparator, Timer, TimerTask, UUID}	broadcast/BitTorrentBroadcast.scala	/^import java.util.{BitSet, Comparator, Timer, TimerTask, UUID}$/;"	i
java.util.{Date, Properties}	scheduler/JobLogger.scala	/^import java.util.{Date, Properties}$/;"	i
java.util.{Date, Random}	storage/DiskBlockManager.scala	/^import java.util.{Date, Random}$/;"	i
java.util.{HashMap => JHashMap}	partial/GroupedCountEvaluator.scala	/^import java.util.{HashMap => JHashMap}$/;"	i
java.util.{HashMap => JHashMap}	partial/GroupedMeanEvaluator.scala	/^import java.util.{HashMap => JHashMap}$/;"	i
java.util.{HashMap => JHashMap}	partial/GroupedSumEvaluator.scala	/^import java.util.{HashMap => JHashMap}$/;"	i
java.util.{HashMap => JHashMap}	rdd/PairRDDFunctions.scala	/^import java.util.{HashMap => JHashMap}$/;"	i
java.util.{HashMap => JHashMap}	rdd/SubtractedRDD.scala	/^import java.util.{HashMap => JHashMap}$/;"	i
java.util.{HashMap => JHashMap}	storage/BlockManagerMasterActor.scala	/^import java.util.{HashMap => JHashMap}$/;"	i
java.util.{List => JList, ArrayList => JArrayList, Map => JMap, Collections}	api/python/PythonRDD.scala	/^import java.util.{List => JList, ArrayList => JArrayList, Map => JMap, Collections}$/;"	i
java.util.{List => JList, Comparator}	api/java/JavaRDDLike.scala	/^import java.util.{List => JList, Comparator}$/;"	i
java.util.{List => JList}	api/java/JavaPairRDD.scala	/^import java.util.{List => JList}$/;"	i
java.util.{Locale, Properties}	metrics/sink/CsvSink.scala	/^import java.util.{Locale, Properties}$/;"	i
java.util.{Locale, Random, UUID}	util/Utils.scala	/^import java.util.{Locale, Random, UUID}$/;"	i
java.util.{Map => JMap}	api/java/JavaSparkContext.scala	/^import java.util.{Map => JMap}$/;"	i
java.util.{Map => JMap}	partial/GroupedCountEvaluator.scala	/^import java.util.{Map => JMap}$/;"	i
java.util.{Map => JMap}	partial/GroupedMeanEvaluator.scala	/^import java.util.{Map => JMap}$/;"	i
java.util.{Map => JMap}	partial/GroupedSumEvaluator.scala	/^import java.util.{Map => JMap}$/;"	i
java.util.{NoSuchElementException, Properties}	scheduler/SchedulableBuilder.scala	/^import java.util.{NoSuchElementException, Properties}$/;"	i
java.util.{PriorityQueue => JPriorityQueue}	util/BoundedPriorityQueue.scala	/^import java.util.{PriorityQueue => JPriorityQueue}$/;"	i
java.util.{TimerTask, Timer}	scheduler/cluster/ClusterScheduler.scala	/^import java.util.{TimerTask, Timer}$/;"	i
java.util.{TimerTask, Timer}	util/MetadataCleaner.scala	/^import java.util.{TimerTask, Timer}$/;"	i
javax.management.MBeanServer	util/SizeEstimator.scala	/^import javax.management.MBeanServer$/;"	i
javax.servlet.http.HttpServletRequest	deploy/master/ui/ApplicationPage.scala	/^import javax.servlet.http.HttpServletRequest$/;"	i
javax.servlet.http.HttpServletRequest	deploy/master/ui/IndexPage.scala	/^import javax.servlet.http.HttpServletRequest$/;"	i
javax.servlet.http.HttpServletRequest	deploy/master/ui/MasterWebUI.scala	/^import javax.servlet.http.HttpServletRequest$/;"	i
javax.servlet.http.HttpServletRequest	deploy/worker/ui/IndexPage.scala	/^import javax.servlet.http.HttpServletRequest$/;"	i
javax.servlet.http.HttpServletRequest	deploy/worker/ui/WorkerWebUI.scala	/^import javax.servlet.http.HttpServletRequest$/;"	i
javax.servlet.http.HttpServletRequest	metrics/sink/MetricsServlet.scala	/^import javax.servlet.http.HttpServletRequest$/;"	i
javax.servlet.http.HttpServletRequest	ui/SparkUI.scala	/^import javax.servlet.http.HttpServletRequest$/;"	i
javax.servlet.http.HttpServletRequest	ui/env/EnvironmentUI.scala	/^import javax.servlet.http.HttpServletRequest$/;"	i
javax.servlet.http.HttpServletRequest	ui/exec/ExecutorsUI.scala	/^import javax.servlet.http.HttpServletRequest$/;"	i
javax.servlet.http.HttpServletRequest	ui/jobs/IndexPage.scala	/^import javax.servlet.http.HttpServletRequest$/;"	i
javax.servlet.http.HttpServletRequest	ui/jobs/JobProgressUI.scala	/^import javax.servlet.http.HttpServletRequest$/;"	i
javax.servlet.http.HttpServletRequest	ui/jobs/PoolPage.scala	/^import javax.servlet.http.HttpServletRequest$/;"	i
javax.servlet.http.HttpServletRequest	ui/jobs/StagePage.scala	/^import javax.servlet.http.HttpServletRequest$/;"	i
javax.servlet.http.HttpServletRequest	ui/storage/BlockManagerUI.scala	/^import javax.servlet.http.HttpServletRequest$/;"	i
javax.servlet.http.HttpServletRequest	ui/storage/IndexPage.scala	/^import javax.servlet.http.HttpServletRequest$/;"	i
javax.servlet.http.HttpServletRequest	ui/storage/RDDPage.scala	/^import javax.servlet.http.HttpServletRequest$/;"	i
javax.servlet.http.{HttpServletResponse, HttpServletRequest}	ui/JettyUtils.scala	/^import javax.servlet.http.{HttpServletResponse, HttpServletRequest}$/;"	i
job	FutureAction.scala	/^    val job = this.synchronized {$/;"	V
job	SparkContext.scala	/^    val job = new NewHadoopJob(conf)$/;"	V
job	rdd/PairRDDFunctions.scala	/^    val job = new NewAPIHadoopJob(conf)$/;"	V
job	scheduler/DAGScheduler.scala	/^        val job = new ActiveJob(jobId, finalStage, func, partitions, callSite, listener, properties)$/;"	V
job	scheduler/DAGScheduler.scala	/^      val job = resultStageToJob(resultStage)$/;"	V
job	scheduler/DAGScheduler.scala	/^      val job = resultStageToJob(stage)$/;"	V
job	scheduler/InputFormatInfo.scala	/^    val job = new Job(conf)$/;"	V
job	scheduler/JobLogger.scala	/^    val job = jobEnd.job$/;"	V
job	scheduler/JobLogger.scala	/^    val job = jobStart.job$/;"	V
jobAttemptId	rdd/PairRDDFunctions.scala	/^    val jobAttemptId = newTaskAttemptID(jobtrackerID, stageId, true, 0, 0)$/;"	V
jobCommitter	rdd/PairRDDFunctions.scala	/^    val jobCommitter = jobFormat.getOutputCommitter(jobTaskContext)$/;"	V
jobConf	rdd/HadoopRDD.scala	/^      val jobConf = getJobConf()$/;"	V
jobConf	rdd/HadoopRDD.scala	/^    val jobConf = getJobConf()$/;"	V
jobConf	rdd/SequenceFileRDDFunctions.scala	/^    val jobConf = new JobConf(self.context.hadoopConfiguration)$/;"	V
jobConf	scheduler/InputFormatInfo.scala	/^    val jobConf = new JobConf(configuration)$/;"	V
jobConfCacheKey	rdd/HadoopRDD.scala	/^  protected val jobConfCacheKey = "rdd_%d_job_conf".format(id)$/;"	V
jobContext	SparkHadoopWriter.scala	/^  @transient private var jobContext: JobContext = null$/;"	v
jobContext	rdd/NewHadoopRDD.scala	/^    val jobContext = newJobContext(conf, jobId)$/;"	V
jobFailed	scheduler/JobListener.scala	/^  def jobFailed(exception: Exception)$/;"	m
jobFinished	scheduler/JobWaiter.scala	/^  def jobFinished = _jobFinished$/;"	m
jobFormat	rdd/PairRDDFunctions.scala	/^    val jobFormat = outputFormatClass.newInstance$/;"	V
jobID	SparkHadoopWriter.scala	/^  private var jobID = 0$/;"	v
jobIDToPrintWriter	scheduler/JobLogger.scala	/^  private val jobIDToPrintWriter = new HashMap[Int, PrintWriter]$/;"	V
jobIDToStages	scheduler/JobLogger.scala	/^  private val jobIDToStages = new HashMap[Int, ListBuffer[Stage]]$/;"	V
jobId	rdd/NewHadoopRDD.scala	/^  @transient private val jobId = new JobID(jobtrackerId, id)$/;"	V
jobId	scheduler/ActiveJob.scala	/^    val jobId: Int,$/;"	V
jobId	scheduler/DAGScheduler.scala	/^    val jobId = nextJobId.getAndIncrement()$/;"	V
jobId	scheduler/Stage.scala	/^    val jobId: Int,$/;"	V
jobIds	scheduler/DAGScheduler.scala	/^        val jobIds = activeJobs.filter(groupId == _.properties.get(SparkContext.SPARK_JOB_GROUP_ID))$/;"	V
jobResult	rdd/RDD.scala	/^    var jobResult = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance())$/;"	v
jobResult	rdd/RDD.scala	/^    var jobResult: Option[T] = None$/;"	v
jobResult	scheduler/JobWaiter.scala	/^  private var jobResult: JobResult = if (jobFinished) JobSucceeded else null$/;"	v
jobTaskContext	rdd/PairRDDFunctions.scala	/^    val jobTaskContext = newTaskAttemptContext(wrappedConf.value, jobAttemptId)$/;"	V
jobs	ui/SparkUI.scala	/^  val jobs = new JobProgressUI(sc)$/;"	V
jobs	ui/UIUtils.scala	/^    val jobs = page match {$/;"	V
jobs	ui/UIWorkloadGenerator.scala	/^    val jobs = Seq[(String, () => Long)]($/;"	V
jobtrackerID	SparkHadoopWriter.scala	/^    val jobtrackerID = formatter.format(new Date())$/;"	V
jobtrackerID	rdd/PairRDDFunctions.scala	/^    val jobtrackerID = formatter.format(new Date())$/;"	V
jobtrackerId	rdd/NewHadoopRDD.scala	/^  private val jobtrackerId: String = {$/;"	V
join	api/java/JavaPairRDD.scala	/^  def join[W](other: JavaPairRDD[K, W]): JavaPairRDD[K, (V, W)] =$/;"	m
join	api/java/JavaPairRDD.scala	/^  def join[W](other: JavaPairRDD[K, W], numPartitions: Int): JavaPairRDD[K, (V, W)] =$/;"	m
join	api/java/JavaPairRDD.scala	/^  def join[W](other: JavaPairRDD[K, W], partitioner: Partitioner): JavaPairRDD[K, (V, W)] =$/;"	m
join	rdd/PairRDDFunctions.scala	/^  def join[W](other: RDD[(K, W)]): RDD[(K, (V, W))] = {$/;"	m
join	rdd/PairRDDFunctions.scala	/^  def join[W](other: RDD[(K, W)], numPartitions: Int): RDD[(K, (V, W))] = {$/;"	m
join	rdd/PairRDDFunctions.scala	/^  def join[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (V, W))] = {$/;"	m
joinResult	api/java/JavaPairRDD.scala	/^    val joinResult = rdd.leftOuterJoin(other)$/;"	V
joinResult	api/java/JavaPairRDD.scala	/^    val joinResult = rdd.leftOuterJoin(other, numPartitions)$/;"	V
joinResult	api/java/JavaPairRDD.scala	/^    val joinResult = rdd.leftOuterJoin(other, partitioner)$/;"	V
joinResult	api/java/JavaPairRDD.scala	/^    val joinResult = rdd.rightOuterJoin(other)$/;"	V
joinResult	api/java/JavaPairRDD.scala	/^    val joinResult = rdd.rightOuterJoin(other, numPartitions)$/;"	V
joinResult	api/java/JavaPairRDD.scala	/^    val joinResult = rdd.rightOuterJoin(other, partitioner)$/;"	V
json	deploy/FaultToleranceTest.scala	/^      val json = JsonParser.parse(masterStream, closeAutomatically = true)$/;"	V
jvmGCTime	executor/TaskMetrics.scala	/^  var jvmGCTime: Long = _$/;"	v
jvmInformation	ui/env/EnvironmentUI.scala	/^    val jvmInformation = Seq($/;"	V
jvmRow	ui/env/EnvironmentUI.scala	/^    def jvmRow(kv: (String, String)) = <tr><td>{kv._1}<\/td><td>{kv._2}<\/td><\/tr>$/;"	m
jvmTable	ui/env/EnvironmentUI.scala	/^    def jvmTable =$/;"	m
k	Partitioner.scala	/^    val k = key.asInstanceOf[K]$/;"	V
k	network/Connection.scala	/^    val k = key()$/;"	V
k	util/AppendOnlyMap.scala	/^    val k = key.asInstanceOf[AnyRef]$/;"	V
kc	Aggregator.scala	/^    var kc: (K, C) = null$/;"	v
kc	SparkContext.scala	/^    val kc = kcf()$/;"	V
kcm	api/java/JavaRDDLike.scala	/^    implicit val kcm: ClassManifest[K] = implicitly[ClassManifest[AnyRef]].asInstanceOf[ClassManifest[K]]$/;"	V
kcm	api/java/JavaRDDLike.scala	/^    implicit val kcm: ClassManifest[K] =$/;"	V
kcm	api/java/JavaSparkContext.scala	/^    implicit val kcm = ClassManifest.fromClass(kClass)$/;"	V
kcm	api/java/JavaSparkContext.scala	/^    implicit val kcm = ClassManifest.fromClass(keyClass)$/;"	V
kcm	api/java/JavaSparkContext.scala	/^    implicit val kcm: ClassManifest[K] = first.kManifest$/;"	V
kcm	api/java/JavaSparkContext.scala	/^    implicit val kcm: ClassManifest[K] =$/;"	V
keepGoing	util/AppendOnlyMap.scala	/^        var keepGoing = true$/;"	v
keepReceiving	broadcast/BitTorrentBroadcast.scala	/^          var keepReceiving = true$/;"	v
keepSending	broadcast/BitTorrentBroadcast.scala	/^          var keepSending = true$/;"	v
key	CacheManager.scala	/^    val key = RDDBlockId(rdd.id, split.index)$/;"	V
key	SparkContext.scala	/^      var key = ""$/;"	v
key	SparkContext.scala	/^    val key = uri.getScheme match {$/;"	V
key	SparkEnv.scala	/^      val key = (pythonExec, envVars)$/;"	V
key	network/Connection.scala	/^  def key() = channel.keyFor(selector)$/;"	m
key	network/ConnectionManager.scala	/^                val key = allKeys.next()$/;"	V
key	network/ConnectionManager.scala	/^            val key = selectedKeys.next$/;"	V
key	rdd/HadoopRDD.scala	/^      val key: K = reader.createKey()$/;"	V
key	scheduler/cluster/ClusterTaskSetManager.scala	/^          val key = ef.description$/;"	V
key	util/AppendOnlyMap.scala	/^        val key = data(2 * oldPos)$/;"	V
keyBy	api/java/JavaRDDLike.scala	/^  def keyBy[K](f: JFunction[T, K]): JavaPairRDD[K, T] = {$/;"	m
keyBy	rdd/RDD.scala	/^  def keyBy[K](f: T => K): RDD[(K, T)] = {$/;"	m
keyClass	rdd/PairRDDFunctions.scala	/^    val keyClass = conf.getOutputKeyClass$/;"	V
keyClass	rdd/SequenceFileRDDFunctions.scala	/^    val keyClass = getWritableClass[K]$/;"	V
keyInterestChangeRequests	network/ConnectionManager.scala	/^  private val keyInterestChangeRequests = new SynchronizedQueue[(SelectionKey, Int)]$/;"	V
keyList	scheduler/cluster/ClusterScheduler.scala	/^    val keyList = _keyList.sortWith($/;"	V
keyType	api/java/function/PairFlatMapFunction.java	/^  public ClassManifest<K> keyType() {$/;"	m	class:PairFlatMapFunction
keyType	api/java/function/PairFunction.java	/^  public ClassManifest<K> keyType() {$/;"	m	class:PairFunction
keys	api/java/JavaPairRDD.scala	/^  def keys(): JavaRDD[K] = JavaRDD.fromRDD[K](rdd.map(_._1))$/;"	m
keys	rdd/PairRDDFunctions.scala	/^  def keys: RDD[K] = self.map(_._1)$/;"	m
kill	deploy/FaultToleranceTest.scala	/^  def kill(dockerId: DockerId) : Unit = {$/;"	m
killLeader	deploy/FaultToleranceTest.scala	/^  def killLeader(): Unit = {$/;"	m
killTask	executor/Executor.scala	/^  def killTask(taskId: Long) {$/;"	m
killTask	scheduler/cluster/SchedulerBackend.scala	/^  def killTask(taskId: Long, executorId: String): Unit = throw new UnsupportedOperationException$/;"	m
killed	executor/Executor.scala	/^    @volatile private var killed = false$/;"	v
killed	scheduler/Task.scala	/^  def killed: Boolean = _killed$/;"	m
klass	executor/Executor.scala	/^        val klass = Class.forName("org.apache.spark.repl.ExecutorClassLoader")$/;"	V
kryo	serializer/KryoSerializer.scala	/^    val kryo = instantiator.newKryo()$/;"	V
kryo	serializer/KryoSerializer.scala	/^  val kryo = ks.newKryo()$/;"	V
kv	Aggregator.scala	/^    var kv: Product2[K, V] = null$/;"	v
kv	util/TimeStampedHashMap.scala	/^      val kv = (entry.getKey, entry.getValue._1)$/;"	V
lastException	storage/BlockManagerMaster.scala	/^    var lastException: Exception = null$/;"	v
lastFetchFailureTime	scheduler/DAGScheduler.scala	/^  var lastFetchFailureTime: Long = 0  \/\/ Used to wait a bit to avoid repeated resubmits$/;"	v
lastHeartbeat	deploy/master/WorkerInfo.scala	/^  @transient var lastHeartbeat: Long = _$/;"	v
lastId	Accumulators.scala	/^  var lastId: Long = 0$/;"	v
lastId	network/Message.scala	/^  var lastId = 1$/;"	v
lastLaunchTime	scheduler/cluster/ClusterTaskSetManager.scala	/^  var lastLaunchTime = clock.getTime()  \/\/ Time we last launched a task at this level$/;"	v
lastOps	network/ConnectionManager.scala	/^                val lastOps = key.interestOps()$/;"	V
lastSeenMs	storage/BlockManagerMasterActor.scala	/^    def lastSeenMs: Long = _lastSeenMs$/;"	m
lastSetSparkEnv	SparkEnv.scala	/^  @volatile private var lastSetSparkEnv : SparkEnv = _$/;"	v
lastSparkMethod	util/Utils.scala	/^    var lastSparkMethod = "<unknown>"$/;"	v
lastSparkMethod	util/Utils.scala	/^  private[spark] class CallSiteInfo(val lastSparkMethod: String, val firstUserFile: String,$/;"	V
lastSyncTime	util/RateLimitedOutputStream.scala	/^  var lastSyncTime = System.nanoTime$/;"	v
lastValidPosition	storage/BlockObjectWriter.scala	/^  private var lastValidPosition = initialPosition$/;"	v
latch	storage/StoragePerfTester.scala	/^    val latch = new CountDownLatch(numMaps)$/;"	V
launchExecutor	deploy/master/Master.scala	/^  def launchExecutor(worker: WorkerInfo, exec: ExecutorInfo, sparkHome: String) {$/;"	m
launchTask	executor/Executor.scala	/^  def launchTask(context: ExecutorBackend, taskId: Long, serializedTask: ByteBuffer) {$/;"	m
launchTask	scheduler/local/LocalScheduler.scala	/^      var launchTask = false$/;"	v
launchTasks	scheduler/cluster/CoarseGrainedSchedulerBackend.scala	/^    def launchTasks(tasks: Seq[Seq[TaskDescription]]) {$/;"	m
launchTime	scheduler/TaskInfo.scala	/^    val launchTime: Long,$/;"	V
launchedTask	scheduler/cluster/ClusterScheduler.scala	/^    var launchedTask = false$/;"	v
leader	deploy/FaultToleranceTest.scala	/^    val leader = getLeader$/;"	V
leader	deploy/master/ZooKeeperLeaderElectionAgent.scala	/^    val leader = masters.sorted.head$/;"	V
leaderElectionAgent	deploy/master/Master.scala	/^  var leaderElectionAgent: ActorRef = _$/;"	v
leaderFile	deploy/master/ZooKeeperLeaderElectionAgent.scala	/^    val leaderFile = WORKING_DIR + "\/" + leader$/;"	V
leaderUrl	deploy/master/ZooKeeperLeaderElectionAgent.scala	/^  private var leaderUrl: String = _$/;"	v
leaders	deploy/FaultToleranceTest.scala	/^    val leaders = masters.filter(_.state == RecoveryState.ALIVE)$/;"	V
left	rdd/AsyncRDDActions.scala	/^        val left = num - results.size$/;"	V
left	rdd/RDD.scala	/^      val left = num - buf.size$/;"	V
leftOuterJoin	api/java/JavaPairRDD.scala	/^  def leftOuterJoin[W](other: JavaPairRDD[K, W]): JavaPairRDD[K, (V, Optional[W])] = {$/;"	m
leftOuterJoin	api/java/JavaPairRDD.scala	/^  def leftOuterJoin[W](other: JavaPairRDD[K, W], numPartitions: Int): JavaPairRDD[K, (V, Optional[W])] = {$/;"	m
leftOuterJoin	api/java/JavaPairRDD.scala	/^  def leftOuterJoin[W](other: JavaPairRDD[K, W], partitioner: Partitioner)$/;"	m
leftOuterJoin	rdd/PairRDDFunctions.scala	/^  def leftOuterJoin[W](other: RDD[(K, W)]): RDD[(K, (V, Option[W]))] = {$/;"	m
leftOuterJoin	rdd/PairRDDFunctions.scala	/^  def leftOuterJoin[W](other: RDD[(K, W)], numPartitions: Int): RDD[(K, (V, Option[W]))] = {$/;"	m
leftOuterJoin	rdd/PairRDDFunctions.scala	/^  def leftOuterJoin[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (V, Option[W]))] = {$/;"	m
len	api/python/PythonWorkerFactory.scala	/^              var len = in.read(buf)$/;"	v
len	api/python/PythonWorkerFactory.scala	/^            var len = in.read(buf)$/;"	v
len2	api/python/PythonRDD.scala	/^              var len2 = stream.readInt()$/;"	v
length	api/python/PythonRDD.scala	/^        val length = file.readInt()$/;"	V
length	api/python/PythonRDD.scala	/^      val length = 2 + 1 + 4 + s.length + 1$/;"	V
length	api/python/PythonRDD.scala	/^      val length = t._1.length + t._2.length - 3 - 3 + 4  \/\/ stripPickle() removes 3 bytes$/;"	V
length	network/Connection.scala	/^      val length = channel.read(ByteBuffer.allocate(1))$/;"	V
length	network/netty/FileHeader.scala	/^    val length = buf.readInt$/;"	V
length	rdd/JdbcRDD.scala	/^    val length = 1 + upperBound - lowerBound$/;"	V
length	scheduler/SplitInfo.scala	/^                val length: Long, val underlyingSplit: Any) {$/;"	V
length	scheduler/SplitInfo.scala	/^    val length = mapredSplit.getLength$/;"	V
length	scheduler/SplitInfo.scala	/^    val length = mapreduceSplit.getLength$/;"	V
length	storage/BlockMessageArray.scala	/^  def length = blockMessages.length $/;"	m
length	storage/DiskStore.scala	/^    val length = file.length$/;"	V
length	storage/ShuffleBlockManager.scala	/^        val length =$/;"	V
length	util/Distribution.scala	/^  val length = endIdx - startIdx$/;"	V
length	util/SerializableBuffer.scala	/^    val length = in.readInt()$/;"	V
length	util/SizeEstimator.scala	/^    val length = JArray.getLength(array)$/;"	V
length	util/Utils.scala	/^    val length = file.length()$/;"	V
length	util/Vector.scala	/^  def length = elements.length$/;"	m
length	util/collection/PrimitiveVector.scala	/^  def length: Int = _numElements$/;"	m
level	storage/BlockInfo.scala	/^private[storage] class BlockInfo(val level: StorageLevel, val tellMaster: Boolean) {$/;"	V
level	storage/BlockManager.scala	/^          val level = info.level$/;"	V
level	storage/BlockManager.scala	/^        val level = info.level$/;"	V
level	storage/BlockMessage.scala	/^  private var level: StorageLevel = null$/;"	v
level	storage/ThreadingTest.scala	/^        val level = randomLevel()$/;"	V
levels	scheduler/cluster/ClusterTaskSetManager.scala	/^    val levels = new ArrayBuffer[TaskLocality.TaskLocality]$/;"	V
libraryOpts	deploy/worker/ExecutorRunner.scala	/^    val libraryOpts = getAppEnv("SPARK_LIBRARY_PATH")$/;"	V
lifecycleEvents	util/AkkaUtils.scala	/^    val lifecycleEvents = if (System.getProperty("spark.akka.logLifecycleEvents", "false").toBoolean) "on" else "off"$/;"	V
lines	rdd/PipedRDD.scala	/^    val lines = Source.fromInputStream(proc.getInputStream).getLines$/;"	V
linkToMaster	deploy/worker/ui/WorkerWebUI.scala	/^    val linkToMaster = <p><a href={worker.activeMasterWebUiUrl}>Back to Master<\/a><\/p>$/;"	V
list	scheduler/InputFormatInfo.scala	/^    val list = instance.getSplits(job)$/;"	V
listIter	broadcast/TreeBroadcast.scala	/^        var listIter = listOfSources.iterator$/;"	v
listOfSources	broadcast/BitTorrentBroadcast.scala	/^  @transient var listOfSources = ListBuffer[SourceInfo]()$/;"	v
listOfSources	broadcast/TreeBroadcast.scala	/^  @transient var listOfSources = ListBuffer[SourceInfo]()$/;"	v
listenPort	broadcast/BitTorrentBroadcast.scala	/^  @transient var listenPort = -1$/;"	v
listenPort	broadcast/TreeBroadcast.scala	/^  @transient var listenPort = -1$/;"	v
listenPortLock	broadcast/BitTorrentBroadcast.scala	/^  @transient var listenPortLock = new Object$/;"	v
listenPortLock	broadcast/TreeBroadcast.scala	/^  @transient var listenPortLock = new Object$/;"	v
listener	deploy/client/TestClient.scala	/^    val listener = new TestListener$/;"	V
listener	scheduler/ActiveJob.scala	/^    val listener: JobListener,$/;"	V
listener	scheduler/DAGScheduler.scala	/^    val listener = new ApproximateActionListener(rdd, func, evaluator, timeout)$/;"	V
listener	ui/exec/ExecutorsUI.scala	/^  def listener = _listener.get$/;"	m
listener	ui/jobs/IndexPage.scala	/^  def listener = parent.listener$/;"	m
listener	ui/jobs/JobProgressUI.scala	/^  def listener = _listener.get$/;"	m
listener	ui/jobs/PoolPage.scala	/^  def listener = parent.listener$/;"	m
listener	ui/jobs/StagePage.scala	/^  def listener = parent.listener$/;"	m
listener	ui/jobs/StageTable.scala	/^  val listener = parent.listener$/;"	V
listenerBus	scheduler/DAGScheduler.scala	/^  private[spark] val listenerBus = new SparkListenerBus()$/;"	V
listings	ui/jobs/StagePage.scala	/^          val listings: Seq[Seq[String]] = Seq(serviceQuantiles,$/;"	V
liveWorkerIPs	deploy/FaultToleranceTest.scala	/^    var liveWorkerIPs: Seq[String] = List()$/;"	v
liveWorkerIPs	deploy/FaultToleranceTest.scala	/^  var liveWorkerIPs: List[String] = _$/;"	v
liveWorkers	deploy/FaultToleranceTest.scala	/^      val liveWorkers = workers.children.filter(w => (w \\ "state").extract[String] == "ALIVE")$/;"	V
loader	executor/Executor.scala	/^    val loader = this.getClass.getClassLoader$/;"	V
loader	scheduler/ResultTask.scala	/^    val loader = Thread.currentThread.getContextClassLoader$/;"	V
loader	scheduler/ShuffleMapTask.scala	/^      val loader = Thread.currentThread.getContextClassLoader$/;"	V
loader	scheduler/cluster/TaskResultGetter.scala	/^            val loader = Thread.currentThread.getContextClassLoader$/;"	V
loading	CacheManager.scala	/^  private val loading = new HashSet[RDDBlockId]()$/;"	V
loc	rdd/CoalescedRDD.scala	/^    val loc = parents.count(p =>$/;"	V
local	storage/BlockManager.scala	/^    val local = getLocal(blockId)$/;"	V
localAccums	Accumulators.scala	/^  val localAccums = Map[Thread, Map[Long, Accumulable[_, _]]]()$/;"	V
localActor	scheduler/local/LocalScheduler.scala	/^  var localActor: ActorRef = null$/;"	v
localBlocksFetched	executor/TaskMetrics.scala	/^  var localBlocksFetched: Int = _$/;"	v
localBlocksToFetch	storage/BlockFetcherIterator.scala	/^    protected val localBlocksToFetch = new ArrayBuffer[BlockId]()$/;"	V
localCluster	SparkContext.scala	/^        val localCluster = new LocalSparkCluster($/;"	V
localDir	storage/DiskBlockManager.scala	/^      var localDir: File = null$/;"	v
localDirId	storage/DiskBlockManager.scala	/^      var localDirId: String = null$/;"	v
localDirs	executor/Executor.scala	/^    val localDirs = Option(System.getenv("YARN_LOCAL_DIRS"))$/;"	V
localDirs	network/netty/ShuffleSender.scala	/^    val localDirs = args.drop(2).map(new File(_))$/;"	V
localDirs	storage/DiskBlockManager.scala	/^  private val localDirs: Array[File] = createLocalDirs()$/;"	V
localFraction	rdd/CoalescedRDD.scala	/^  def localFraction: Double = {$/;"	m
localHostName	util/Utils.scala	/^  def localHostName(): String = {$/;"	m
localHostPort	util/Utils.scala	/^  def localHostPort(): String = {$/;"	m
localHostname	deploy/LocalSparkCluster.scala	/^  private val localHostname = Utils.localHostName()$/;"	V
localIpAddress	util/Utils.scala	/^  lazy val localIpAddress: String = findLocalIpAddress()$/;"	V
localIpAddressHostname	util/Utils.scala	/^  lazy val localIpAddressHostname: String = getAddressHostName(localIpAddress)$/;"	V
localName	executor/Executor.scala	/^        val localName = name.split("\/").last$/;"	V
localProperties	SparkContext.scala	/^  private val localProperties = new InheritableThreadLocal[Properties] {$/;"	V
localSourceInfo	broadcast/BitTorrentBroadcast.scala	/^    var localSourceInfo = SourceInfo($/;"	v
localValue	Accumulators.scala	/^  def localValue = value_$/;"	m
localityWaits	scheduler/cluster/ClusterTaskSetManager.scala	/^  val localityWaits = myLocalityLevels.map(getLocalityWait) \/\/ Time to wait at each level$/;"	V
location	scheduler/InputFormatInfo.scala	/^        val location = split.hostLocation$/;"	V
location	scheduler/MapStatus.scala	/^private[spark] class MapStatus(var location: BlockManagerId, var compressedSizes: Array[Byte])$/;"	v
locations	rdd/CheckpointRDD.scala	/^    val locations = fs.getFileBlockLocations(status, 0, status.getLen)$/;"	V
locations	scheduler/cluster/ClusterTaskSetManager.scala	/^          val locations = tasks(index).preferredLocations.map(_.host)$/;"	V
locations	storage/BlockManager.scala	/^    val locations = Random.shuffle(master.getLocations(blockId))$/;"	V
locations	storage/BlockManager.scala	/^    val locations = master.getLocations(blockIds).toArray$/;"	V
locations	storage/BlockManagerMasterActor.scala	/^      val locations = blockLocations.get(blockId)$/;"	V
locations	storage/BlockManagerMasterActor.scala	/^    val locations = blockLocations.get(blockId)$/;"	V
locations	storage/BlockManagerMasterActor.scala	/^    var locations: mutable.HashSet[BlockManagerId] = null$/;"	v
locations_	rdd/BlockRDD.scala	/^  @transient lazy val locations_ = BlockManager.blockIdsToHosts(blockIds, SparkEnv.get)$/;"	V
locs	scheduler/DAGScheduler.scala	/^          val locs = getPreferredLocs(n.rdd, inPart)$/;"	V
locs	scheduler/DAGScheduler.scala	/^        val locs = getPreferredLocs(stage.rdd, p)$/;"	V
locs	scheduler/DAGScheduler.scala	/^        val locs = getPreferredLocs(stage.rdd, partition)$/;"	V
locs	scheduler/DAGScheduler.scala	/^        val locs = stage.outputLocs.map(list => if (list.isEmpty) null else list.head).toArray$/;"	V
locs	scheduler/DAGScheduler.scala	/^      val locs = BlockManager.blockIdsToBlockManagers(blockIds, env, blockManagerMaster)$/;"	V
locs	scheduler/ShuffleMapTask.scala	/^    @transient private var locs: Seq[TaskLocation])$/;"	v
locs	scheduler/cluster/ClusterTaskSetManager.scala	/^            val locs = ef.stackTrace.map(loc => "\\tat %s".format(loc.toString))$/;"	V
locs	scheduler/local/LocalTaskSetManager.scala	/^      val locs = reason.stackTrace.map(loc => "\\tat %s".format(loc.toString))$/;"	V
log	deploy/worker/ui/WorkerWebUI.scala	/^  def log(request: HttpServletRequest): String = {$/;"	m
logDir	scheduler/JobLogger.scala	/^  private val logDir =$/;"	V
logErrorWithStack	util/Utils.scala	/^  def logErrorWithStack(msg: String) {$/;"	m
logLength	deploy/worker/ui/WorkerWebUI.scala	/^    val logLength = file.length$/;"	V
logLength	deploy/worker/ui/WorkerWebUI.scala	/^    val logLength = file.length()$/;"	V
logPage	deploy/worker/ui/WorkerWebUI.scala	/^  def logPage(request: HttpServletRequest): Seq[scala.xml.Node] = {$/;"	m
logPageLength	deploy/worker/ui/WorkerWebUI.scala	/^    val logPageLength = math.min(byteLength, maxBytes)$/;"	V
logText	deploy/worker/ui/WorkerWebUI.scala	/^    val logText = <node>{Utils.offsetBytes(path, startByte, endByte)}<\/node>$/;"	V
logType	deploy/worker/ui/WorkerWebUI.scala	/^    val logType = request.getParameter("logType")$/;"	V
log_	Logging.scala	/^  @transient private var log_ : Logger = null$/;"	v
lookup	api/java/JavaPairRDD.scala	/^  def lookup(key: K): JList[V] = seqAsJavaList(rdd.lookup(key))$/;"	m
lookup	rdd/PairRDDFunctions.scala	/^  def lookup(key: K): Seq[V] = {$/;"	m
low	partial/CountEvaluator.scala	/^      val low = mean - confFactor * stdev$/;"	V
low	partial/GroupedCountEvaluator.scala	/^        val low = mean - confFactor * stdev$/;"	V
low	partial/GroupedMeanEvaluator.scala	/^        val low = mean - confFactor * stdev$/;"	V
low	partial/GroupedSumEvaluator.scala	/^        val low = sumEstimate - confFactor * sumStdev$/;"	V
low	partial/MeanEvaluator.scala	/^      val low = mean - confFactor * stdev$/;"	V
low	partial/SumEvaluator.scala	/^      val low = sumEstimate - confFactor * sumStdev$/;"	V
lower	rdd/JdbcRDD.scala	/^private[spark] class JdbcPartition(idx: Int, val lower: Long, val upper: Long) extends Partition {$/;"	V
lower	util/Utils.scala	/^    val lower = str.toLowerCase$/;"	V
m2	util/StatCounter.scala	/^  private var m2: Double = 0  \/\/ Running variance numerator (sum of (x - mean)^2)$/;"	v
main	deploy/client/TestClient.scala	/^  def main(args: Array[String]) {$/;"	m
main	deploy/client/TestExecutor.scala	/^  def main(args: Array[String]) {$/;"	m
main	deploy/master/Master.scala	/^  def main(argStrings: Array[String]) {$/;"	m
main	deploy/worker/Worker.scala	/^  def main(argStrings: Array[String]) {$/;"	m
main	executor/CoarseGrainedExecutorBackend.scala	/^  def main(args: Array[String]) {$/;"	m
main	executor/MesosExecutorBackend.scala	/^  def main(args: Array[String]) {$/;"	m
main	network/ConnectionManager.scala	/^  def main(args: Array[String]) {$/;"	m
main	network/ConnectionManagerTest.scala	/^  def main(args: Array[String]) {$/;"	m
main	network/ReceiverTest.scala	/^  def main(args: Array[String]) {$/;"	m
main	network/SenderTest.scala	/^  def main(args: Array[String]) {$/;"	m
main	network/netty/FileHeader.scala	/^  def main (args:Array[String]) {$/;"	m
main	network/netty/ShuffleCopier.scala	/^  def main(args: Array[String]) {$/;"	m
main	network/netty/ShuffleSender.scala	/^  def main(args: Array[String]) {$/;"	m
main	rdd/CheckpointRDD.scala	/^  def main(args: Array[String]) {$/;"	m
main	storage/BlockMessage.scala	/^  def main(args: Array[String]) {$/;"	m
main	storage/BlockMessageArray.scala	/^  def main(args: Array[String]) {$/;"	m
main	storage/StoragePerfTester.scala	/^  def main(args: Array[String]) = {$/;"	m
main	storage/ThreadingTest.scala	/^  def main(args: Array[String]) {$/;"	m
main	ui/UIWorkloadGenerator.scala	/^  def main(args: Array[String]) {$/;"	m
makeOffers	scheduler/cluster/CoarseGrainedSchedulerBackend.scala	/^    def makeOffers(executorId: String) {$/;"	m
makeRDD	SparkContext.scala	/^   def makeRDD[T: ClassManifest](seq: Seq[(T, Seq[String])]): RDD[T] = {$/;"	m
makeRDD	SparkContext.scala	/^  def makeRDD[T: ClassManifest](seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] = {$/;"	m
makeRunCmd	deploy/FaultToleranceTest.scala	/^  def makeRunCmd(imageTag: String, args: String = "", mountDir: String = ""): ProcessBuilder = {$/;"	m
manager	deploy/worker/Worker.scala	/^        val manager = new ExecutorRunner(appId, execId, appDesc, cores_, memory_,$/;"	V
manager	network/ConnectionManager.scala	/^    val manager = new ConnectionManager(9999)$/;"	V
manager	network/ReceiverTest.scala	/^    val manager = new ConnectionManager(9999)$/;"	V
manager	network/SenderTest.scala	/^    val manager = new ConnectionManager(0)$/;"	V
manager	scheduler/cluster/ClusterScheduler.scala	/^      val manager = new ClusterTaskSetManager(this, taskSet)$/;"	V
manager	scheduler/local/LocalScheduler.scala	/^      val manager = new LocalTaskSetManager(this, taskSet)$/;"	V
managerId	network/Connection.scala	/^    val managerId = ConnectionManagerId.fromSocketAddress(header.address)$/;"	V
map	api/java/JavaRDDLike.scala	/^  def map[K2, V2](f: PairFunction[T, K2, V2]): JavaPairRDD[K2, V2] = {$/;"	m
map	api/java/JavaRDDLike.scala	/^  def map[R](f: DoubleFunction[T]): JavaDoubleRDD =$/;"	m
map	api/java/JavaRDDLike.scala	/^  def map[R](f: JFunction[T, R]): JavaRDD[R] =$/;"	m
map	partial/PartialResult.scala	/^  def map[T](f: R => T) : PartialResult[T] = {$/;"	m
map	rdd/CoGroupedRDD.scala	/^    val map = new AppendOnlyMap[K, Seq[ArrayBuffer[Any]]]$/;"	V
map	rdd/PairRDDFunctions.scala	/^      val map = new JHashMap[K, V]$/;"	V
map	rdd/PairRDDFunctions.scala	/^    val map = new mutable.HashMap[K, V]$/;"	V
map	rdd/RDD.scala	/^      val map = new OLMap[T]$/;"	V
map	rdd/RDD.scala	/^  def map[U: ClassManifest](f: T => U): RDD[U] = new MappedRDD(this, sc.clean(f))$/;"	m
map	rdd/SubtractedRDD.scala	/^    val map = new JHashMap[K, ArrayBuffer[V]]$/;"	V
mapIdToIndex	storage/ShuffleBlockManager.scala	/^    private val mapIdToIndex = new PrimitiveKeyOpenHashMap[Int, Int]()$/;"	V
mapOutputTracker	SparkEnv.scala	/^    val mapOutputTracker = new MapOutputTracker()$/;"	V
mapOutputTracker	SparkEnv.scala	/^    val mapOutputTracker: MapOutputTracker,$/;"	V
mapOutputTracker	scheduler/cluster/ClusterScheduler.scala	/^  val mapOutputTracker = SparkEnv.get.mapOutputTracker$/;"	V
mapPartitions	api/java/JavaRDDLike.scala	/^  def mapPartitions(f: DoubleFlatMapFunction[java.util.Iterator[T]]): JavaDoubleRDD = {$/;"	m
mapPartitions	api/java/JavaRDDLike.scala	/^  def mapPartitions[K2, V2](f: PairFlatMapFunction[java.util.Iterator[T], K2, V2]):$/;"	m
mapPartitions	api/java/JavaRDDLike.scala	/^  def mapPartitions[U](f: FlatMapFunction[java.util.Iterator[T], U]): JavaRDD[U] = {$/;"	m
mapPartitions	rdd/RDD.scala	/^  def mapPartitions[U: ClassManifest]($/;"	m
mapPartitionsWithContext	rdd/RDD.scala	/^  def mapPartitionsWithContext[U: ClassManifest]($/;"	m
mapPartitionsWithIndex	api/java/JavaRDDLike.scala	/^  def mapPartitionsWithIndex[R: ClassManifest]($/;"	m
mapPartitionsWithIndex	rdd/RDD.scala	/^  def mapPartitionsWithIndex[U: ClassManifest]($/;"	m
mapPartitionsWithSplit	rdd/RDD.scala	/^  def mapPartitionsWithSplit[U: ClassManifest]($/;"	m
mapStage	scheduler/DAGScheduler.scala	/^                val mapStage = getShuffleMapStage(shufDep, stage.jobId)$/;"	V
mapStage	scheduler/DAGScheduler.scala	/^              val mapStage = getShuffleMapStage(shufDep, stage.jobId)$/;"	V
mapStage	scheduler/DAGScheduler.scala	/^        val mapStage = shuffleToMapStage(shuffleId)$/;"	V
mapStatuses	MapOutputTracker.scala	/^  private var mapStatuses = new TimeStampedHashMap[Int, Array[MapStatus]]$/;"	v
mapValues	api/java/JavaPairRDD.scala	/^  def mapValues[U](f: JFunction[V, U]): JavaPairRDD[K, U] = {$/;"	m
mapValues	rdd/PairRDDFunctions.scala	/^  def mapValues[U](f: V => U): RDD[(K, U)] = {$/;"	m
mapWith	rdd/RDD.scala	/^  def mapWith[A: ClassManifest, U: ClassManifest]$/;"	m
mapper	metrics/sink/MetricsServlet.scala	/^  val mapper = new ObjectMapper().registerModule($/;"	V
mapredInputFormat	scheduler/InputFormatInfo.scala	/^  var mapredInputFormat: Boolean = false$/;"	v
mapreduceInputFormat	scheduler/InputFormatInfo.scala	/^  var mapreduceInputFormat: Boolean = false$/;"	v
markFailed	scheduler/TaskInfo.scala	/^  def markFailed(time: Long = System.currentTimeMillis) {$/;"	m
markFinished	deploy/master/ApplicationInfo.scala	/^  def markFinished(endState: ApplicationState.Value) {$/;"	m
markGettingResult	scheduler/TaskInfo.scala	/^  def markGettingResult(time: Long = System.currentTimeMillis) {$/;"	m
markReady	storage/BlockInfo.scala	/^  def markReady(sizeInBytes: Long) {$/;"	m
markStageAsFinished	scheduler/DAGScheduler.scala	/^    def markStageAsFinished(stage: Stage) = {$/;"	m
markSuccessful	scheduler/TaskInfo.scala	/^  def markSuccessful(time: Long = System.currentTimeMillis) {$/;"	m
marked	storage/BlockManager.scala	/^      var marked = false$/;"	v
mask	util/AppendOnlyMap.scala	/^  private var mask = capacity - 1$/;"	v
mask	util/collection/OpenHashSet.scala	/^    val mask = data.length - 1$/;"	V
master	SparkContext.scala	/^    val master: String,$/;"	V
master	deploy/client/Client.scala	/^    var master: ActorRef = null$/;"	v
master	deploy/master/MasterSource.scala	/^private[spark] class MasterSource(val master: Master) extends Source {$/;"	V
master	deploy/master/ui/ApplicationPage.scala	/^  val master = parent.masterActorRef$/;"	V
master	deploy/master/ui/IndexPage.scala	/^  val master = parent.masterActorRef$/;"	V
master	deploy/master/ui/MasterWebUI.scala	/^class MasterWebUI(val master: Master, requestedPort: Int) extends Logging {$/;"	V
master	deploy/worker/Worker.scala	/^  var master: ActorRef = null$/;"	v
master	storage/BlockManager.scala	/^    val master: BlockManagerMaster,$/;"	V
master	ui/UIWorkloadGenerator.scala	/^    val master = args(0)$/;"	V
masterActor	deploy/master/LeaderElectionAgent.scala	/^  val masterActor: ActorRef$/;"	V
masterActor	deploy/master/LeaderElectionAgent.scala	/^private[spark] class MonarchyLeaderAgent(val masterActor: ActorRef) extends LeaderElectionAgent {$/;"	V
masterActor	deploy/master/ZooKeeperLeaderElectionAgent.scala	/^private[spark] class ZooKeeperLeaderElectionAgent(val masterActor: ActorRef, masterUrl: String)$/;"	V
masterActorRef	deploy/master/ui/MasterWebUI.scala	/^  val masterActorRef = master.self$/;"	V
masterActorSystems	deploy/LocalSparkCluster.scala	/^  private val masterActorSystems = ArrayBuffer[ActorSystem]()$/;"	V
masterAddress	deploy/client/Client.scala	/^    var masterAddress: Address = null$/;"	v
masterIndex	deploy/worker/Worker.scala	/^  var masterIndex = 0$/;"	v
masterLock	deploy/worker/Worker.scala	/^  val masterLock: Object = new Object()$/;"	V
masterMetricsSystem	deploy/master/Master.scala	/^  val masterMetricsSystem = MetricsSystem.createMetricsSystem("master")$/;"	V
masterPublicAddress	deploy/master/Master.scala	/^  val masterPublicAddress = {$/;"	V
masterSource	broadcast/TreeBroadcast.scala	/^    val masterSource =$/;"	V
masterSource	deploy/master/Master.scala	/^  val masterSource = new MasterSource(this)$/;"	V
masterStream	deploy/FaultToleranceTest.scala	/^      val masterStream = new InputStreamReader(new URL("http:\/\/%s:8080\/json".format(ip)).openStream)$/;"	V
masterUrl	deploy/LocalSparkCluster.scala	/^    val masterUrl = "spark:\/\/" + localHostname + ":" + masterPort$/;"	V
masterUrl	deploy/master/Master.scala	/^  val masterUrl = "spark:\/\/" + host + ":" + port$/;"	V
masterUrls	SparkContext.scala	/^        val masterUrls = localCluster.start()$/;"	V
masterUrls	SparkContext.scala	/^        val masterUrls = sparkUrl.split(",").map("spark:\/\/" + _)$/;"	V
masterUrls	deploy/FaultToleranceTest.scala	/^    val masterUrls = getMasterUrls(masters)$/;"	V
masterWebUiUrl	deploy/master/Master.scala	/^  var masterWebUiUrl: String = _$/;"	v
masterWithoutProtocol	SparkContext.scala	/^        val masterWithoutProtocol = master.replaceFirst("^mesos:\/\/", "")  \/\/ Strip initial mesos:\/\/$/;"	V
masters	deploy/FaultToleranceTest.scala	/^  val masters = ListBuffer[TestMasterInfo]()$/;"	V
masters	deploy/LocalSparkCluster.scala	/^    val masters = Array(masterUrl)$/;"	V
masters	deploy/master/ZooKeeperLeaderElectionAgent.scala	/^    val masters = zk.getChildren(WORKING_DIR).toList$/;"	V
masters	deploy/worker/WorkerArguments.scala	/^  var masters: Array[String] = null$/;"	v
matches	metrics/MetricsSystem.scala	/^      def matches(name: String, metric: Metric): Boolean = name.startsWith(source.sourceName)$/;"	m
maxAttempts	util/Utils.scala	/^    val maxAttempts = 10$/;"	V
maxBytes	deploy/worker/ui/WorkerWebUI.scala	/^    val maxBytes = 1024 * 1024$/;"	V
maxBytesInFlight	storage/BlockManager.scala	/^  val maxBytesInFlight =$/;"	V
maxCores	deploy/ApplicationDescription.scala	/^    val maxCores: Int, \/* Integer.MAX_VALUE denotes an unlimited number of cores *\/$/;"	V
maxCores	scheduler/cluster/SimrSchedulerBackend.scala	/^  val maxCores = System.getProperty("spark.simr.executor.cores", "1").toInt$/;"	V
maxCores	scheduler/cluster/SparkDeploySchedulerBackend.scala	/^  val maxCores = System.getProperty("spark.cores.max", Int.MaxValue.toString).toInt$/;"	V
maxCores	scheduler/cluster/mesos/CoarseMesosSchedulerBackend.scala	/^  val maxCores = System.getProperty("spark.cores.max", Int.MaxValue.toString).toInt$/;"	V
maxFailures	scheduler/local/LocalScheduler.scala	/^private[spark] class LocalScheduler(threads: Int, val maxFailures: Int, val sc: SparkContext)$/;"	V
maxLeechers	broadcast/TreeBroadcast.scala	/^        var maxLeechers = -1$/;"	v
maxMem	storage/BlockManagerMasterActor.scala	/^      val maxMem: Long,$/;"	V
maxMem	storage/BlockManagerSource.scala	/^      val maxMem = storageStatusList.map(_.maxMem).reduce(_ + _)$/;"	V
maxMem	ui/exec/ExecutorsUI.scala	/^    val maxMem = status.maxMem.toString$/;"	V
maxMem	ui/exec/ExecutorsUI.scala	/^    val maxMem = storageStatusList.map(_.maxMem).fold(0L)(_+_)$/;"	V
maxPartitions	scheduler/DAGScheduler.scala	/^    val maxPartitions = rdd.partitions.length$/;"	V
maxSampleSize	Partitioner.scala	/^      val maxSampleSize = partitions * 20.0$/;"	V
maxSelected	rdd/RDD.scala	/^    var maxSelected = 0$/;"	v
maybeShuffleRead	ui/jobs/StagePage.scala	/^    val maybeShuffleRead = metrics.flatMap{m => m.shuffleReadMetrics}.map{s => s.remoteBytesRead}$/;"	V
maybeShuffleWrite	ui/jobs/StagePage.scala	/^    val maybeShuffleWrite = metrics.flatMap{m => m.shuffleWriteMetrics}.map{s => s.shuffleBytesWritten}$/;"	V
maybeWriteTime	ui/jobs/StagePage.scala	/^    val maybeWriteTime = metrics.flatMap{m => m.shuffleWriteMetrics}.map{s => s.shuffleWriteTime}$/;"	V
mb	network/ConnectionManager.scala	/^      val mb = size * count \/ 1024.0 \/ 1024.0$/;"	V
mb	network/ConnectionManager.scala	/^    val mb = buffers.map(_.remaining).reduceLeft(_ + _) \/ 1024.0 \/ 1024.0$/;"	V
mb	network/ConnectionManager.scala	/^    val mb = size * count \/ 1024.0 \/ 1024.0$/;"	V
mb	network/ConnectionManagerTest.scala	/^        val mb = size * results.size \/ 1024.0 \/ 1024.0$/;"	V
mb	network/SenderTest.scala	/^      val mb = size \/ 1024.0 \/ 1024.0$/;"	V
mean	api/java/JavaDoubleRDD.scala	/^  def mean(): Double = srdd.mean()$/;"	m
mean	partial/BoundedDouble.scala	/^class BoundedDouble(val mean: Double, val confidence: Double, val low: Double, val high: Double) {$/;"	V
mean	partial/CountEvaluator.scala	/^      val mean = (sum + 1 - p) \/ p$/;"	V
mean	partial/GroupedCountEvaluator.scala	/^        val mean = (sum + 1 - p) \/ p$/;"	V
mean	partial/GroupedMeanEvaluator.scala	/^        val mean = counter.mean$/;"	V
mean	partial/GroupedMeanEvaluator.scala	/^        val mean = entry.getValue.mean$/;"	V
mean	partial/MeanEvaluator.scala	/^      val mean = counter.mean$/;"	V
mean	rdd/DoubleRDDFunctions.scala	/^  def mean(): Double = stats().mean$/;"	m
mean	util/StatCounter.scala	/^  def mean: Double = mu$/;"	m
meanApprox	api/java/JavaDoubleRDD.scala	/^  def meanApprox(timeout: Long): PartialResult[BoundedDouble] = srdd.meanApprox(timeout)$/;"	m
meanApprox	api/java/JavaDoubleRDD.scala	/^  def meanApprox(timeout: Long, confidence: Double): PartialResult[BoundedDouble] =$/;"	m
meanApprox	rdd/DoubleRDDFunctions.scala	/^  def meanApprox(timeout: Long, confidence: Double = 0.95): PartialResult[BoundedDouble] = {$/;"	m
meanEstimate	partial/GroupedSumEvaluator.scala	/^        val meanEstimate = counter.mean$/;"	V
meanEstimate	partial/SumEvaluator.scala	/^      val meanEstimate = counter.mean$/;"	V
meanVar	partial/GroupedSumEvaluator.scala	/^        val meanVar = counter.sampleVariance \/ counter.count$/;"	V
meanVar	partial/SumEvaluator.scala	/^      val meanVar = counter.sampleVariance \/ counter.count$/;"	V
medianDuration	scheduler/cluster/ClusterTaskSetManager.scala	/^      val medianDuration = durations(min((0.5 * tasksSuccessful).round.toInt, durations.size - 1))$/;"	V
megabytesToString	util/Utils.scala	/^  def megabytesToString(megabytes: Long): String = {$/;"	m
mem	scheduler/cluster/mesos/CoarseMesosSchedulerBackend.scala	/^        val mem = getResource(offer.getResourcesList, "mem")$/;"	V
mem	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^          val mem = getResource(o.getResourcesList, "mem")$/;"	V
memGaugeSet	metrics/source/JvmSource.scala	/^  val memGaugeSet = new MemoryUsageGaugeSet$/;"	V
memRemaining	storage/StorageUtils.scala	/^  def memRemaining : Long = maxMem - memUsed()$/;"	m
memSize	storage/BlockManager.scala	/^          val memSize = if (inMem) memoryStore.getSize(blockId) else droppedMemorySize$/;"	V
memSize	storage/BlockManagerMessages.scala	/^      var memSize: Long,$/;"	v
memSize	storage/StorageUtils.scala	/^      val memSize = rddBlocks.map(_.memSize).reduce(_ + _)$/;"	V
memUsed	storage/StorageUtils.scala	/^  def memUsed() = blocks.values.map(_.memSize).reduceOption(_+_).getOrElse(0L)$/;"	m
memUsed	ui/exec/ExecutorsUI.scala	/^    val memUsed = status.memUsed().toString$/;"	V
memUsed	ui/exec/ExecutorsUI.scala	/^    val memUsed = storageStatusList.map(_.memUsed()).fold(0L)(_+_)$/;"	V
memUsedByRDD	storage/StorageUtils.scala	/^  def memUsedByRDD(rddId: Int) =$/;"	m
memory	deploy/master/ExecutorInfo.scala	/^    val memory: Int) {$/;"	V
memory	deploy/master/WorkerInfo.scala	/^    val memory: Int,$/;"	V
memory	deploy/worker/ExecutorRunner.scala	/^    val memory: Int,$/;"	V
memory	deploy/worker/WorkerArguments.scala	/^  var memory = inferDefaultMemory()$/;"	v
memory	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^    val memory = Resource.newBuilder()$/;"	V
memoryFraction	storage/BlockManager.scala	/^    val memoryFraction = System.getProperty("spark.storage.memoryFraction", "0.66").toDouble$/;"	V
memoryFree	deploy/master/WorkerInfo.scala	/^  def memoryFree: Int = memory - memoryUsed$/;"	m
memoryFree	deploy/worker/Worker.scala	/^  def memoryFree: Int = memory - memoryUsed$/;"	m
memoryOpts	deploy/worker/ExecutorRunner.scala	/^    val memoryOpts = Seq("-Xms" + memory + "M", "-Xmx" + memory + "M")$/;"	V
memoryPerSlave	deploy/ApplicationDescription.scala	/^    val memoryPerSlave: Int,$/;"	V
memoryPerSlaveInt	SparkContext.scala	/^        val memoryPerSlaveInt = memoryPerSlave.toInt$/;"	V
memoryStore	storage/BlockManager.scala	/^  private[storage] val memoryStore: BlockStore = new MemoryStore(this, maxMemory)$/;"	V
memoryStringToMb	util/Utils.scala	/^  def memoryStringToMb(str: String): Int = {$/;"	m
memoryUsed	deploy/master/WorkerInfo.scala	/^  @transient var memoryUsed: Int = _$/;"	v
memoryUsed	deploy/worker/Worker.scala	/^  var memoryUsed = 0$/;"	v
merge	Accumulators.scala	/^  def merge(term: R) { value_ = param.addInPlace(value_, term)}$/;"	m
merge	partial/ApproximateEvaluator.scala	/^  def merge(outputId: Int, taskResult: U): Unit$/;"	m
merge	util/StatCounter.scala	/^  def merge(other: StatCounter): StatCounter = {$/;"	m
merge	util/StatCounter.scala	/^  def merge(value: Double): StatCounter = {$/;"	m
merge	util/StatCounter.scala	/^  def merge(values: TraversableOnce[Double]): StatCounter = {$/;"	m
mergeMaps	rdd/PairRDDFunctions.scala	/^    def mergeMaps(m1: JHashMap[K, V], m2: JHashMap[K, V]): JHashMap[K, V] = {$/;"	m
mergeMaps	rdd/RDD.scala	/^    def mergeMaps(m1: OLMap[T], m2: OLMap[T]): OLMap[T] = {$/;"	m
mergeResult	rdd/RDD.scala	/^    val mergeResult = (index: Int, taskResult: Option[T]) => {$/;"	V
mergeResult	rdd/RDD.scala	/^    val mergeResult = (index: Int, taskResult: T) => jobResult = op(jobResult, taskResult)$/;"	V
mergeResult	rdd/RDD.scala	/^    val mergeResult = (index: Int, taskResult: U) => jobResult = combOp(jobResult, taskResult)$/;"	V
mergeValue	rdd/PairRDDFunctions.scala	/^    def mergeValue(buf: ArrayBuffer[V], v: V) = buf += v$/;"	m
mesosTaskId	executor/MesosExecutorBackend.scala	/^    val mesosTaskId = TaskID.newBuilder().setValue(taskId.toString).build()$/;"	V
mesosTasks	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^        val mesosTasks = offers.map(o => Collections.emptyList[MesosTaskInfo]())$/;"	V
message	deploy/worker/ExecutorRunner.scala	/^        val message = e.getClass + ": " + e.getMessage$/;"	V
message	deploy/worker/ExecutorRunner.scala	/^      val message = "Command exited with code " + exitCode$/;"	V
message	network/Connection.scala	/^          \/*val message = messages(nextMessageToBeUsed)*\/$/;"	V
message	network/Connection.scala	/^          val message = messages(0)$/;"	V
message	network/Connection.scala	/^          val message = messages.dequeue$/;"	V
message	network/Connection.scala	/^      val message = messages.getOrElseUpdate(header.id, createNewMessage)$/;"	V
message	network/ConnectionManager.scala	/^      val message: Message,$/;"	V
message	scheduler/cluster/ExecutorLossReason.scala	/^class ExecutorLossReason(val message: String) {$/;"	V
messageStatuses	network/ConnectionManager.scala	/^  private val messageStatuses = new HashMap[Int, MessageStatus]$/;"	V
messageText	deploy/client/Client.scala	/^        val messageText = message.map(s => " (" + s + ")").getOrElse("")$/;"	V
messageType	broadcast/MultiTracker.scala	/^                    val messageType = ois.readObject.asInstanceOf[Int]$/;"	V
messages	network/Connection.scala	/^    val messages = new HashMap[Int, BufferMessage]()$/;"	V
messages	network/Connection.scala	/^    val messages = new Queue[Message]()$/;"	V
metaId	broadcast/TorrentBroadcast.scala	/^    val metaId = BroadcastHelperBlockId(broadcastId, "meta")$/;"	V
metaInfo	broadcast/TorrentBroadcast.scala	/^    val metaInfo = TorrentInfo(null, totalBlocks, totalBytes)$/;"	V
metadataCleaner	MapOutputTracker.scala	/^  val metadataCleaner = new MetadataCleaner(MetadataCleanerType.MAP_OUTPUT_TRACKER, this.cleanup)$/;"	V
metadataCleaner	SparkContext.scala	/^  private[spark] val metadataCleaner = new MetadataCleaner(MetadataCleanerType.SPARK_CONTEXT, this.cleanup)$/;"	V
metadataCleaner	scheduler/DAGScheduler.scala	/^  val metadataCleaner = new MetadataCleaner(MetadataCleanerType.DAG_SCHEDULER, this.cleanup)$/;"	V
metadataCleaner	scheduler/ResultTask.scala	/^  val metadataCleaner = new MetadataCleaner(MetadataCleanerType.RESULT_TASK, serializedInfoCache.clearOldValues)$/;"	V
metadataCleaner	scheduler/ShuffleMapTask.scala	/^  val metadataCleaner = new MetadataCleaner(MetadataCleanerType.SHUFFLE_MAP_TASK, serializedInfoCache.clearOldValues)$/;"	V
metadataCleaner	storage/BlockManager.scala	/^  private val metadataCleaner = new MetadataCleaner(MetadataCleanerType.BLOCK_MANAGER, this.dropOldNonBroadcastBlocks)$/;"	V
metadataCleaner	storage/ShuffleBlockManager.scala	/^  val metadataCleaner = new MetadataCleaner(MetadataCleanerType.SHUFFLE_BLOCK_MANAGER, this.cleanup)$/;"	V
method	deploy/worker/WorkerArguments.scala	/^        val method = beanClass.getDeclaredMethod("getTotalPhysicalMemory")$/;"	V
method	deploy/worker/WorkerArguments.scala	/^        val method = beanClass.getDeclaredMethod("getTotalPhysicalMemorySize")$/;"	V
metricRegistry	deploy/master/ApplicationSource.scala	/^  val metricRegistry = new MetricRegistry()$/;"	V
metricRegistry	deploy/master/MasterSource.scala	/^  val metricRegistry = new MetricRegistry()$/;"	V
metricRegistry	deploy/worker/WorkerSource.scala	/^  val metricRegistry = new MetricRegistry()$/;"	V
metricRegistry	executor/ExecutorSource.scala	/^  val metricRegistry = new MetricRegistry()$/;"	V
metricRegistry	metrics/source/JvmSource.scala	/^  val metricRegistry = new MetricRegistry()$/;"	V
metricRegistry	metrics/source/Source.scala	/^  def metricRegistry: MetricRegistry$/;"	m
metricRegistry	scheduler/DAGSchedulerSource.scala	/^  val metricRegistry = new MetricRegistry()$/;"	V
metricRegistry	storage/BlockManagerSource.scala	/^  val metricRegistry = new MetricRegistry()$/;"	V
metrics	executor/Executor.scala	/^          val metrics = attemptedTask.flatMap(t => t.metrics)$/;"	V
metrics	scheduler/Task.scala	/^  var metrics: Option[TaskMetrics] = None$/;"	v
metricsConfig	metrics/MetricsSystem.scala	/^  val metricsConfig = new MetricsConfig(Option(confFile))$/;"	V
metricsHandlers	deploy/master/ui/MasterWebUI.scala	/^  val metricsHandlers = master.masterMetricsSystem.getServletHandlers ++$/;"	V
metricsHandlers	deploy/worker/ui/WorkerWebUI.scala	/^  val metricsHandlers = worker.metricsSystem.getServletHandlers$/;"	V
metricsServlet	metrics/MetricsSystem.scala	/^  private var metricsServlet: Option[MetricsServlet] = None$/;"	v
metricsServletHandlers	ui/SparkUI.scala	/^  val metricsServletHandlers = SparkEnv.get.metricsSystem.getServletHandlers$/;"	V
metricsSystem	SparkEnv.scala	/^    val metricsSystem = if (isDriver) {$/;"	V
metricsSystem	SparkEnv.scala	/^    val metricsSystem: MetricsSystem) {$/;"	V
metricsSystem	deploy/worker/Worker.scala	/^  val metricsSystem = MetricsSystem.createMetricsSystem("worker")$/;"	V
millisToString	scheduler/SparkListener.scala	/^  def millisToString(ms: Long) = {$/;"	m
minBlocksIndices	broadcast/BitTorrentBroadcast.scala	/^          var minBlocksIndices = ListBuffer[Int]()$/;"	v
minFinishedForSpeculation	scheduler/cluster/ClusterTaskSetManager.scala	/^    val minFinishedForSpeculation = (SPECULATION_QUANTILE * numTasks).floor.toInt$/;"	V
minPowerOfTwo	rdd/CoalescedRDD.scala	/^    val minPowerOfTwo = if (groupArr(r1).size < groupArr(r2).size) groupArr(r1) else groupArr(r2)$/;"	V
minRequestSize	storage/BlockFetcherIterator.scala	/^          val minRequestSize = math.max(maxBytesInFlight \/ 5, 1L)$/;"	V
minSeenTime	storage/BlockManagerMasterActor.scala	/^    val minSeenTime = now - slaveTimeout$/;"	V
minShare	scheduler/Pool.scala	/^  var minShare = initMinShare$/;"	v
minShare	scheduler/Schedulable.scala	/^  def minShare: Int$/;"	m
minShare	scheduler/SchedulableBuilder.scala	/^      var minShare = DEFAULT_MINIMUM_SHARE$/;"	v
minShare	scheduler/cluster/ClusterTaskSetManager.scala	/^  var minShare = 0$/;"	v
minShare	scheduler/local/LocalTaskSetManager.scala	/^  var minShare: Int = 0$/;"	v
minShare1	scheduler/SchedulingAlgorithm.scala	/^    val minShare1 = s1.minShare$/;"	V
minShare2	scheduler/SchedulingAlgorithm.scala	/^    val minShare2 = s2.minShare$/;"	V
minShareRatio1	scheduler/SchedulingAlgorithm.scala	/^    val minShareRatio1 = runningTasks1.toDouble \/ math.max(minShare1, 1.0).toDouble$/;"	V
minShareRatio2	scheduler/SchedulingAlgorithm.scala	/^    val minShareRatio2 = runningTasks2.toDouble \/ math.max(minShare2, 1.0).toDouble$/;"	V
minVal	broadcast/BitTorrentBroadcast.scala	/^          var minVal = Integer.MAX_VALUE$/;"	v
minute	util/Utils.scala	/^    val minute = 60 * second$/;"	V
minutes	deploy/WebUI.scala	/^    val minutes = seconds \/ 60$/;"	V
minutes	scheduler/SparkListener.scala	/^  val minutes = seconds * 60$/;"	V
missing	scheduler/DAGScheduler.scala	/^      val missing = getMissingParentStages(stage).sortBy(_.id)$/;"	V
missing	scheduler/DAGScheduler.scala	/^    val missing = new HashSet[Stage]$/;"	V
mkdir	deploy/master/SparkZooKeeperSession.scala	/^  def mkdir(path: String) {$/;"	m
mkdirRecursive	deploy/master/SparkZooKeeperSession.scala	/^  def mkdirRecursive(path: String) {$/;"	m
mode	metrics/sink/GangliaSink.scala	/^  val mode = propertyToOption(GANGLIA_KEY_MODE)$/;"	V
mountCmd	deploy/FaultToleranceTest.scala	/^    val mountCmd = if (mountDir != "") { " -v " + mountDir } else ""$/;"	V
move	util/collection/OpenHashMap.scala	/^  protected var move = (oldPos: Int, newPos: Int) => {$/;"	v
move	util/collection/OpenHashSet.scala	/^  private val move = move1 _$/;"	V
move	util/collection/PrimitiveKeyOpenHashMap.scala	/^  protected var move = (oldPos: Int, newPos: Int) => {$/;"	v
ms	network/ConnectionManager.scala	/^      val ms = finishTime - startTime$/;"	V
ms	network/ConnectionManager.scala	/^    val ms = finishTime - startTime$/;"	V
ms	network/ConnectionManagerTest.scala	/^        val ms = finishTime - startTime$/;"	V
ms	network/SenderTest.scala	/^      val ms = finishTime - startTime$/;"	V
msDurationToString	util/Utils.scala	/^  def msDurationToString(ms: Long): String = {$/;"	m
msg	scheduler/cluster/CoarseGrainedSchedulerBackend.scala	/^            val msg = "Ignored task status update (%d state %s) from unknown executor %s with ID %s"$/;"	V
mt	util/collection/OpenHashSet.scala	/^    val mt = classManifest[T]$/;"	V
mu	util/StatCounter.scala	/^  private var mu: Double = 0  \/\/ Running mean of our values$/;"	v
multiplier	rdd/RDD.scala	/^    val multiplier = 3.0$/;"	V
multiply	util/Vector.scala	/^  def multiply (d: Double) = this * d$/;"	m
myInfo	storage/BlockManager.scala	/^    val myInfo = {$/;"	V
myLeaderFile	deploy/master/ZooKeeperLeaderElectionAgent.scala	/^  private var myLeaderFile: String = _$/;"	v
myLocalityLevels	scheduler/cluster/ClusterTaskSetManager.scala	/^  val myLocalityLevels = computeValidLocalityLevels()$/;"	V
myName	util/ClosureCleaner.scala	/^  var myName: String = null$/;"	v
myPending	scheduler/DAGScheduler.scala	/^    val myPending = pendingTasks.getOrElseUpdate(stage, new HashSet)$/;"	V
myResult	rdd/RDD.scala	/^    val myResult = mapPartitions(countPartition).reduce(mergeMaps)$/;"	V
n	util/StatCounter.scala	/^  private var n: Long = 0     \/\/ Running count of our values$/;"	v
n	util/Utils.scala	/^    var n = 0$/;"	v
name	SparkEnv.scala	/^      val name = System.getProperty(propertyName, defaultClassName)$/;"	V
name	deploy/ApplicationDescription.scala	/^    val name: String,$/;"	V
name	rdd/RDD.scala	/^  @transient var name: String = null$/;"	v
name	scheduler/Pool.scala	/^  var name = poolName$/;"	v
name	scheduler/Schedulable.scala	/^  def name: String$/;"	m
name	scheduler/Stage.scala	/^  val name = callSite.getOrElse(rdd.origin)$/;"	V
name	scheduler/StageInfo.scala	/^  val name = stage.name$/;"	V
name	scheduler/TaskDescription.scala	/^    val name: String,$/;"	V
name	scheduler/cluster/ClusterTaskSetManager.scala	/^  var name = "TaskSet_"+taskSet.stageId.toString$/;"	v
name	scheduler/local/LocalTaskSetManager.scala	/^  var name: String = "TaskSet_" + taskSet.stageId.toString$/;"	v
name	storage/BlockId.scala	/^  def name = "broadcast_" + broadcastId$/;"	m
name	storage/BlockId.scala	/^  def name = "input-" + streamId + "-" + uniqueId$/;"	m
name	storage/BlockId.scala	/^  def name = "rdd_" + rddId + "_" + splitIndex$/;"	m
name	storage/BlockId.scala	/^  def name = "shuffle_" + shuffleId + "_" + mapId + "_" + reduceId$/;"	m
name	storage/BlockId.scala	/^  def name = "taskresult_" + taskId$/;"	m
name	storage/BlockId.scala	/^  def name = "test_" + id$/;"	m
name	storage/BlockId.scala	/^  def name = broadcastId.name + "_" + hType$/;"	m
name	storage/BlockId.scala	/^  def name: String$/;"	m
name	util/MetadataCleaner.scala	/^  val name = cleanerType.toString$/;"	V
nameLink	ui/jobs/StageTable.scala	/^    val nameLink =$/;"	V
needBlocksBitVector	broadcast/BitTorrentBroadcast.scala	/^        var needBlocksBitVector: BitSet = null$/;"	v
needForceReregister	network/Connection.scala	/^  private var needForceReregister = false$/;"	v
needReregister	network/ConnectionManager.scala	/^            val needReregister = register || conn.resetForceReregister()$/;"	V
needReregister	storage/BlockManager.scala	/^    val needReregister = !tryToReportBlockStatus(blockId, info, droppedMemorySize)$/;"	V
net.liftweb.json.JsonAST.JValue	deploy/master/ui/ApplicationPage.scala	/^import net.liftweb.json.JsonAST.JValue$/;"	i
net.liftweb.json.JsonAST.JValue	deploy/master/ui/IndexPage.scala	/^import net.liftweb.json.JsonAST.JValue$/;"	i
net.liftweb.json.JsonAST.JValue	deploy/worker/ui/IndexPage.scala	/^import net.liftweb.json.JsonAST.JValue$/;"	i
net.liftweb.json.JsonDSL._	deploy/JsonProtocol.scala	/^import net.liftweb.json.JsonDSL._$/;"	i
net.liftweb.json.JsonParser	deploy/FaultToleranceTest.scala	/^import net.liftweb.json.JsonParser$/;"	i
net.liftweb.json.{JValue, pretty, render}	ui/JettyUtils.scala	/^import net.liftweb.json.{JValue, pretty, render}$/;"	i
nettyPort	storage/BlockManager.scala	/^  private val nettyPort: Int = {$/;"	V
nettyPort	storage/BlockManagerId.scala	/^  def nettyPort: Int = nettyPort_$/;"	m
nettyPortConfig	storage/BlockManager.scala	/^    val nettyPortConfig = System.getProperty("spark.shuffle.sender.port", "0").toInt$/;"	V
nettyPort_	storage/BlockManagerId.scala	/^    private var nettyPort_ : Int$/;"	v
networkSize	storage/BlockFetcherIterator.scala	/^            val networkSize = blockMessage.getData.limit()$/;"	V
newAPIHadoopFile	SparkContext.scala	/^  def newAPIHadoopFile[K, V, F <: NewInputFormat[K, V]]($/;"	m
newAPIHadoopFile	SparkContext.scala	/^  def newAPIHadoopFile[K, V, F <: NewInputFormat[K, V]](path: String)$/;"	m
newAPIHadoopFile	api/java/JavaSparkContext.scala	/^  def newAPIHadoopFile[K, V, F <: NewInputFormat[K, V]]($/;"	m
newAPIHadoopRDD	SparkContext.scala	/^  def newAPIHadoopRDD[K, V, F <: NewInputFormat[K, V]]($/;"	m
newAPIHadoopRDD	api/java/JavaSparkContext.scala	/^  def newAPIHadoopRDD[K, V, F <: NewInputFormat[K, V]]($/;"	m
newApplicationId	deploy/master/Master.scala	/^  def newApplicationId(submitDate: Date): String = {$/;"	m
newArray	util/collection/PrimitiveVector.scala	/^    val newArray = new Array[V](newLength)$/;"	V
newAttemptId	scheduler/Stage.scala	/^  def newAttemptId(): Int = {$/;"	m
newBitset	util/collection/OpenHashSet.scala	/^    val newBitset = new BitSet(newCapacity)$/;"	V
newBlockMessage	storage/BlockMessage.scala	/^    val newBlockMessage = new BlockMessage()$/;"	V
newBlockMessage	storage/BlockMessageArray.scala	/^      val newBlockMessage = BlockMessage.fromByteBuffer(newBuffer)$/;"	V
newBlockMessageArray	storage/BlockMessageArray.scala	/^    val newBlockMessageArray = BlockMessageArray.fromBufferMessage(newBufferMessage)$/;"	V
newBlockMessageArray	storage/BlockMessageArray.scala	/^    val newBlockMessageArray = new BlockMessageArray()$/;"	V
newBlockMessages	storage/BlockMessageArray.scala	/^    val newBlockMessages = new ArrayBuffer[BlockMessage]()$/;"	V
newBlocks	storage/StorageUtils.scala	/^      val newBlocks = status.rddBlocks.filterKeys(_.rddId == rddId).toMap[BlockId, BlockStatus]$/;"	V
newBroadcast	broadcast/BitTorrentBroadcast.scala	/^  def newBroadcast[T](value_ : T, isLocal: Boolean, id: Long) =$/;"	m
newBroadcast	broadcast/Broadcast.scala	/^  def newBroadcast[T](value_ : T, isLocal: Boolean) =$/;"	m
newBroadcast	broadcast/BroadcastFactory.scala	/^  def newBroadcast[T](value: T, isLocal: Boolean, id: Long): Broadcast[T]$/;"	m
newBroadcast	broadcast/HttpBroadcast.scala	/^  def newBroadcast[T](value_ : T, isLocal: Boolean, id: Long) =$/;"	m
newBroadcast	broadcast/TorrentBroadcast.scala	/^  def newBroadcast[T](value_ : T, isLocal: Boolean, id: Long) =$/;"	m
newBroadcast	broadcast/TreeBroadcast.scala	/^  def newBroadcast[T](value_ : T, isLocal: Boolean, id: Long) =$/;"	m
newBuffer	network/BufferMessage.scala	/^        val newBuffer = if (buffer.remaining <= maxChunkSize) {$/;"	V
newBuffer	network/BufferMessage.scala	/^      val newBuffer = buffer.slice().limit(chunkSize).asInstanceOf[ByteBuffer]$/;"	V
newBuffer	storage/BlockMessageArray.scala	/^      val newBuffer = buffer.slice()$/;"	V
newBuffer	storage/BlockMessageArray.scala	/^    val newBuffer = ByteBuffer.allocate(totalSize)$/;"	V
newBufferMessage	storage/BlockMessageArray.scala	/^    val newBufferMessage = Message.createBufferMessage(newBuffer) $/;"	V
newCapacity	util/AppendOnlyMap.scala	/^    val newCapacity = capacity * 2$/;"	V
newCapacity	util/collection/OpenHashSet.scala	/^    val newCapacity = _capacity * 2$/;"	V
newChannel	network/ConnectionManager.scala	/^    var newChannel = serverChannel.accept()$/;"	v
newChunk	network/BufferMessage.scala	/^        val newChunk = new MessageChunk(new MessageChunkHeader($/;"	V
newChunk	network/BufferMessage.scala	/^      val newChunk = new MessageChunk($/;"	V
newChunk	network/BufferMessage.scala	/^      val newChunk = new MessageChunk(new MessageChunkHeader($/;"	V
newConfiguration	deploy/SparkHadoopUtil.scala	/^  def newConfiguration(): Configuration = new Configuration()$/;"	m
newConnection	network/ConnectionManager.scala	/^        val newConnection = new ReceivingConnection(newChannel, selector)$/;"	V
newConnection	network/ConnectionManager.scala	/^      val newConnection = new SendingConnection(inetSocketAddress, selector, connectionManagerId)$/;"	V
newCtor	util/ClosureCleaner.scala	/^      val newCtor = rf.newConstructorForSerialization(cls, parentCtor)$/;"	V
newDaemonCachedThreadPool	util/Utils.scala	/^  def newDaemonCachedThreadPool(prefix: String): ThreadPoolExecutor = {$/;"	m
newDaemonFixedThreadPool	util/Utils.scala	/^  def newDaemonFixedThreadPool(nThreads: Int, prefix: String): ThreadPoolExecutor = {$/;"	m
newData	util/AppendOnlyMap.scala	/^    val newData = new Array[AnyRef](2 * newCapacity)$/;"	V
newData	util/collection/OpenHashSet.scala	/^    val newData = new Array[T](newCapacity)$/;"	V
newDir	storage/DiskBlockManager.scala	/^          val newDir = new File(localDirs(dirId), "%02x".format(subDirId))$/;"	V
newHeader	network/netty/FileHeader.scala	/^    val newHeader = FileHeader.create(buf)$/;"	V
newId	Accumulators.scala	/^  def newId: Long = synchronized {$/;"	m
newInfo	util/SizeEstimator.scala	/^    val newInfo = new ClassInfo(shellSize, pointerFields)$/;"	V
newInputFormat	rdd/HadoopRDD.scala	/^    val newInputFormat = ReflectionUtils.newInstance(inputFormatClass.asInstanceOf[Class[_]], conf)$/;"	V
newInstance	serializer/JavaSerializer.scala	/^  def newInstance(): SerializerInstance = new JavaSerializerInstance$/;"	m
newInstance	serializer/KryoSerializer.scala	/^  def newInstance(): SerializerInstance = {$/;"	m
newInstance	serializer/Serializer.scala	/^  def newInstance(): SerializerInstance$/;"	m
newJobConf	rdd/HadoopRDD.scala	/^      val newJobConf = new JobConf(broadcastedConf.value.value)$/;"	V
newKryo	serializer/KryoSerializer.scala	/^  def newKryo(): Kryo = {$/;"	m
newKryoOutput	serializer/KryoSerializer.scala	/^  def newKryoOutput() = new KryoOutput(bufferSize)$/;"	m
newList	scheduler/Stage.scala	/^      val newList = prevList.filterNot(_.location.executorId == execId)$/;"	V
newList	scheduler/Stage.scala	/^    val newList = prevList.filterNot(_.location == bmAddress)$/;"	V
newMap	util/TimeStampedHashMap.scala	/^    val newMap = new TimeStampedHashMap[A, B1]$/;"	V
newMap	util/TimeStampedHashMap.scala	/^    val newMap = new TimeStampedHashMap[A, B]$/;"	V
newMask	util/AppendOnlyMap.scala	/^    val newMask = newCapacity - 1$/;"	V
newMesosTaskId	scheduler/cluster/mesos/CoarseMesosSchedulerBackend.scala	/^  def newMesosTaskId(): Int = {$/;"	m
newMessage	network/Connection.scala	/^        val newMessage = Message.create(header).asInstanceOf[BufferMessage]$/;"	V
newMessage	network/Message.scala	/^    val newMessage: Message = header.typ match {$/;"	V
newPeerToTalkTo	broadcast/BitTorrentBroadcast.scala	/^          var newPeerToTalkTo = oisSource.readObject.asInstanceOf[SourceInfo]$/;"	v
newPos	util/AppendOnlyMap.scala	/^        var newPos = rehash(key.hashCode) & newMask$/;"	v
newPos	util/collection/OpenHashSet.scala	/^        val newPos = putInto(newBitset, newData, _data(pos))$/;"	V
newRDD	rdd/RDDCheckpointData.scala	/^    val newRDD = new CheckpointRDD[T](rdd.context, path.toString)$/;"	V
newRemainingMem	storage/StorageUtils.scala	/^      \/\/val newRemainingMem = status.maxMem - newBlocks.values.map(_.memSize).reduce(_ + _)$/;"	V
newSet	util/TimeStampedHashSet.scala	/^    val newSet = new TimeStampedHashSet[A]$/;"	V
newTaskId	scheduler/cluster/ClusterScheduler.scala	/^  def newTaskId(): Long = nextTaskId.getAndIncrement()$/;"	m
newValue	util/AppendOnlyMap.scala	/^        val newValue = updateFunc(false, null.asInstanceOf[V])$/;"	V
newValue	util/AppendOnlyMap.scala	/^        val newValue = updateFunc(true, data(2 * pos + 1).asInstanceOf[V])$/;"	V
newValue	util/collection/OpenHashMap.scala	/^        val newValue = defaultValue$/;"	V
newValue	util/collection/PrimitiveKeyOpenHashMap.scala	/^      val newValue = defaultValue$/;"	V
newlyRunnable	scheduler/DAGScheduler.scala	/^                val newlyRunnable = new ArrayBuffer[Stage]$/;"	V
next	InterruptibleIterator.scala	/^  def next(): T = delegate.next()$/;"	m
next	api/python/PythonRDD.scala	/^      def next(): Array[Byte] = {$/;"	m
next	rdd/CoalescedRDD.scala	/^    def next(): (String, Partition) = {$/;"	m
next	rdd/PipedRDD.scala	/^      def next() = lines.next()$/;"	m
next	util/CompletionIterator.scala	/^  def next = sub.next$/;"	m
next	util/IdGenerator.scala	/^  def next: Int = id.incrementAndGet$/;"	m
next	util/collection/OpenHashMap.scala	/^    def next() = {$/;"	m
next	util/collection/PrimitiveKeyOpenHashMap.scala	/^    def next() = {$/;"	m
nextAppNumber	deploy/master/Master.scala	/^  var nextAppNumber = 0$/;"	v
nextAttemptId	scheduler/Stage.scala	/^  private var nextAttemptId = 0$/;"	v
nextBroadcastId	broadcast/Broadcast.scala	/^  private val nextBroadcastId = new AtomicLong(0)$/;"	V
nextButton	deploy/worker/ui/WorkerWebUI.scala	/^    val nextButton =$/;"	V
nextChar	util/Utils.scala	/^      var nextChar = s.charAt(i)$/;"	v
nextExecutorId	deploy/master/ApplicationInfo.scala	/^  @transient private var nextExecutorId: Int = _$/;"	v
nextFileId	storage/ShuffleBlockManager.scala	/^    val nextFileId = new AtomicInteger(0)$/;"	V
nextFloat	ui/UIWorkloadGenerator.scala	/^    def nextFloat() = (new Random()).nextFloat()$/;"	m
nextJobId	scheduler/DAGScheduler.scala	/^  private[scheduler] val nextJobId = new AtomicInteger(0)$/;"	V
nextMesosTaskId	scheduler/cluster/mesos/CoarseMesosSchedulerBackend.scala	/^  var nextMesosTaskId = 0$/;"	v
nextMessageToBeUsed	network/Connection.scala	/^    var nextMessageToBeUsed = 0$/;"	v
nextPair	util/collection/OpenHashMap.scala	/^    var nextPair: (K, V) = computeNextPair()$/;"	v
nextPair	util/collection/PrimitiveKeyOpenHashMap.scala	/^    var nextPair: (K, V) = computeNextPair()$/;"	v
nextPos	util/collection/OpenHashSet.scala	/^  def nextPos(fromPos: Int): Int = _bitset.nextSetBit(fromPos)$/;"	m
nextRddId	SparkContext.scala	/^  private val nextRddId = new AtomicInteger(0)$/;"	V
nextSetBit	util/collection/BitSet.scala	/^  def nextSetBit(fromIndex: Int): Int = {$/;"	m
nextShuffleId	SparkContext.scala	/^  private val nextShuffleId = new AtomicInteger(0)$/;"	V
nextStageId	scheduler/DAGScheduler.scala	/^  private val nextStageId = new AtomicInteger(0)$/;"	V
nextTaskId	scheduler/cluster/ClusterScheduler.scala	/^  val nextTaskId = new AtomicLong(0)$/;"	V
nextValue	util/AppendOnlyMap.scala	/^    def nextValue(): (K, V) = {$/;"	m
nextValue	util/NextIterator.scala	/^  private var nextValue: U = _$/;"	v
noLocality	rdd/CoalescedRDD.scala	/^  var noLocality = true  \/\/ if true if no preferredLocations exists for parent RDD$/;"	v
nodeToSplit	scheduler/InputFormatInfo.scala	/^    val nodeToSplit = new HashMap[String, HashSet[SplitInfo]]$/;"	V
nonNegativeHash	util/Utils.scala	/^  def nonNegativeHash(obj: AnyRef): Int = {$/;"	m
nonNegativeMod	util/Utils.scala	/^  def nonNegativeMod(x: Int, mod: Int): Int = {$/;"	m
normalApprox	partial/StudentTCacher.scala	/^  val normalApprox = Probability.normalInverse(1 - (1 - confidence) \/ 2)$/;"	V
now	SparkHadoopWriter.scala	/^  private val now = new Date()$/;"	V
now	deploy/master/Master.scala	/^    val now = System.currentTimeMillis()$/;"	V
now	scheduler/cluster/ClusterTaskSetManager.scala	/^          val now = clock.getTime()$/;"	V
now	storage/BlockManagerMasterActor.scala	/^    val now = System.currentTimeMillis()$/;"	V
now	ui/jobs/IndexPage.scala	/^      val now = System.currentTimeMillis()$/;"	V
now	ui/jobs/StagePage.scala	/^      val now = System.currentTimeMillis()$/;"	V
now	util/RateLimitedOutputStream.scala	/^    val now = System.nanoTime$/;"	V
nullValue	util/AppendOnlyMap.scala	/^  private var nullValue: V = null.asInstanceOf[V]$/;"	v
nullValue	util/collection/OpenHashMap.scala	/^  private var nullValue: V = null.asInstanceOf[V]$/;"	v
numAlive	deploy/FaultToleranceTest.scala	/^    var numAlive = 0$/;"	v
numAvailableOutputs	scheduler/Stage.scala	/^  var numAvailableOutputs = 0$/;"	v
numBlockConsumed	storage/ThreadingTest.scala	/^    var numBlockConsumed = 0$/;"	v
numBlocks	storage/ShuffleBlockManager.scala	/^    def numBlocks = mapIdToIndex.size$/;"	m
numBlocksPerProducer	storage/ThreadingTest.scala	/^  val numBlocksPerProducer = 20000$/;"	V
numBlocksRemoved	storage/BlockManagerSlaveActor.scala	/^      val numBlocksRemoved = blockManager.removeRdd(rddId)$/;"	V
numBlocksToSend	broadcast/BitTorrentBroadcast.scala	/^          var numBlocksToSend = MultiTracker.MaxChatBlocks$/;"	v
numBytes	scheduler/ResultTask.scala	/^    val numBytes = in.readInt()$/;"	V
numBytes	scheduler/ShuffleMapTask.scala	/^    val numBytes = in.readInt()$/;"	V
numCompleted	ui/jobs/StagePage.scala	/^      val numCompleted = tasks.count(_._1.finished)$/;"	V
numCopiesPerBlock	broadcast/BitTorrentBroadcast.scala	/^          var numCopiesPerBlock = Array.tabulate [Int](totalBlocks)(_ => 0)$/;"	v
numCopiesPerBlock	broadcast/BitTorrentBroadcast.scala	/^      var numCopiesPerBlock = Array.tabulate [Int](totalBlocks)(_ => 0)$/;"	v
numCopiesSent	broadcast/BitTorrentBroadcast.scala	/^  @transient var numCopiesSent: Array[Int] = null$/;"	v
numCores	scheduler/cluster/CoarseGrainedSchedulerBackend.scala	/^        val numCores = freeCores(executorId)$/;"	V
numCreated	rdd/CoalescedRDD.scala	/^    var numCreated = 0$/;"	v
numFailed	deploy/FaultToleranceTest.scala	/^  var numFailed = 0$/;"	v
numFailures	scheduler/cluster/ClusterTaskSetManager.scala	/^  val numFailures = new Array[Int](numTasks)$/;"	V
numFailures	scheduler/local/LocalTaskSetManager.scala	/^  val numFailures = new Array[Int](numTasks)$/;"	V
numFiles	scheduler/Task.scala	/^    val numFiles = dataIn.readInt()$/;"	V
numFinished	scheduler/ActiveJob.scala	/^  var numFinished = 0$/;"	v
numFinished	scheduler/local/LocalTaskSetManager.scala	/^  var numFinished = 0$/;"	v
numGets	storage/BlockFetcherIterator.scala	/^      val numGets = remoteRequests.size - fetchRequests.size$/;"	V
numJars	scheduler/Task.scala	/^    val numJars = dataIn.readInt()$/;"	V
numLiveApps	deploy/FaultToleranceTest.scala	/^    var numLiveApps = 0$/;"	v
numLiveApps	deploy/FaultToleranceTest.scala	/^  var numLiveApps = 0$/;"	v
numLocal	storage/BlockFetcherIterator.scala	/^    private var numLocal = 0$/;"	v
numLocalBlocks	storage/BlockFetchTracker.scala	/^  def numLocalBlocks: Int$/;"	m
numMaps	storage/StoragePerfTester.scala	/^    val numMaps = sys.env.get("NUM_MAPS").map(_.toInt).getOrElse(8)$/;"	V
numOutputSplits	scheduler/ShuffleMapTask.scala	/^    val numOutputSplits = dep.partitioner.numPartitions$/;"	V
numOutputSplits	storage/StoragePerfTester.scala	/^    val numOutputSplits = sys.env.get("NUM_REDUCERS").map(_.toInt).getOrElse(500)$/;"	V
numPart	rdd/CheckpointRDD.scala	/^      val numPart =  partitionFiles.size$/;"	V
numPartitions	Partitioner.scala	/^  def numPartitions = partitions$/;"	m
numPartitions	Partitioner.scala	/^  def numPartitions: Int$/;"	m
numPartitions	api/python/PythonPartitioner.scala	/^  override val numPartitions: Int,$/;"	V
numPartitions	rdd/CheckpointRDD.scala	/^    val numPartitions =$/;"	V
numPartitions	scheduler/ActiveJob.scala	/^  val numPartitions = partitions.length$/;"	V
numPartitions	scheduler/Stage.scala	/^  val numPartitions = rdd.partitions.size$/;"	V
numPartitions	scheduler/StageInfo.scala	/^  val numPartitions = stage.numPartitions$/;"	V
numPartitionsInRdd2	rdd/CartesianRDD.scala	/^  val numPartitionsInRdd2 = rdd2.partitions.size$/;"	V
numPartsToTry	rdd/AsyncRDDActions.scala	/^        var numPartsToTry = 1$/;"	v
numPartsToTry	rdd/RDD.scala	/^      var numPartsToTry = 1$/;"	v
numPassed	deploy/FaultToleranceTest.scala	/^  var numPassed = 0$/;"	v
numProducers	storage/ThreadingTest.scala	/^  val numProducers = 5$/;"	V
numRdds	rdd/CoGroupedRDD.scala	/^    val numRdds = split.deps.size$/;"	V
numRemote	storage/BlockFetcherIterator.scala	/^    private var numRemote = 0$/;"	v
numRemoteBlocks	storage/BlockFetchTracker.scala	/^  def numRemoteBlocks: Int$/;"	m
numRunningTasks	scheduler/cluster/ClusterTaskSetManager.scala	/^    val numRunningTasks = runningTasksSet.size$/;"	V
numStandby	deploy/FaultToleranceTest.scala	/^    var numStandby = 0$/;"	v
numTasks	scheduler/Stage.scala	/^    val numTasks: Int,$/;"	V
numTasks	scheduler/StageInfo.scala	/^  val numTasks = stage.numTasks$/;"	V
numTasks	scheduler/cluster/ClusterTaskSetManager.scala	/^  val numTasks = tasks.length$/;"	V
numTasks	scheduler/local/LocalTaskSetManager.scala	/^  val numTasks = taskSet.tasks.size$/;"	V
numThreadsToCreate	broadcast/BitTorrentBroadcast.scala	/^        var numThreadsToCreate = 0$/;"	v
numTotalJobs	scheduler/DAGScheduler.scala	/^  def numTotalJobs: Int = nextJobId.get()$/;"	m
numUpdates	scheduler/TaskResult.scala	/^    val numUpdates = in.readInt$/;"	V
numUsable	deploy/master/Master.scala	/^        val numUsable = usableWorkers.length$/;"	V
numWords	util/collection/BitSet.scala	/^  private[this] val numWords = words.length$/;"	V
numfmt	SparkHadoopWriter.scala	/^    val numfmt = NumberFormat.getInstance()$/;"	V
obj	api/python/PythonRDD.scala	/^              val obj = new Array[Byte](exLength)$/;"	V
obj	api/python/PythonRDD.scala	/^              val obj = new Array[Byte](length)$/;"	V
obj	api/python/PythonRDD.scala	/^        val obj = _nextObj$/;"	V
obj	api/python/PythonRDD.scala	/^        val obj = new Array[Byte](length)$/;"	V
obj	broadcast/HttpBroadcast.scala	/^    val obj = serIn.readObject[T]()$/;"	V
obj	serializer/KryoSerializer.scala	/^    val obj = kryo.readClassAndObject(input).asInstanceOf[T]$/;"	V
obj	storage/BlockManagerId.scala	/^    val obj = new BlockManagerId()$/;"	V
obj	storage/StorageLevel.scala	/^    val obj = new StorageLevel()$/;"	V
obj	util/ClosureCleaner.scala	/^      val obj = newCtor.newInstance().asInstanceOf[AnyRef]$/;"	V
objIn	MapOutputTracker.scala	/^    val objIn = new ObjectInputStream(new GZIPInputStream(new ByteArrayInputStream(bytes)))$/;"	V
objIn	scheduler/ResultTask.scala	/^    val objIn = ser.deserializeStream(in)$/;"	V
objIn	scheduler/ShuffleMapTask.scala	/^      val objIn = ser.deserializeStream(in)$/;"	V
objIn	scheduler/ShuffleMapTask.scala	/^    val objIn = new ObjectInputStream(in)$/;"	V
objIn	serializer/JavaSerializer.scala	/^  val objIn = new ObjectInputStream(in) {$/;"	V
objOut	MapOutputTracker.scala	/^    val objOut = new ObjectOutputStream(new GZIPOutputStream(out))$/;"	V
objOut	scheduler/ResultTask.scala	/^        val objOut = ser.serializeStream(new GZIPOutputStream(out))$/;"	V
objOut	scheduler/ShuffleMapTask.scala	/^        val objOut = ser.serializeStream(new GZIPOutputStream(out))$/;"	V
objOut	serializer/JavaSerializer.scala	/^  val objOut = new ObjectOutputStream(out)$/;"	V
objOut	storage/BlockObjectWriter.scala	/^  private var objOut: SerializationStream = null$/;"	v
objectFile	SparkContext.scala	/^  def objectFile[T: ClassManifest]($/;"	m
objectFile	api/java/JavaSparkContext.scala	/^  def objectFile[T](path: String): JavaRDD[T] = {$/;"	m
objectFile	api/java/JavaSparkContext.scala	/^  def objectFile[T](path: String, minSplits: Int): JavaRDD[T] = {$/;"	m
objectSer	scheduler/TaskResult.scala	/^    val objectSer = SparkEnv.get.serializer.newInstance()$/;"	V
objectSize	util/SizeEstimator.scala	/^  private var objectSize = 8$/;"	v
objs	api/python/PythonRDD.scala	/^    val objs = new collection.mutable.ArrayBuffer[Array[Byte]]$/;"	V
offerNum	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^            val offerNum = offerableIndices(index)$/;"	V
offerableIndices	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^        val offerableIndices = new ArrayBuffer[Int]$/;"	V
offerableWorkers	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^        val offerableWorkers = new ArrayBuffer[WorkerOffer]$/;"	V
offset	deploy/worker/ui/WorkerWebUI.scala	/^    val offset = Option(request.getParameter("offset")).map(_.toLong)$/;"	V
offset	storage/ShuffleBlockManager.scala	/^        val offset = blockOffsets(index)$/;"	V
offsetBytes	util/Utils.scala	/^  def offsetBytes(path: String, start: Long, end: Long): String = {$/;"	m
offsets	storage/ShuffleBlockManager.scala	/^            val offsets = writers.map(_.fileSegment().offset)$/;"	V
ois	broadcast/BitTorrentBroadcast.scala	/^      private val ois = new ObjectInputStream(clientSocket.getInputStream)$/;"	V
ois	broadcast/MultiTracker.scala	/^                  val ois = new ObjectInputStream(clientSocket.getInputStream)$/;"	V
ois	broadcast/TreeBroadcast.scala	/^      private val ois = new ObjectInputStream(clientSocket.getInputStream)$/;"	V
ois	util/Utils.scala	/^    val ois = new ObjectInputStream(bis) {$/;"	V
ois	util/Utils.scala	/^    val ois = new ObjectInputStream(bis)$/;"	V
oisDriver	broadcast/TreeBroadcast.scala	/^    var oisDriver: ObjectInputStream = null$/;"	v
oisGuide	broadcast/BitTorrentBroadcast.scala	/^      var oisGuide: ObjectInputStream = null$/;"	v
oisST	broadcast/MultiTracker.scala	/^    val oisST = new ObjectInputStream(socket.getInputStream)$/;"	V
oisSource	broadcast/BitTorrentBroadcast.scala	/^      private var oisSource: ObjectInputStream = null$/;"	v
oisSource	broadcast/TreeBroadcast.scala	/^    var oisSource: ObjectInputStream = null$/;"	v
oisTracker	broadcast/MultiTracker.scala	/^    var oisTracker: ObjectInputStream = null$/;"	v
old	partial/GroupedMeanEvaluator.scala	/^      val old = sums.get(entry.getKey)$/;"	V
old	partial/GroupedSumEvaluator.scala	/^      val old = sums.get(entry.getKey)$/;"	V
old	rdd/PairRDDFunctions.scala	/^        val old = m1.get(k)$/;"	V
old	rdd/PairRDDFunctions.scala	/^        val old = map.get(k)$/;"	V
old	scheduler/ResultTask.scala	/^      val old = serializedInfoCache.get(stageId).orNull$/;"	V
old	scheduler/ShuffleMapTask.scala	/^      val old = serializedInfoCache.get(stageId).orNull$/;"	V
old	storage/DiskBlockManager.scala	/^        val old = subDirs(dirId)(subDirId)$/;"	V
oldBlockOpt	storage/BlockManager.scala	/^      val oldBlockOpt = blockInfo.putIfAbsent(blockId, tinfo)$/;"	V
oldClassLoader	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^    val oldClassLoader = Thread.currentThread.getContextClassLoader$/;"	V
oldClassLoader	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^    val oldClassLoader = setClassLoader()$/;"	V
oldClassLoader	serializer/KryoSerializer.scala	/^    val oldClassLoader = kryo.getClassLoader$/;"	V
oldPos	util/AppendOnlyMap.scala	/^    var oldPos = 0$/;"	v
onBlockMessageReceive	storage/BlockManagerWorker.scala	/^  def onBlockMessageReceive(msg: Message, id: ConnectionManagerId): Option[Message] = {$/;"	m
onClose	network/Connection.scala	/^  def onClose(callback: Connection => Unit) {$/;"	m
onCloseCallback	network/Connection.scala	/^  var onCloseCallback: Connection => Unit = null$/;"	v
onComplete	FutureAction.scala	/^  def onComplete[U](func: (Try[T]) => U)(implicit executor: ExecutionContext)$/;"	m
onComplete	partial/PartialResult.scala	/^  def onComplete(handler: R => Unit): PartialResult[R] = synchronized {$/;"	m
onCompleteCallbacks	TaskContext.scala	/^  @transient private val onCompleteCallbacks = new ArrayBuffer[() => Unit]$/;"	V
onDisk	storage/BlockManager.scala	/^          val onDisk = level.useDisk && diskStore.contains(blockId)$/;"	V
onException	network/Connection.scala	/^  def onException(callback: (Connection, Exception) => Unit) {$/;"	m
onExceptionCallback	network/Connection.scala	/^  var onExceptionCallback: (Connection, Exception) => Unit = null$/;"	v
onFail	partial/PartialResult.scala	/^  def onFail(handler: Exception => Unit) {$/;"	m
onJobEnd	scheduler/SparkListener.scala	/^  def onJobEnd(jobEnd: SparkListenerJobEnd) { }$/;"	m
onJobStart	scheduler/SparkListener.scala	/^  def onJobStart(jobStart: SparkListenerJobStart) { }$/;"	m
onKeyInterestChange	network/Connection.scala	/^  def onKeyInterestChange(callback: (Connection, Int) => Unit) {$/;"	m
onKeyInterestChangeCallback	network/Connection.scala	/^  var onKeyInterestChangeCallback: (Connection, Int) => Unit = null$/;"	v
onReceive	network/Connection.scala	/^  def onReceive(callback: (Connection, Message) => Unit) {onReceiveCallback = callback}$/;"	m
onReceiveCallback	network/Connection.scala	/^  var onReceiveCallback: (Connection , Message) => Unit = null$/;"	v
onReceiveCallback	network/ConnectionManager.scala	/^  private var onReceiveCallback: (BufferMessage, ConnectionManagerId) => Option[Message]= null$/;"	v
onReceiveMessage	network/ConnectionManager.scala	/^  def onReceiveMessage(callback: (Message, ConnectionManagerId) => Option[Message]) {$/;"	m
onStageCompleted	scheduler/SparkListener.scala	/^  def onStageCompleted(stageCompleted: StageCompleted) { }$/;"	m
onStageSubmitted	scheduler/SparkListener.scala	/^  def onStageSubmitted(stageSubmitted: SparkListenerStageSubmitted) { }$/;"	m
onTaskEnd	scheduler/SparkListener.scala	/^  def onTaskEnd(taskEnd: SparkListenerTaskEnd) { }$/;"	m
onTaskGettingResult	scheduler/SparkListener.scala	/^ def onTaskGettingResult(taskGettingResult: SparkListenerTaskGettingResult) { }$/;"	m
onTaskStart	scheduler/SparkListener.scala	/^  def onTaskStart(taskStart: SparkListenerTaskStart) { }$/;"	m
ones	util/Vector.scala	/^  def ones(length: Int) = Vector(length, _ => 1)$/;"	m
oos	broadcast/BitTorrentBroadcast.scala	/^      private val oos = new ObjectOutputStream(clientSocket.getOutputStream)$/;"	V
oos	broadcast/MultiTracker.scala	/^                  val oos = new ObjectOutputStream(clientSocket.getOutputStream)$/;"	V
oos	broadcast/MultiTracker.scala	/^    val oos = new ObjectOutputStream(baos)$/;"	V
oos	broadcast/TreeBroadcast.scala	/^      private val oos = new ObjectOutputStream(clientSocket.getOutputStream)$/;"	V
oos	util/Utils.scala	/^    val oos = new ObjectOutputStream(bos)$/;"	V
oosDriver	broadcast/TreeBroadcast.scala	/^    var oosDriver: ObjectOutputStream = null$/;"	v
oosGuide	broadcast/BitTorrentBroadcast.scala	/^      var oosGuide: ObjectOutputStream = null$/;"	v
oosST	broadcast/MultiTracker.scala	/^    val oosST = new ObjectOutputStream(socket.getOutputStream)$/;"	V
oosSource	broadcast/BitTorrentBroadcast.scala	/^      private var oosSource: ObjectOutputStream = null$/;"	v
oosSource	broadcast/TreeBroadcast.scala	/^    var oosSource: ObjectOutputStream = null$/;"	v
oosTracker	broadcast/MultiTracker.scala	/^    var oosTracker: ObjectOutputStream = null$/;"	v
opStrs	network/ConnectionManager.scala	/^                    val opStrs = ArrayBuffer[String]()$/;"	V
open	storage/BlockObjectWriter.scala	/^  def open(): BlockObjectWriter$/;"	m
optionToOptional	api/java/JavaUtils.scala	/^  def optionToOptional[T](option: Option[T]): Optional[T] =$/;"	m
org.apache	package.scala	/^package org.apache$/;"	p
org.apache.hadoop.conf.Configuration	SerializableWritable.scala	/^import org.apache.hadoop.conf.Configuration$/;"	i
org.apache.hadoop.conf.Configuration	SparkContext.scala	/^import org.apache.hadoop.conf.Configuration$/;"	i
org.apache.hadoop.conf.Configuration	api/java/JavaPairRDD.scala	/^import org.apache.hadoop.conf.Configuration$/;"	i
org.apache.hadoop.conf.Configuration	api/java/JavaSparkContext.scala	/^import org.apache.hadoop.conf.Configuration$/;"	i
org.apache.hadoop.conf.Configuration	deploy/SparkHadoopUtil.scala	/^import org.apache.hadoop.conf.Configuration$/;"	i
org.apache.hadoop.conf.Configuration	rdd/CheckpointRDD.scala	/^import org.apache.hadoop.conf.Configuration$/;"	i
org.apache.hadoop.conf.Configuration	rdd/PairRDDFunctions.scala	/^import org.apache.hadoop.conf.Configuration$/;"	i
org.apache.hadoop.conf.Configuration	rdd/RDDCheckpointData.scala	/^import org.apache.hadoop.conf.Configuration$/;"	i
org.apache.hadoop.conf.Configuration	scheduler/InputFormatInfo.scala	/^import org.apache.hadoop.conf.Configuration$/;"	i
org.apache.hadoop.conf.Configuration	scheduler/cluster/SimrSchedulerBackend.scala	/^import org.apache.hadoop.conf.Configuration$/;"	i
org.apache.hadoop.conf.{Configurable, Configuration}	rdd/NewHadoopRDD.scala	/^import org.apache.hadoop.conf.{Configurable, Configuration}$/;"	i
org.apache.hadoop.conf.{Configuration, Configurable}	rdd/HadoopRDD.scala	/^import org.apache.hadoop.conf.{Configuration, Configurable}$/;"	i
org.apache.hadoop.fs.FileSystem	SparkHadoopWriter.scala	/^import org.apache.hadoop.fs.FileSystem$/;"	i
org.apache.hadoop.fs.FileSystem	executor/ExecutorSource.scala	/^import org.apache.hadoop.fs.FileSystem$/;"	i
org.apache.hadoop.fs.Path	SparkContext.scala	/^import org.apache.hadoop.fs.Path$/;"	i
org.apache.hadoop.fs.Path	SparkHadoopWriter.scala	/^import org.apache.hadoop.fs.Path$/;"	i
org.apache.hadoop.fs.Path	rdd/CheckpointRDD.scala	/^import org.apache.hadoop.fs.Path$/;"	i
org.apache.hadoop.fs.Path	rdd/PairRDDFunctions.scala	/^import org.apache.hadoop.fs.Path$/;"	i
org.apache.hadoop.fs.Path	rdd/RDDCheckpointData.scala	/^import org.apache.hadoop.fs.Path$/;"	i
org.apache.hadoop.fs.{Path, FileSystem, FileUtil}	util/Utils.scala	/^import org.apache.hadoop.fs.{Path, FileSystem, FileUtil}$/;"	i
org.apache.hadoop.fs.{Path, FileSystem}	scheduler/cluster/SimrSchedulerBackend.scala	/^import org.apache.hadoop.fs.{Path, FileSystem}$/;"	i
org.apache.hadoop.io.ArrayWritable	SparkContext.scala	/^import org.apache.hadoop.io.ArrayWritable$/;"	i
org.apache.hadoop.io.BooleanWritable	SparkContext.scala	/^import org.apache.hadoop.io.BooleanWritable$/;"	i
org.apache.hadoop.io.BytesWritable	SparkContext.scala	/^import org.apache.hadoop.io.BytesWritable$/;"	i
org.apache.hadoop.io.BytesWritable	rdd/RDD.scala	/^import org.apache.hadoop.io.BytesWritable$/;"	i
org.apache.hadoop.io.DoubleWritable	SparkContext.scala	/^import org.apache.hadoop.io.DoubleWritable$/;"	i
org.apache.hadoop.io.FloatWritable	SparkContext.scala	/^import org.apache.hadoop.io.FloatWritable$/;"	i
org.apache.hadoop.io.IntWritable	SparkContext.scala	/^import org.apache.hadoop.io.IntWritable$/;"	i
org.apache.hadoop.io.LongWritable	SparkContext.scala	/^import org.apache.hadoop.io.LongWritable$/;"	i
org.apache.hadoop.io.NullWritable	SparkContext.scala	/^import org.apache.hadoop.io.NullWritable$/;"	i
org.apache.hadoop.io.NullWritable	rdd/RDD.scala	/^import org.apache.hadoop.io.NullWritable$/;"	i
org.apache.hadoop.io.ObjectWritable	SerializableWritable.scala	/^import org.apache.hadoop.io.ObjectWritable$/;"	i
org.apache.hadoop.io.SequenceFile.CompressionType	rdd/PairRDDFunctions.scala	/^import org.apache.hadoop.io.SequenceFile.CompressionType$/;"	i
org.apache.hadoop.io.Text	SparkContext.scala	/^import org.apache.hadoop.io.Text$/;"	i
org.apache.hadoop.io.Text	rdd/RDD.scala	/^import org.apache.hadoop.io.Text$/;"	i
org.apache.hadoop.io.Writable	SerializableWritable.scala	/^import org.apache.hadoop.io.Writable$/;"	i
org.apache.hadoop.io.Writable	SparkContext.scala	/^import org.apache.hadoop.io.Writable$/;"	i
org.apache.hadoop.io.Writable	rdd/NewHadoopRDD.scala	/^import org.apache.hadoop.io.Writable$/;"	i
org.apache.hadoop.io.Writable	rdd/SequenceFileRDDFunctions.scala	/^import org.apache.hadoop.io.Writable$/;"	i
org.apache.hadoop.io.compress.CompressionCodec	api/java/JavaPairRDD.scala	/^import org.apache.hadoop.io.compress.CompressionCodec$/;"	i
org.apache.hadoop.io.compress.CompressionCodec	api/java/JavaRDDLike.scala	/^import org.apache.hadoop.io.compress.CompressionCodec$/;"	i
org.apache.hadoop.io.compress.CompressionCodec	rdd/PairRDDFunctions.scala	/^import org.apache.hadoop.io.compress.CompressionCodec$/;"	i
org.apache.hadoop.io.compress.CompressionCodec	rdd/RDD.scala	/^import org.apache.hadoop.io.compress.CompressionCodec$/;"	i
org.apache.hadoop.io.compress.CompressionCodec	rdd/SequenceFileRDDFunctions.scala	/^import org.apache.hadoop.io.compress.CompressionCodec$/;"	i
org.apache.hadoop.io.{NullWritable, BytesWritable}	rdd/CheckpointRDD.scala	/^import org.apache.hadoop.io.{NullWritable, BytesWritable}$/;"	i
org.apache.hadoop.mapred	SparkHadoopWriter.scala	/^package org.apache.hadoop.mapred$/;"	p
org.apache.hadoop.mapred.FileInputFormat	SparkContext.scala	/^import org.apache.hadoop.mapred.FileInputFormat$/;"	i
org.apache.hadoop.mapred.FileInputFormat	rdd/HadoopRDD.scala	/^import org.apache.hadoop.mapred.FileInputFormat$/;"	i
org.apache.hadoop.mapred.FileOutputFormat	rdd/PairRDDFunctions.scala	/^import org.apache.hadoop.mapred.FileOutputFormat$/;"	i
org.apache.hadoop.mapred.InputFormat	SparkContext.scala	/^import org.apache.hadoop.mapred.InputFormat$/;"	i
org.apache.hadoop.mapred.InputFormat	api/java/JavaSparkContext.scala	/^import org.apache.hadoop.mapred.InputFormat$/;"	i
org.apache.hadoop.mapred.InputFormat	rdd/HadoopRDD.scala	/^import org.apache.hadoop.mapred.InputFormat$/;"	i
org.apache.hadoop.mapred.InputSplit	rdd/HadoopRDD.scala	/^import org.apache.hadoop.mapred.InputSplit$/;"	i
org.apache.hadoop.mapred.JobConf	SparkContext.scala	/^import org.apache.hadoop.mapred.JobConf$/;"	i
org.apache.hadoop.mapred.JobConf	api/java/JavaPairRDD.scala	/^import org.apache.hadoop.mapred.JobConf$/;"	i
org.apache.hadoop.mapred.JobConf	api/java/JavaSparkContext.scala	/^import org.apache.hadoop.mapred.JobConf$/;"	i
org.apache.hadoop.mapred.JobConf	deploy/SparkHadoopUtil.scala	/^import org.apache.hadoop.mapred.JobConf$/;"	i
org.apache.hadoop.mapred.JobConf	rdd/HadoopRDD.scala	/^import org.apache.hadoop.mapred.JobConf$/;"	i
org.apache.hadoop.mapred.JobConf	rdd/SequenceFileRDDFunctions.scala	/^import org.apache.hadoop.mapred.JobConf$/;"	i
org.apache.hadoop.mapred.OutputFormat	api/java/JavaPairRDD.scala	/^import org.apache.hadoop.mapred.OutputFormat$/;"	i
org.apache.hadoop.mapred.OutputFormat	rdd/PairRDDFunctions.scala	/^import org.apache.hadoop.mapred.OutputFormat$/;"	i
org.apache.hadoop.mapred.RecordReader	rdd/HadoopRDD.scala	/^import org.apache.hadoop.mapred.RecordReader$/;"	i
org.apache.hadoop.mapred.Reporter	rdd/HadoopRDD.scala	/^import org.apache.hadoop.mapred.Reporter$/;"	i
org.apache.hadoop.mapred.SequenceFileInputFormat	SparkContext.scala	/^import org.apache.hadoop.mapred.SequenceFileInputFormat$/;"	i
org.apache.hadoop.mapred.SequenceFileOutputFormat	rdd/SequenceFileRDDFunctions.scala	/^import org.apache.hadoop.mapred.SequenceFileOutputFormat$/;"	i
org.apache.hadoop.mapred.TextInputFormat	SparkContext.scala	/^import org.apache.hadoop.mapred.TextInputFormat$/;"	i
org.apache.hadoop.mapred.TextOutputFormat	rdd/RDD.scala	/^import org.apache.hadoop.mapred.TextOutputFormat$/;"	i
org.apache.hadoop.mapred._	rdd/PairRDDFunctions.scala	/^import org.apache.hadoop.mapred._$/;"	i
org.apache.hadoop.mapred.{FileInputFormat, JobConf}	scheduler/InputFormatInfo.scala	/^import org.apache.hadoop.mapred.{FileInputFormat, JobConf}$/;"	i
org.apache.hadoop.mapred.{FileInputFormat, SequenceFileInputFormat, JobConf, Reporter}	rdd/CheckpointRDD.scala	/^import org.apache.hadoop.mapred.{FileInputFormat, SequenceFileInputFormat, JobConf, Reporter}$/;"	i
org.apache.hadoop.mapreduce.Job	scheduler/InputFormatInfo.scala	/^import org.apache.hadoop.mapreduce.Job$/;"	i
org.apache.hadoop.mapreduce.SparkHadoopMapReduceUtil	rdd/PairRDDFunctions.scala	/^import org.apache.hadoop.mapreduce.SparkHadoopMapReduceUtil$/;"	i
org.apache.hadoop.mapreduce._	rdd/NewHadoopRDD.scala	/^import org.apache.hadoop.mapreduce._$/;"	i
org.apache.hadoop.mapreduce.lib.input.{FileInputFormat => NewFileInputFormat}	SparkContext.scala	/^import org.apache.hadoop.mapreduce.lib.input.{FileInputFormat => NewFileInputFormat}$/;"	i
org.apache.hadoop.mapreduce.lib.output.{FileOutputFormat => NewFileOutputFormat}	rdd/PairRDDFunctions.scala	/^import org.apache.hadoop.mapreduce.lib.output.{FileOutputFormat => NewFileOutputFormat}$/;"	i
org.apache.hadoop.mapreduce.{InputFormat => NewInputFormat}	SparkContext.scala	/^import org.apache.hadoop.mapreduce.{InputFormat => NewInputFormat}$/;"	i
org.apache.hadoop.mapreduce.{InputFormat => NewInputFormat}	api/java/JavaSparkContext.scala	/^import org.apache.hadoop.mapreduce.{InputFormat => NewInputFormat}$/;"	i
org.apache.hadoop.mapreduce.{Job => NewAPIHadoopJob}	rdd/PairRDDFunctions.scala	/^import org.apache.hadoop.mapreduce.{Job => NewAPIHadoopJob}$/;"	i
org.apache.hadoop.mapreduce.{Job => NewHadoopJob}	SparkContext.scala	/^import org.apache.hadoop.mapreduce.{Job => NewHadoopJob}$/;"	i
org.apache.hadoop.mapreduce.{OutputFormat => NewOutputFormat}	api/java/JavaPairRDD.scala	/^import org.apache.hadoop.mapreduce.{OutputFormat => NewOutputFormat}$/;"	i
org.apache.hadoop.mapreduce.{OutputFormat => NewOutputFormat}	rdd/PairRDDFunctions.scala	/^import org.apache.hadoop.mapreduce.{OutputFormat => NewOutputFormat}$/;"	i
org.apache.hadoop.mapreduce.{RecordWriter => NewRecordWriter}	rdd/PairRDDFunctions.scala	/^import org.apache.hadoop.mapreduce.{RecordWriter => NewRecordWriter}$/;"	i
org.apache.hadoop.security.UserGroupInformation	deploy/SparkHadoopUtil.scala	/^import org.apache.hadoop.security.UserGroupInformation$/;"	i
org.apache.hadoop.security.UserGroupInformation	scheduler/InputFormatInfo.scala	/^import org.apache.hadoop.security.UserGroupInformation$/;"	i
org.apache.hadoop.util.ReflectionUtils	rdd/CheckpointRDD.scala	/^import org.apache.hadoop.util.ReflectionUtils$/;"	i
org.apache.hadoop.util.ReflectionUtils	rdd/HadoopRDD.scala	/^import org.apache.hadoop.util.ReflectionUtils$/;"	i
org.apache.hadoop.util.ReflectionUtils	scheduler/InputFormatInfo.scala	/^import org.apache.hadoop.util.ReflectionUtils$/;"	i
org.apache.mesos.MesosNativeLibrary	SparkContext.scala	/^import org.apache.mesos.MesosNativeLibrary$/;"	i
org.apache.mesos.Protos.{TaskInfo => MesosTaskInfo, TaskState => MesosTaskState, _}	scheduler/cluster/mesos/CoarseMesosSchedulerBackend.scala	/^import org.apache.mesos.Protos.{TaskInfo => MesosTaskInfo, TaskState => MesosTaskState, _}$/;"	i
org.apache.mesos.Protos.{TaskInfo => MesosTaskInfo, TaskState => MesosTaskState, _}	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^import org.apache.mesos.Protos.{TaskInfo => MesosTaskInfo, TaskState => MesosTaskState, _}$/;"	i
org.apache.mesos.Protos.{TaskState => MesosTaskState}	TaskState.scala	/^import org.apache.mesos.Protos.{TaskState => MesosTaskState}$/;"	i
org.apache.mesos.Protos.{TaskStatus => MesosTaskStatus, _}	executor/MesosExecutorBackend.scala	/^import org.apache.mesos.Protos.{TaskStatus => MesosTaskStatus, _}$/;"	i
org.apache.mesos._	scheduler/cluster/mesos/CoarseMesosSchedulerBackend.scala	/^import org.apache.mesos._$/;"	i
org.apache.mesos._	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^import org.apache.mesos._$/;"	i
org.apache.mesos.{Executor => MesosExecutor, MesosExecutorDriver, MesosNativeLibrary, ExecutorDriver}	executor/MesosExecutorBackend.scala	/^import org.apache.mesos.{Executor => MesosExecutor, MesosExecutorDriver, MesosNativeLibrary, ExecutorDriver}$/;"	i
org.apache.mesos.{Scheduler => MScheduler}	scheduler/cluster/mesos/CoarseMesosSchedulerBackend.scala	/^import org.apache.mesos.{Scheduler => MScheduler}$/;"	i
org.apache.mesos.{Scheduler => MScheduler}	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^import org.apache.mesos.{Scheduler => MScheduler}$/;"	i
org.apache.spark	Accumulators.scala	/^package org.apache.spark$/;"	p
org.apache.spark	Aggregator.scala	/^package org.apache.spark$/;"	p
org.apache.spark	BlockStoreShuffleFetcher.scala	/^package org.apache.spark$/;"	p
org.apache.spark	CacheManager.scala	/^package org.apache.spark$/;"	p
org.apache.spark	Dependency.scala	/^package org.apache.spark$/;"	p
org.apache.spark	FetchFailedException.scala	/^package org.apache.spark$/;"	p
org.apache.spark	FutureAction.scala	/^package org.apache.spark$/;"	p
org.apache.spark	HttpFileServer.scala	/^package org.apache.spark$/;"	p
org.apache.spark	HttpServer.scala	/^package org.apache.spark$/;"	p
org.apache.spark	InterruptibleIterator.scala	/^package org.apache.spark$/;"	p
org.apache.spark	Logging.scala	/^package org.apache.spark$/;"	p
org.apache.spark	MapOutputTracker.scala	/^package org.apache.spark$/;"	p
org.apache.spark	Partition.scala	/^package org.apache.spark$/;"	p
org.apache.spark	Partitioner.scala	/^package org.apache.spark$/;"	p
org.apache.spark	SerializableWritable.scala	/^package org.apache.spark$/;"	p
org.apache.spark	ShuffleFetcher.scala	/^package org.apache.spark$/;"	p
org.apache.spark	SparkContext.scala	/^package org.apache.spark$/;"	p
org.apache.spark	SparkEnv.scala	/^package org.apache.spark$/;"	p
org.apache.spark	SparkException.scala	/^package org.apache.spark$/;"	p
org.apache.spark	SparkFiles.java	/^package org.apache.spark;$/;"	p
org.apache.spark	TaskContext.scala	/^package org.apache.spark$/;"	p
org.apache.spark	TaskEndReason.scala	/^package org.apache.spark$/;"	p
org.apache.spark	TaskState.scala	/^package org.apache.spark$/;"	p
org.apache.spark.Aggregator	rdd/PairRDDFunctions.scala	/^import org.apache.spark.Aggregator$/;"	i
org.apache.spark.Dependency	rdd/SubtractedRDD.scala	/^import org.apache.spark.Dependency$/;"	i
org.apache.spark.HashPartitioner	api/java/JavaPairRDD.scala	/^import org.apache.spark.HashPartitioner$/;"	i
org.apache.spark.Logging	SparkHadoopWriter.scala	/^import org.apache.spark.Logging$/;"	i
org.apache.spark.Logging	deploy/LocalSparkCluster.scala	/^import org.apache.spark.Logging$/;"	i
org.apache.spark.Logging	deploy/client/Client.scala	/^import org.apache.spark.Logging$/;"	i
org.apache.spark.Logging	deploy/master/FileSystemPersistenceEngine.scala	/^import org.apache.spark.Logging$/;"	i
org.apache.spark.Logging	deploy/master/SparkZooKeeperSession.scala	/^import org.apache.spark.Logging$/;"	i
org.apache.spark.Logging	deploy/master/ZooKeeperLeaderElectionAgent.scala	/^import org.apache.spark.Logging$/;"	i
org.apache.spark.Logging	deploy/master/ZooKeeperPersistenceEngine.scala	/^import org.apache.spark.Logging$/;"	i
org.apache.spark.Logging	deploy/worker/Worker.scala	/^import org.apache.spark.Logging$/;"	i
org.apache.spark.Logging	executor/CoarseGrainedExecutorBackend.scala	/^import org.apache.spark.Logging$/;"	i
org.apache.spark.Logging	executor/MesosExecutorBackend.scala	/^import org.apache.spark.Logging$/;"	i
org.apache.spark.Logging	metrics/MetricsConfig.scala	/^import org.apache.spark.Logging$/;"	i
org.apache.spark.Logging	metrics/MetricsSystem.scala	/^import org.apache.spark.Logging$/;"	i
org.apache.spark.Logging	network/netty/FileHeader.scala	/^import org.apache.spark.Logging$/;"	i
org.apache.spark.Logging	network/netty/ShuffleCopier.scala	/^import org.apache.spark.Logging$/;"	i
org.apache.spark.Logging	network/netty/ShuffleSender.scala	/^import org.apache.spark.Logging$/;"	i
org.apache.spark.Logging	rdd/SequenceFileRDDFunctions.scala	/^import org.apache.spark.Logging$/;"	i
org.apache.spark.Logging	scheduler/Pool.scala	/^import org.apache.spark.Logging$/;"	i
org.apache.spark.Logging	scheduler/SchedulableBuilder.scala	/^import org.apache.spark.Logging$/;"	i
org.apache.spark.Logging	scheduler/SparkListenerBus.scala	/^import org.apache.spark.Logging$/;"	i
org.apache.spark.Logging	storage/BlockFetcherIterator.scala	/^import org.apache.spark.Logging$/;"	i
org.apache.spark.Logging	storage/BlockObjectWriter.scala	/^import org.apache.spark.Logging$/;"	i
org.apache.spark.Logging	storage/BlockStore.scala	/^import org.apache.spark.Logging$/;"	i
org.apache.spark.Logging	storage/DiskBlockManager.scala	/^import org.apache.spark.Logging$/;"	i
org.apache.spark.Logging	storage/DiskStore.scala	/^import org.apache.spark.Logging$/;"	i
org.apache.spark.Logging	ui/JettyUtils.scala	/^import org.apache.spark.Logging$/;"	i
org.apache.spark.Logging	util/ClosureCleaner.scala	/^import org.apache.spark.Logging$/;"	i
org.apache.spark.Logging	util/MetadataCleaner.scala	/^import org.apache.spark.Logging$/;"	i
org.apache.spark.Logging	util/SizeEstimator.scala	/^import org.apache.spark.Logging$/;"	i
org.apache.spark.Logging	util/TimeStampedHashMap.scala	/^import org.apache.spark.Logging$/;"	i
org.apache.spark.OneToOneDependency	rdd/SubtractedRDD.scala	/^import org.apache.spark.OneToOneDependency$/;"	i
org.apache.spark.Partition	rdd/SubtractedRDD.scala	/^import org.apache.spark.Partition$/;"	i
org.apache.spark.Partitioner	api/java/JavaDoubleRDD.scala	/^import org.apache.spark.Partitioner$/;"	i
org.apache.spark.Partitioner	api/java/JavaPairRDD.scala	/^import org.apache.spark.Partitioner$/;"	i
org.apache.spark.Partitioner	api/python/PythonPartitioner.scala	/^import org.apache.spark.Partitioner$/;"	i
org.apache.spark.Partitioner	rdd/PairRDDFunctions.scala	/^import org.apache.spark.Partitioner$/;"	i
org.apache.spark.Partitioner	rdd/SubtractedRDD.scala	/^import org.apache.spark.Partitioner$/;"	i
org.apache.spark.Partitioner._	api/java/JavaPairRDD.scala	/^import org.apache.spark.Partitioner._$/;"	i
org.apache.spark.Partitioner._	rdd/RDD.scala	/^import org.apache.spark.Partitioner._$/;"	i
org.apache.spark.Partitioner.defaultPartitioner	rdd/PairRDDFunctions.scala	/^import org.apache.spark.Partitioner.defaultPartitioner$/;"	i
org.apache.spark.SerializableWritable	SparkHadoopWriter.scala	/^import org.apache.spark.SerializableWritable$/;"	i
org.apache.spark.ShuffleDependency	rdd/SubtractedRDD.scala	/^import org.apache.spark.ShuffleDependency$/;"	i
org.apache.spark.SparkContext	scheduler/DAGSchedulerSource.scala	/^import org.apache.spark.SparkContext$/;"	i
org.apache.spark.SparkContext	scheduler/cluster/SchedulerBackend.scala	/^import org.apache.spark.SparkContext$/;"	i
org.apache.spark.SparkContext	storage/BlockManagerSource.scala	/^import org.apache.spark.SparkContext$/;"	i
org.apache.spark.SparkContext	storage/StoragePerfTester.scala	/^import org.apache.spark.SparkContext$/;"	i
org.apache.spark.SparkContext	ui/UIUtils.scala	/^import org.apache.spark.SparkContext$/;"	i
org.apache.spark.SparkContext	ui/UIWorkloadGenerator.scala	/^import org.apache.spark.SparkContext$/;"	i
org.apache.spark.SparkContext	ui/env/EnvironmentUI.scala	/^import org.apache.spark.SparkContext$/;"	i
org.apache.spark.SparkContext.DoubleAccumulatorParam	api/java/JavaSparkContext.scala	/^import org.apache.spark.SparkContext.DoubleAccumulatorParam$/;"	i
org.apache.spark.SparkContext.IntAccumulatorParam	api/java/JavaSparkContext.scala	/^import org.apache.spark.SparkContext.IntAccumulatorParam$/;"	i
org.apache.spark.SparkContext._	network/ConnectionManagerTest.scala	/^import org.apache.spark.SparkContext._$/;"	i
org.apache.spark.SparkContext._	rdd/PairRDDFunctions.scala	/^import org.apache.spark.SparkContext._$/;"	i
org.apache.spark.SparkContext._	rdd/RDD.scala	/^import org.apache.spark.SparkContext._$/;"	i
org.apache.spark.SparkContext._	rdd/SequenceFileRDDFunctions.scala	/^import org.apache.spark.SparkContext._$/;"	i
org.apache.spark.SparkContext._	ui/UIWorkloadGenerator.scala	/^import org.apache.spark.SparkContext._$/;"	i
org.apache.spark.SparkContext.doubleRDDToDoubleRDDFunctions	api/java/JavaDoubleRDD.scala	/^import org.apache.spark.SparkContext.doubleRDDToDoubleRDDFunctions$/;"	i
org.apache.spark.SparkContext.rddToPairRDDFunctions	api/java/JavaPairRDD.scala	/^import org.apache.spark.SparkContext.rddToPairRDDFunctions$/;"	i
org.apache.spark.SparkEnv	rdd/SubtractedRDD.scala	/^import org.apache.spark.SparkEnv$/;"	i
org.apache.spark.SparkException	storage/BlockFetcherIterator.scala	/^import org.apache.spark.SparkException$/;"	i
org.apache.spark.TaskContext	rdd/SubtractedRDD.scala	/^import org.apache.spark.TaskContext$/;"	i
org.apache.spark.TaskContext	scheduler/ActiveJob.scala	/^import org.apache.spark.TaskContext$/;"	i
org.apache.spark.TaskContext	scheduler/Task.scala	/^import org.apache.spark.TaskContext$/;"	i
org.apache.spark.TaskState	executor/MesosExecutorBackend.scala	/^import org.apache.spark.TaskState$/;"	i
org.apache.spark.TaskState.TaskState	executor/CoarseGrainedExecutorBackend.scala	/^import org.apache.spark.TaskState.TaskState$/;"	i
org.apache.spark.TaskState.TaskState	executor/ExecutorBackend.scala	/^import org.apache.spark.TaskState.TaskState$/;"	i
org.apache.spark.TaskState.TaskState	executor/MesosExecutorBackend.scala	/^import org.apache.spark.TaskState.TaskState$/;"	i
org.apache.spark.TaskState.TaskState	scheduler/TaskSetManager.scala	/^import org.apache.spark.TaskState.TaskState$/;"	i
org.apache.spark.TaskState.TaskState	scheduler/cluster/ClusterScheduler.scala	/^import org.apache.spark.TaskState.TaskState$/;"	i
org.apache.spark.TaskState.TaskState	scheduler/cluster/ClusterTaskSetManager.scala	/^import org.apache.spark.TaskState.TaskState$/;"	i
org.apache.spark.TaskState.TaskState	scheduler/cluster/CoarseGrainedClusterMessage.scala	/^import org.apache.spark.TaskState.TaskState$/;"	i
org.apache.spark.TaskState.TaskState	scheduler/cluster/TaskResultGetter.scala	/^import org.apache.spark.TaskState.TaskState$/;"	i
org.apache.spark.TaskState.TaskState	scheduler/local/LocalScheduler.scala	/^import org.apache.spark.TaskState.TaskState$/;"	i
org.apache.spark.TaskState.TaskState	scheduler/local/LocalTaskSetManager.scala	/^import org.apache.spark.TaskState.TaskState$/;"	i
org.apache.spark._	api/java/JavaRDD.scala	/^import org.apache.spark._$/;"	i
org.apache.spark._	api/python/PythonRDD.scala	/^import org.apache.spark._$/;"	i
org.apache.spark._	api/python/PythonWorkerFactory.scala	/^import org.apache.spark._$/;"	i
org.apache.spark._	broadcast/BitTorrentBroadcast.scala	/^import org.apache.spark._$/;"	i
org.apache.spark._	broadcast/Broadcast.scala	/^import org.apache.spark._$/;"	i
org.apache.spark._	broadcast/MultiTracker.scala	/^import org.apache.spark._$/;"	i
org.apache.spark._	broadcast/SourceInfo.scala	/^import org.apache.spark._$/;"	i
org.apache.spark._	broadcast/TorrentBroadcast.scala	/^import org.apache.spark._$/;"	i
org.apache.spark._	broadcast/TreeBroadcast.scala	/^import org.apache.spark._$/;"	i
org.apache.spark._	executor/Executor.scala	/^import org.apache.spark._$/;"	i
org.apache.spark._	network/Connection.scala	/^import org.apache.spark._$/;"	i
org.apache.spark._	network/ConnectionManager.scala	/^import org.apache.spark._$/;"	i
org.apache.spark._	network/ConnectionManagerTest.scala	/^import org.apache.spark._$/;"	i
org.apache.spark._	partial/ApproximateActionListener.scala	/^import org.apache.spark._$/;"	i
org.apache.spark._	rdd/CartesianRDD.scala	/^import org.apache.spark._$/;"	i
org.apache.spark._	rdd/CheckpointRDD.scala	/^    import org.apache.spark._$/;"	i
org.apache.spark._	rdd/CheckpointRDD.scala	/^import org.apache.spark._$/;"	i
org.apache.spark._	rdd/CoalescedRDD.scala	/^import org.apache.spark._$/;"	i
org.apache.spark._	rdd/HadoopRDD.scala	/^import org.apache.spark._$/;"	i
org.apache.spark._	rdd/PairRDDFunctions.scala	/^import org.apache.spark._$/;"	i
org.apache.spark._	rdd/ParallelCollectionRDD.scala	/^import org.apache.spark._$/;"	i
org.apache.spark._	rdd/RDD.scala	/^import org.apache.spark._$/;"	i
org.apache.spark._	scheduler/DAGScheduler.scala	/^import org.apache.spark._$/;"	i
org.apache.spark._	scheduler/DAGSchedulerEvent.scala	/^import org.apache.spark._$/;"	i
org.apache.spark._	scheduler/JobLogger.scala	/^import org.apache.spark._$/;"	i
org.apache.spark._	scheduler/ResultTask.scala	/^import org.apache.spark._$/;"	i
org.apache.spark._	scheduler/ShuffleMapTask.scala	/^import org.apache.spark._$/;"	i
org.apache.spark._	scheduler/Stage.scala	/^import org.apache.spark._$/;"	i
org.apache.spark._	scheduler/cluster/ClusterScheduler.scala	/^import org.apache.spark._$/;"	i
org.apache.spark._	scheduler/cluster/TaskResultGetter.scala	/^import org.apache.spark._$/;"	i
org.apache.spark._	scheduler/local/LocalScheduler.scala	/^import org.apache.spark._$/;"	i
org.apache.spark._	storage/BlockMessageArray.scala	/^import org.apache.spark._$/;"	i
org.apache.spark.api.java	api/java/JavaDoubleRDD.scala	/^package org.apache.spark.api.java$/;"	p
org.apache.spark.api.java	api/java/JavaPairRDD.scala	/^package org.apache.spark.api.java$/;"	p
org.apache.spark.api.java	api/java/JavaRDD.scala	/^package org.apache.spark.api.java$/;"	p
org.apache.spark.api.java	api/java/JavaRDDLike.scala	/^package org.apache.spark.api.java$/;"	p
org.apache.spark.api.java	api/java/JavaSparkContext.scala	/^package org.apache.spark.api.java$/;"	p
org.apache.spark.api.java	api/java/JavaSparkContextVarargsWorkaround.java	/^package org.apache.spark.api.java;$/;"	p
org.apache.spark.api.java	api/java/JavaUtils.scala	/^package org.apache.spark.api.java$/;"	p
org.apache.spark.api.java	api/java/StorageLevels.java	/^package org.apache.spark.api.java;$/;"	p
org.apache.spark.api.java.JavaPairRDD._	api/java/JavaRDDLike.scala	/^import org.apache.spark.api.java.JavaPairRDD._$/;"	i
org.apache.spark.api.java.JavaRDD	rdd/RDD.scala	/^import org.apache.spark.api.java.JavaRDD$/;"	i
org.apache.spark.api.java.function	api/java/function/DoubleFlatMapFunction.java	/^package org.apache.spark.api.java.function;$/;"	p
org.apache.spark.api.java.function	api/java/function/DoubleFunction.java	/^package org.apache.spark.api.java.function;$/;"	p
org.apache.spark.api.java.function	api/java/function/FlatMapFunction.scala	/^package org.apache.spark.api.java.function$/;"	p
org.apache.spark.api.java.function	api/java/function/FlatMapFunction2.scala	/^package org.apache.spark.api.java.function$/;"	p
org.apache.spark.api.java.function	api/java/function/Function.java	/^package org.apache.spark.api.java.function;$/;"	p
org.apache.spark.api.java.function	api/java/function/Function2.java	/^package org.apache.spark.api.java.function;$/;"	p
org.apache.spark.api.java.function	api/java/function/Function3.java	/^package org.apache.spark.api.java.function;$/;"	p
org.apache.spark.api.java.function	api/java/function/Function4.java	/^package org.apache.spark.api.java.function;$/;"	p
org.apache.spark.api.java.function	api/java/function/PairFlatMapFunction.java	/^package org.apache.spark.api.java.function;$/;"	p
org.apache.spark.api.java.function	api/java/function/PairFunction.java	/^package org.apache.spark.api.java.function;$/;"	p
org.apache.spark.api.java.function	api/java/function/VoidFunction.scala	/^package org.apache.spark.api.java.function$/;"	p
org.apache.spark.api.java.function	api/java/function/WrappedFunction1.scala	/^package org.apache.spark.api.java.function$/;"	p
org.apache.spark.api.java.function	api/java/function/WrappedFunction2.scala	/^package org.apache.spark.api.java.function$/;"	p
org.apache.spark.api.java.function	api/java/function/WrappedFunction3.scala	/^package org.apache.spark.api.java.function$/;"	p
org.apache.spark.api.java.function	api/java/function/WrappedFunction4.scala	/^package org.apache.spark.api.java.function$/;"	p
org.apache.spark.api.java.function.{Function => JFunction}	api/java/JavaDoubleRDD.scala	/^import org.apache.spark.api.java.function.{Function => JFunction}$/;"	i
org.apache.spark.api.java.function.{Function => JFunction}	api/java/JavaPairRDD.scala	/^import org.apache.spark.api.java.function.{Function => JFunction}$/;"	i
org.apache.spark.api.java.function.{Function => JFunction}	api/java/JavaRDD.scala	/^import org.apache.spark.api.java.function.{Function => JFunction}$/;"	i
org.apache.spark.api.java.function.{Function2 => JFunction2, Function => JFunction, _}	api/java/JavaRDDLike.scala	/^import org.apache.spark.api.java.function.{Function2 => JFunction2, Function => JFunction, _}$/;"	i
org.apache.spark.api.java.function.{Function2 => JFunction2}	api/java/JavaPairRDD.scala	/^import org.apache.spark.api.java.function.{Function2 => JFunction2}$/;"	i
org.apache.spark.api.java.{JavaSparkContext, JavaPairRDD, JavaRDD}	api/python/PythonRDD.scala	/^import org.apache.spark.api.java.{JavaSparkContext, JavaPairRDD, JavaRDD}$/;"	i
org.apache.spark.api.python	api/python/PythonPartitioner.scala	/^package org.apache.spark.api.python$/;"	p
org.apache.spark.api.python	api/python/PythonRDD.scala	/^package org.apache.spark.api.python$/;"	p
org.apache.spark.api.python	api/python/PythonWorkerFactory.scala	/^package org.apache.spark.api.python$/;"	p
org.apache.spark.api.python.PythonWorkerFactory	SparkEnv.scala	/^import org.apache.spark.api.python.PythonWorkerFactory$/;"	i
org.apache.spark.broadcast	broadcast/BitTorrentBroadcast.scala	/^package org.apache.spark.broadcast$/;"	p
org.apache.spark.broadcast	broadcast/Broadcast.scala	/^package org.apache.spark.broadcast$/;"	p
org.apache.spark.broadcast	broadcast/BroadcastFactory.scala	/^package org.apache.spark.broadcast$/;"	p
org.apache.spark.broadcast	broadcast/HttpBroadcast.scala	/^package org.apache.spark.broadcast$/;"	p
org.apache.spark.broadcast	broadcast/MultiTracker.scala	/^package org.apache.spark.broadcast$/;"	p
org.apache.spark.broadcast	broadcast/SourceInfo.scala	/^package org.apache.spark.broadcast$/;"	p
org.apache.spark.broadcast	broadcast/TorrentBroadcast.scala	/^package org.apache.spark.broadcast$/;"	p
org.apache.spark.broadcast	broadcast/TreeBroadcast.scala	/^package org.apache.spark.broadcast$/;"	p
org.apache.spark.broadcast.Broadcast	api/java/JavaSparkContext.scala	/^import org.apache.spark.broadcast.Broadcast$/;"	i
org.apache.spark.broadcast.Broadcast	api/python/PythonRDD.scala	/^import org.apache.spark.broadcast.Broadcast$/;"	i
org.apache.spark.broadcast.Broadcast	rdd/HadoopRDD.scala	/^import org.apache.spark.broadcast.Broadcast$/;"	i
org.apache.spark.broadcast.Broadcast	rdd/PipedRDD.scala	/^import org.apache.spark.broadcast.Broadcast$/;"	i
org.apache.spark.broadcast.BroadcastManager	SparkEnv.scala	/^import org.apache.spark.broadcast.BroadcastManager$/;"	i
org.apache.spark.broadcast.HttpBroadcast	serializer/KryoSerializer.scala	/^import org.apache.spark.broadcast.HttpBroadcast$/;"	i
org.apache.spark.deploy	deploy/ApplicationDescription.scala	/^package org.apache.spark.deploy$/;"	p
org.apache.spark.deploy	deploy/Command.scala	/^package org.apache.spark.deploy$/;"	p
org.apache.spark.deploy	deploy/DeployMessage.scala	/^package org.apache.spark.deploy$/;"	p
org.apache.spark.deploy	deploy/ExecutorDescription.scala	/^package org.apache.spark.deploy$/;"	p
org.apache.spark.deploy	deploy/ExecutorState.scala	/^package org.apache.spark.deploy$/;"	p
org.apache.spark.deploy	deploy/FaultToleranceTest.scala	/^package org.apache.spark.deploy$/;"	p
org.apache.spark.deploy	deploy/JsonProtocol.scala	/^package org.apache.spark.deploy$/;"	p
org.apache.spark.deploy	deploy/LocalSparkCluster.scala	/^package org.apache.spark.deploy$/;"	p
org.apache.spark.deploy	deploy/SparkHadoopUtil.scala	/^package org.apache.spark.deploy$/;"	p
org.apache.spark.deploy	deploy/WebUI.scala	/^package org.apache.spark.deploy$/;"	p
org.apache.spark.deploy.ApplicationDescription	deploy/master/ApplicationInfo.scala	/^import org.apache.spark.deploy.ApplicationDescription$/;"	i
org.apache.spark.deploy.DeployMessages.ExecutorStateChanged	deploy/worker/ExecutorRunner.scala	/^import org.apache.spark.deploy.DeployMessages.ExecutorStateChanged$/;"	i
org.apache.spark.deploy.DeployMessages._	deploy/client/Client.scala	/^import org.apache.spark.deploy.DeployMessages._$/;"	i
org.apache.spark.deploy.DeployMessages._	deploy/master/Master.scala	/^import org.apache.spark.deploy.DeployMessages._$/;"	i
org.apache.spark.deploy.DeployMessages._	deploy/worker/Worker.scala	/^import org.apache.spark.deploy.DeployMessages._$/;"	i
org.apache.spark.deploy.DeployMessages.{MasterStateResponse, RequestMasterState}	deploy/master/ui/ApplicationPage.scala	/^import org.apache.spark.deploy.DeployMessages.{MasterStateResponse, RequestMasterState}$/;"	i
org.apache.spark.deploy.DeployMessages.{MasterStateResponse, RequestMasterState}	deploy/master/ui/IndexPage.scala	/^import org.apache.spark.deploy.DeployMessages.{MasterStateResponse, RequestMasterState}$/;"	i
org.apache.spark.deploy.DeployMessages.{MasterStateResponse, WorkerStateResponse}	deploy/JsonProtocol.scala	/^import org.apache.spark.deploy.DeployMessages.{MasterStateResponse, WorkerStateResponse}$/;"	i
org.apache.spark.deploy.DeployMessages.{RequestWorkerState, WorkerStateResponse}	deploy/worker/ui/IndexPage.scala	/^import org.apache.spark.deploy.DeployMessages.{RequestWorkerState, WorkerStateResponse}$/;"	i
org.apache.spark.deploy.DeployWebUI	deploy/master/ui/IndexPage.scala	/^import org.apache.spark.deploy.DeployWebUI$/;"	i
org.apache.spark.deploy.ExecutorState.ExecutorState	deploy/DeployMessage.scala	/^import org.apache.spark.deploy.ExecutorState.ExecutorState$/;"	i
org.apache.spark.deploy.JsonProtocol	deploy/master/ui/ApplicationPage.scala	/^import org.apache.spark.deploy.JsonProtocol$/;"	i
org.apache.spark.deploy.JsonProtocol	deploy/master/ui/IndexPage.scala	/^import org.apache.spark.deploy.JsonProtocol$/;"	i
org.apache.spark.deploy.JsonProtocol	deploy/worker/ui/IndexPage.scala	/^import org.apache.spark.deploy.JsonProtocol$/;"	i
org.apache.spark.deploy.SparkHadoopUtil	executor/Executor.scala	/^import org.apache.spark.deploy.SparkHadoopUtil$/;"	i
org.apache.spark.deploy.SparkHadoopUtil	rdd/CheckpointRDD.scala	/^import org.apache.spark.deploy.SparkHadoopUtil$/;"	i
org.apache.spark.deploy.SparkHadoopUtil	rdd/HadoopRDD.scala	/^import org.apache.spark.deploy.SparkHadoopUtil$/;"	i
org.apache.spark.deploy.SparkHadoopUtil	scheduler/InputFormatInfo.scala	/^import org.apache.spark.deploy.SparkHadoopUtil$/;"	i
org.apache.spark.deploy.SparkHadoopUtil	util/Utils.scala	/^import org.apache.spark.deploy.SparkHadoopUtil$/;"	i
org.apache.spark.deploy.client	deploy/client/Client.scala	/^package org.apache.spark.deploy.client$/;"	p
org.apache.spark.deploy.client	deploy/client/ClientListener.scala	/^package org.apache.spark.deploy.client$/;"	p
org.apache.spark.deploy.client	deploy/client/TestClient.scala	/^package org.apache.spark.deploy.client$/;"	p
org.apache.spark.deploy.client	deploy/client/TestExecutor.scala	/^package org.apache.spark.deploy.client$/;"	p
org.apache.spark.deploy.client.{Client, ClientListener}	scheduler/cluster/SparkDeploySchedulerBackend.scala	/^import org.apache.spark.deploy.client.{Client, ClientListener}$/;"	i
org.apache.spark.deploy.master	deploy/master/ApplicationInfo.scala	/^package org.apache.spark.deploy.master$/;"	p
org.apache.spark.deploy.master	deploy/master/ApplicationSource.scala	/^package org.apache.spark.deploy.master$/;"	p
org.apache.spark.deploy.master	deploy/master/ApplicationState.scala	/^package org.apache.spark.deploy.master$/;"	p
org.apache.spark.deploy.master	deploy/master/ExecutorInfo.scala	/^package org.apache.spark.deploy.master$/;"	p
org.apache.spark.deploy.master	deploy/master/FileSystemPersistenceEngine.scala	/^package org.apache.spark.deploy.master$/;"	p
org.apache.spark.deploy.master	deploy/master/LeaderElectionAgent.scala	/^package org.apache.spark.deploy.master$/;"	p
org.apache.spark.deploy.master	deploy/master/Master.scala	/^package org.apache.spark.deploy.master$/;"	p
org.apache.spark.deploy.master	deploy/master/MasterArguments.scala	/^package org.apache.spark.deploy.master$/;"	p
org.apache.spark.deploy.master	deploy/master/MasterMessages.scala	/^package org.apache.spark.deploy.master$/;"	p
org.apache.spark.deploy.master	deploy/master/MasterSource.scala	/^package org.apache.spark.deploy.master$/;"	p
org.apache.spark.deploy.master	deploy/master/PersistenceEngine.scala	/^package org.apache.spark.deploy.master$/;"	p
org.apache.spark.deploy.master	deploy/master/RecoveryState.scala	/^package org.apache.spark.deploy.master$/;"	p
org.apache.spark.deploy.master	deploy/master/SparkZooKeeperSession.scala	/^package org.apache.spark.deploy.master$/;"	p
org.apache.spark.deploy.master	deploy/master/WorkerInfo.scala	/^package org.apache.spark.deploy.master$/;"	p
org.apache.spark.deploy.master	deploy/master/WorkerState.scala	/^package org.apache.spark.deploy.master$/;"	p
org.apache.spark.deploy.master	deploy/master/ZooKeeperLeaderElectionAgent.scala	/^package org.apache.spark.deploy.master$/;"	p
org.apache.spark.deploy.master	deploy/master/ZooKeeperPersistenceEngine.scala	/^package org.apache.spark.deploy.master$/;"	p
org.apache.spark.deploy.master.ExecutorInfo	deploy/master/ui/ApplicationPage.scala	/^import org.apache.spark.deploy.master.ExecutorInfo$/;"	i
org.apache.spark.deploy.master.Master	deploy/LocalSparkCluster.scala	/^import org.apache.spark.deploy.master.Master$/;"	i
org.apache.spark.deploy.master.Master	deploy/client/Client.scala	/^import org.apache.spark.deploy.master.Master$/;"	i
org.apache.spark.deploy.master.Master	deploy/master/ui/MasterWebUI.scala	/^import org.apache.spark.deploy.master.Master$/;"	i
org.apache.spark.deploy.master.Master	deploy/worker/Worker.scala	/^import org.apache.spark.deploy.master.Master$/;"	i
org.apache.spark.deploy.master.MasterMessages.ElectedLeader	deploy/master/LeaderElectionAgent.scala	/^import org.apache.spark.deploy.master.MasterMessages.ElectedLeader$/;"	i
org.apache.spark.deploy.master.MasterMessages._	deploy/master/Master.scala	/^import org.apache.spark.deploy.master.MasterMessages._$/;"	i
org.apache.spark.deploy.master.MasterMessages._	deploy/master/ZooKeeperLeaderElectionAgent.scala	/^import org.apache.spark.deploy.master.MasterMessages._$/;"	i
org.apache.spark.deploy.master.RecoveryState	deploy/FaultToleranceTest.scala	/^import org.apache.spark.deploy.master.RecoveryState$/;"	i
org.apache.spark.deploy.master.RecoveryState.MasterState	deploy/DeployMessage.scala	/^import org.apache.spark.deploy.master.RecoveryState.MasterState$/;"	i
org.apache.spark.deploy.master.ui	deploy/master/ui/ApplicationPage.scala	/^package org.apache.spark.deploy.master.ui$/;"	p
org.apache.spark.deploy.master.ui	deploy/master/ui/IndexPage.scala	/^package org.apache.spark.deploy.master.ui$/;"	p
org.apache.spark.deploy.master.ui	deploy/master/ui/MasterWebUI.scala	/^package org.apache.spark.deploy.master.ui$/;"	p
org.apache.spark.deploy.master.ui.MasterWebUI	deploy/master/Master.scala	/^import org.apache.spark.deploy.master.ui.MasterWebUI$/;"	i
org.apache.spark.deploy.master.{ApplicationInfo, WorkerInfo}	deploy/JsonProtocol.scala	/^import org.apache.spark.deploy.master.{ApplicationInfo, WorkerInfo}$/;"	i
org.apache.spark.deploy.master.{ApplicationInfo, WorkerInfo}	deploy/master/ui/IndexPage.scala	/^import org.apache.spark.deploy.master.{ApplicationInfo, WorkerInfo}$/;"	i
org.apache.spark.deploy.master.{WorkerInfo, ApplicationInfo}	deploy/DeployMessage.scala	/^import org.apache.spark.deploy.master.{WorkerInfo, ApplicationInfo}$/;"	i
org.apache.spark.deploy.worker	deploy/worker/ExecutorRunner.scala	/^package org.apache.spark.deploy.worker$/;"	p
org.apache.spark.deploy.worker	deploy/worker/Worker.scala	/^package org.apache.spark.deploy.worker$/;"	p
org.apache.spark.deploy.worker	deploy/worker/WorkerArguments.scala	/^package org.apache.spark.deploy.worker$/;"	p
org.apache.spark.deploy.worker	deploy/worker/WorkerSource.scala	/^package org.apache.spark.deploy.worker$/;"	p
org.apache.spark.deploy.worker.ExecutorRunner	deploy/DeployMessage.scala	/^import org.apache.spark.deploy.worker.ExecutorRunner$/;"	i
org.apache.spark.deploy.worker.ExecutorRunner	deploy/JsonProtocol.scala	/^import org.apache.spark.deploy.worker.ExecutorRunner$/;"	i
org.apache.spark.deploy.worker.ExecutorRunner	deploy/worker/ui/IndexPage.scala	/^import org.apache.spark.deploy.worker.ExecutorRunner$/;"	i
org.apache.spark.deploy.worker.Worker	deploy/LocalSparkCluster.scala	/^import org.apache.spark.deploy.worker.Worker$/;"	i
org.apache.spark.deploy.worker.Worker	deploy/worker/ui/WorkerWebUI.scala	/^import org.apache.spark.deploy.worker.Worker$/;"	i
org.apache.spark.deploy.worker.ui	deploy/worker/ui/IndexPage.scala	/^package org.apache.spark.deploy.worker.ui$/;"	p
org.apache.spark.deploy.worker.ui	deploy/worker/ui/WorkerWebUI.scala	/^package org.apache.spark.deploy.worker.ui$/;"	p
org.apache.spark.deploy.worker.ui.WorkerWebUI	deploy/worker/Worker.scala	/^import org.apache.spark.deploy.worker.ui.WorkerWebUI$/;"	i
org.apache.spark.deploy.{ApplicationDescription, ExecutorState}	deploy/client/Client.scala	/^import org.apache.spark.deploy.{ApplicationDescription, ExecutorState}$/;"	i
org.apache.spark.deploy.{ApplicationDescription, ExecutorState}	deploy/master/Master.scala	/^import org.apache.spark.deploy.{ApplicationDescription, ExecutorState}$/;"	i
org.apache.spark.deploy.{Command, ApplicationDescription}	deploy/client/TestClient.scala	/^import org.apache.spark.deploy.{Command, ApplicationDescription}$/;"	i
org.apache.spark.deploy.{Command, ApplicationDescription}	scheduler/cluster/SparkDeploySchedulerBackend.scala	/^import org.apache.spark.deploy.{Command, ApplicationDescription}$/;"	i
org.apache.spark.deploy.{ExecutorDescription, ExecutorState}	deploy/master/ExecutorInfo.scala	/^import org.apache.spark.deploy.{ExecutorDescription, ExecutorState}$/;"	i
org.apache.spark.deploy.{ExecutorDescription, ExecutorState}	deploy/worker/Worker.scala	/^import org.apache.spark.deploy.{ExecutorDescription, ExecutorState}$/;"	i
org.apache.spark.deploy.{ExecutorState, ApplicationDescription}	deploy/worker/ExecutorRunner.scala	/^import org.apache.spark.deploy.{ExecutorState, ApplicationDescription}$/;"	i
org.apache.spark.deploy.{LocalSparkCluster, SparkHadoopUtil}	SparkContext.scala	/^import org.apache.spark.deploy.{LocalSparkCluster, SparkHadoopUtil}$/;"	i
org.apache.spark.executor	executor/CoarseGrainedExecutorBackend.scala	/^package org.apache.spark.executor$/;"	p
org.apache.spark.executor	executor/Executor.scala	/^package org.apache.spark.executor$/;"	p
org.apache.spark.executor	executor/ExecutorBackend.scala	/^package org.apache.spark.executor$/;"	p
org.apache.spark.executor	executor/ExecutorExitCode.scala	/^package org.apache.spark.executor$/;"	p
org.apache.spark.executor	executor/ExecutorSource.scala	/^package org.apache.spark.executor$/;"	p
org.apache.spark.executor	executor/ExecutorURLClassLoader.scala	/^package org.apache.spark.executor$/;"	p
org.apache.spark.executor	executor/MesosExecutorBackend.scala	/^package org.apache.spark.executor$/;"	p
org.apache.spark.executor	executor/TaskMetrics.scala	/^package org.apache.spark.executor$/;"	p
org.apache.spark.executor.ExecutorExitCode	scheduler/cluster/ExecutorLossReason.scala	/^import org.apache.spark.executor.ExecutorExitCode$/;"	i
org.apache.spark.executor.ExecutorExitCode	storage/DiskBlockManager.scala	/^import org.apache.spark.executor.ExecutorExitCode$/;"	i
org.apache.spark.executor.ShuffleWriteMetrics	scheduler/ShuffleMapTask.scala	/^import org.apache.spark.executor.ShuffleWriteMetrics$/;"	i
org.apache.spark.executor.TaskMetrics	ShuffleFetcher.scala	/^import org.apache.spark.executor.TaskMetrics$/;"	i
org.apache.spark.executor.TaskMetrics	TaskContext.scala	/^import org.apache.spark.executor.TaskMetrics$/;"	i
org.apache.spark.executor.TaskMetrics	TaskEndReason.scala	/^import org.apache.spark.executor.TaskMetrics$/;"	i
org.apache.spark.executor.TaskMetrics	scheduler/DAGScheduler.scala	/^import org.apache.spark.executor.TaskMetrics$/;"	i
org.apache.spark.executor.TaskMetrics	scheduler/DAGSchedulerEvent.scala	/^import org.apache.spark.executor.TaskMetrics$/;"	i
org.apache.spark.executor.TaskMetrics	scheduler/JobLogger.scala	/^import org.apache.spark.executor.TaskMetrics$/;"	i
org.apache.spark.executor.TaskMetrics	scheduler/SparkListener.scala	/^import org.apache.spark.executor.TaskMetrics$/;"	i
org.apache.spark.executor.TaskMetrics	scheduler/StageInfo.scala	/^import org.apache.spark.executor.TaskMetrics$/;"	i
org.apache.spark.executor.TaskMetrics	scheduler/Task.scala	/^import org.apache.spark.executor.TaskMetrics$/;"	i
org.apache.spark.executor.TaskMetrics	scheduler/TaskResult.scala	/^import org.apache.spark.executor.TaskMetrics$/;"	i
org.apache.spark.executor.TaskMetrics	ui/exec/ExecutorsUI.scala	/^import org.apache.spark.executor.TaskMetrics$/;"	i
org.apache.spark.executor.TaskMetrics	ui/jobs/JobProgressListener.scala	/^import org.apache.spark.executor.TaskMetrics$/;"	i
org.apache.spark.executor.TaskMetrics	ui/jobs/StagePage.scala	/^import org.apache.spark.executor.TaskMetrics$/;"	i
org.apache.spark.executor.{Executor, ExecutorBackend}	scheduler/local/LocalScheduler.scala	/^import org.apache.spark.executor.{Executor, ExecutorBackend}$/;"	i
org.apache.spark.executor.{ShuffleReadMetrics, TaskMetrics}	BlockStoreShuffleFetcher.scala	/^import org.apache.spark.executor.{ShuffleReadMetrics, TaskMetrics}$/;"	i
org.apache.spark.io	io/CompressionCodec.scala	/^package org.apache.spark.io$/;"	p
org.apache.spark.io.CompressionCodec	broadcast/HttpBroadcast.scala	/^import org.apache.spark.io.CompressionCodec$/;"	i
org.apache.spark.io.CompressionCodec	storage/BlockManager.scala	/^import org.apache.spark.io.CompressionCodec$/;"	i
org.apache.spark.metrics	metrics/MetricsConfig.scala	/^package org.apache.spark.metrics$/;"	p
org.apache.spark.metrics	metrics/MetricsSystem.scala	/^package org.apache.spark.metrics$/;"	p
org.apache.spark.metrics.MetricsSystem	SparkEnv.scala	/^import org.apache.spark.metrics.MetricsSystem$/;"	i
org.apache.spark.metrics.MetricsSystem	deploy/master/Master.scala	/^import org.apache.spark.metrics.MetricsSystem$/;"	i
org.apache.spark.metrics.MetricsSystem	deploy/worker/Worker.scala	/^import org.apache.spark.metrics.MetricsSystem$/;"	i
org.apache.spark.metrics.MetricsSystem	metrics/sink/ConsoleSink.scala	/^import org.apache.spark.metrics.MetricsSystem$/;"	i
org.apache.spark.metrics.MetricsSystem	metrics/sink/CsvSink.scala	/^import org.apache.spark.metrics.MetricsSystem$/;"	i
org.apache.spark.metrics.MetricsSystem	metrics/sink/GangliaSink.scala	/^import org.apache.spark.metrics.MetricsSystem$/;"	i
org.apache.spark.metrics.sink	metrics/sink/ConsoleSink.scala	/^package org.apache.spark.metrics.sink$/;"	p
org.apache.spark.metrics.sink	metrics/sink/CsvSink.scala	/^package org.apache.spark.metrics.sink$/;"	p
org.apache.spark.metrics.sink	metrics/sink/GangliaSink.scala	/^package org.apache.spark.metrics.sink$/;"	p
org.apache.spark.metrics.sink	metrics/sink/JmxSink.scala	/^package org.apache.spark.metrics.sink$/;"	p
org.apache.spark.metrics.sink	metrics/sink/MetricsServlet.scala	/^package org.apache.spark.metrics.sink$/;"	p
org.apache.spark.metrics.sink	metrics/sink/Sink.scala	/^package org.apache.spark.metrics.sink$/;"	p
org.apache.spark.metrics.sink.{MetricsServlet, Sink}	metrics/MetricsSystem.scala	/^import org.apache.spark.metrics.sink.{MetricsServlet, Sink}$/;"	i
org.apache.spark.metrics.source	metrics/source/JvmSource.scala	/^package org.apache.spark.metrics.source$/;"	p
org.apache.spark.metrics.source	metrics/source/Source.scala	/^package org.apache.spark.metrics.source$/;"	p
org.apache.spark.metrics.source.Source	deploy/master/ApplicationSource.scala	/^import org.apache.spark.metrics.source.Source$/;"	i
org.apache.spark.metrics.source.Source	deploy/master/MasterSource.scala	/^import org.apache.spark.metrics.source.Source$/;"	i
org.apache.spark.metrics.source.Source	deploy/worker/WorkerSource.scala	/^import org.apache.spark.metrics.source.Source$/;"	i
org.apache.spark.metrics.source.Source	executor/ExecutorSource.scala	/^import org.apache.spark.metrics.source.Source$/;"	i
org.apache.spark.metrics.source.Source	metrics/MetricsSystem.scala	/^import org.apache.spark.metrics.source.Source$/;"	i
org.apache.spark.metrics.source.Source	scheduler/DAGSchedulerSource.scala	/^import org.apache.spark.metrics.source.Source$/;"	i
org.apache.spark.metrics.source.Source	storage/BlockManagerSource.scala	/^import org.apache.spark.metrics.source.Source$/;"	i
org.apache.spark.network	network/BufferMessage.scala	/^package org.apache.spark.network$/;"	p
org.apache.spark.network	network/Connection.scala	/^package org.apache.spark.network$/;"	p
org.apache.spark.network	network/ConnectionManager.scala	/^package org.apache.spark.network$/;"	p
org.apache.spark.network	network/ConnectionManagerId.scala	/^package org.apache.spark.network$/;"	p
org.apache.spark.network	network/ConnectionManagerTest.scala	/^package org.apache.spark.network$/;"	p
org.apache.spark.network	network/Message.scala	/^package org.apache.spark.network$/;"	p
org.apache.spark.network	network/MessageChunk.scala	/^package org.apache.spark.network$/;"	p
org.apache.spark.network	network/MessageChunkHeader.scala	/^package org.apache.spark.network$/;"	p
org.apache.spark.network	network/ReceiverTest.scala	/^package org.apache.spark.network$/;"	p
org.apache.spark.network	network/SenderTest.scala	/^package org.apache.spark.network$/;"	p
org.apache.spark.network.BufferMessage	storage/BlockFetcherIterator.scala	/^import org.apache.spark.network.BufferMessage$/;"	i
org.apache.spark.network.ConnectionManager	SparkEnv.scala	/^import org.apache.spark.network.ConnectionManager$/;"	i
org.apache.spark.network.ConnectionManagerId	network/netty/ShuffleCopier.scala	/^import org.apache.spark.network.ConnectionManagerId$/;"	i
org.apache.spark.network.ConnectionManagerId	storage/BlockFetcherIterator.scala	/^import org.apache.spark.network.ConnectionManagerId$/;"	i
org.apache.spark.network._	storage/BlockManager.scala	/^import org.apache.spark.network._$/;"	i
org.apache.spark.network._	storage/BlockManagerWorker.scala	/^import org.apache.spark.network._$/;"	i
org.apache.spark.network._	storage/BlockMessage.scala	/^import org.apache.spark.network._$/;"	i
org.apache.spark.network._	storage/BlockMessageArray.scala	/^import org.apache.spark.network._$/;"	i
org.apache.spark.network.netty	network/netty/FileHeader.scala	/^package org.apache.spark.network.netty$/;"	p
org.apache.spark.network.netty	network/netty/ShuffleCopier.scala	/^package org.apache.spark.network.netty$/;"	p
org.apache.spark.network.netty	network/netty/ShuffleSender.scala	/^package org.apache.spark.network.netty$/;"	p
org.apache.spark.network.netty.ShuffleCopier	storage/BlockFetcherIterator.scala	/^import org.apache.spark.network.netty.ShuffleCopier$/;"	i
org.apache.spark.network.netty.{PathResolver, ShuffleSender}	storage/DiskBlockManager.scala	/^import org.apache.spark.network.netty.{PathResolver, ShuffleSender}$/;"	i
org.apache.spark.partial	partial/ApproximateActionListener.scala	/^package org.apache.spark.partial$/;"	p
org.apache.spark.partial	partial/ApproximateEvaluator.scala	/^package org.apache.spark.partial$/;"	p
org.apache.spark.partial	partial/BoundedDouble.scala	/^package org.apache.spark.partial$/;"	p
org.apache.spark.partial	partial/CountEvaluator.scala	/^package org.apache.spark.partial$/;"	p
org.apache.spark.partial	partial/GroupedCountEvaluator.scala	/^package org.apache.spark.partial$/;"	p
org.apache.spark.partial	partial/GroupedMeanEvaluator.scala	/^package org.apache.spark.partial$/;"	p
org.apache.spark.partial	partial/GroupedSumEvaluator.scala	/^package org.apache.spark.partial$/;"	p
org.apache.spark.partial	partial/MeanEvaluator.scala	/^package org.apache.spark.partial$/;"	p
org.apache.spark.partial	partial/PartialResult.scala	/^package org.apache.spark.partial$/;"	p
org.apache.spark.partial	partial/StudentTCacher.scala	/^package org.apache.spark.partial$/;"	p
org.apache.spark.partial	partial/SumEvaluator.scala	/^package org.apache.spark.partial$/;"	p
org.apache.spark.partial.BoundedDouble	api/java/JavaPairRDD.scala	/^import org.apache.spark.partial.BoundedDouble$/;"	i
org.apache.spark.partial.BoundedDouble	rdd/DoubleRDDFunctions.scala	/^import org.apache.spark.partial.BoundedDouble$/;"	i
org.apache.spark.partial.BoundedDouble	rdd/RDD.scala	/^import org.apache.spark.partial.BoundedDouble$/;"	i
org.apache.spark.partial.CountEvaluator	rdd/RDD.scala	/^import org.apache.spark.partial.CountEvaluator$/;"	i
org.apache.spark.partial.GroupedCountEvaluator	rdd/RDD.scala	/^import org.apache.spark.partial.GroupedCountEvaluator$/;"	i
org.apache.spark.partial.MeanEvaluator	rdd/DoubleRDDFunctions.scala	/^import org.apache.spark.partial.MeanEvaluator$/;"	i
org.apache.spark.partial.PartialResult	api/java/JavaPairRDD.scala	/^import org.apache.spark.partial.PartialResult$/;"	i
org.apache.spark.partial.PartialResult	rdd/DoubleRDDFunctions.scala	/^import org.apache.spark.partial.PartialResult$/;"	i
org.apache.spark.partial.PartialResult	rdd/RDD.scala	/^import org.apache.spark.partial.PartialResult$/;"	i
org.apache.spark.partial.SumEvaluator	rdd/DoubleRDDFunctions.scala	/^import org.apache.spark.partial.SumEvaluator$/;"	i
org.apache.spark.partial.{ApproximateActionListener, ApproximateEvaluator, PartialResult}	scheduler/DAGScheduler.scala	/^import org.apache.spark.partial.{ApproximateActionListener, ApproximateEvaluator, PartialResult}$/;"	i
org.apache.spark.partial.{ApproximateEvaluator, PartialResult}	SparkContext.scala	/^import org.apache.spark.partial.{ApproximateEvaluator, PartialResult}$/;"	i
org.apache.spark.partial.{BoundedDouble, PartialResult}	api/java/JavaDoubleRDD.scala	/^import org.apache.spark.partial.{BoundedDouble, PartialResult}$/;"	i
org.apache.spark.partial.{BoundedDouble, PartialResult}	rdd/PairRDDFunctions.scala	/^import org.apache.spark.partial.{BoundedDouble, PartialResult}$/;"	i
org.apache.spark.partial.{PartialResult, BoundedDouble}	api/java/JavaRDDLike.scala	/^import org.apache.spark.partial.{PartialResult, BoundedDouble}$/;"	i
org.apache.spark.rdd	rdd/AsyncRDDActions.scala	/^package org.apache.spark.rdd$/;"	p
org.apache.spark.rdd	rdd/BlockRDD.scala	/^package org.apache.spark.rdd$/;"	p
org.apache.spark.rdd	rdd/CartesianRDD.scala	/^package org.apache.spark.rdd$/;"	p
org.apache.spark.rdd	rdd/CheckpointRDD.scala	/^package org.apache.spark.rdd$/;"	p
org.apache.spark.rdd	rdd/CoGroupedRDD.scala	/^package org.apache.spark.rdd$/;"	p
org.apache.spark.rdd	rdd/CoalescedRDD.scala	/^package org.apache.spark.rdd$/;"	p
org.apache.spark.rdd	rdd/DoubleRDDFunctions.scala	/^package org.apache.spark.rdd$/;"	p
org.apache.spark.rdd	rdd/EmptyRDD.scala	/^package org.apache.spark.rdd$/;"	p
org.apache.spark.rdd	rdd/FilteredRDD.scala	/^package org.apache.spark.rdd$/;"	p
org.apache.spark.rdd	rdd/FlatMappedRDD.scala	/^package org.apache.spark.rdd$/;"	p
org.apache.spark.rdd	rdd/FlatMappedValuesRDD.scala	/^package org.apache.spark.rdd$/;"	p
org.apache.spark.rdd	rdd/GlommedRDD.scala	/^package org.apache.spark.rdd$/;"	p
org.apache.spark.rdd	rdd/HadoopRDD.scala	/^package org.apache.spark.rdd$/;"	p
org.apache.spark.rdd	rdd/JdbcRDD.scala	/^package org.apache.spark.rdd$/;"	p
org.apache.spark.rdd	rdd/MapPartitionsRDD.scala	/^package org.apache.spark.rdd$/;"	p
org.apache.spark.rdd	rdd/MappedRDD.scala	/^package org.apache.spark.rdd$/;"	p
org.apache.spark.rdd	rdd/MappedValuesRDD.scala	/^package org.apache.spark.rdd$/;"	p
org.apache.spark.rdd	rdd/NewHadoopRDD.scala	/^package org.apache.spark.rdd$/;"	p
org.apache.spark.rdd	rdd/OrderedRDDFunctions.scala	/^package org.apache.spark.rdd$/;"	p
org.apache.spark.rdd	rdd/PairRDDFunctions.scala	/^package org.apache.spark.rdd$/;"	p
org.apache.spark.rdd	rdd/ParallelCollectionRDD.scala	/^package org.apache.spark.rdd$/;"	p
org.apache.spark.rdd	rdd/PartitionPruningRDD.scala	/^package org.apache.spark.rdd$/;"	p
org.apache.spark.rdd	rdd/PipedRDD.scala	/^package org.apache.spark.rdd$/;"	p
org.apache.spark.rdd	rdd/RDD.scala	/^package org.apache.spark.rdd$/;"	p
org.apache.spark.rdd	rdd/RDDCheckpointData.scala	/^package org.apache.spark.rdd$/;"	p
org.apache.spark.rdd	rdd/SampledRDD.scala	/^package org.apache.spark.rdd$/;"	p
org.apache.spark.rdd	rdd/SequenceFileRDDFunctions.scala	/^package org.apache.spark.rdd$/;"	p
org.apache.spark.rdd	rdd/ShuffledRDD.scala	/^package org.apache.spark.rdd$/;"	p
org.apache.spark.rdd	rdd/SubtractedRDD.scala	/^package org.apache.spark.rdd$/;"	p
org.apache.spark.rdd	rdd/UnionRDD.scala	/^package org.apache.spark.rdd$/;"	p
org.apache.spark.rdd	rdd/ZippedPartitionsRDD.scala	/^package org.apache.spark.rdd$/;"	p
org.apache.spark.rdd	rdd/ZippedRDD.scala	/^package org.apache.spark.rdd$/;"	p
org.apache.spark.rdd.OrderedRDDFunctions	api/java/JavaPairRDD.scala	/^import org.apache.spark.rdd.OrderedRDDFunctions$/;"	i
org.apache.spark.rdd.PipedRDD	api/python/PythonRDD.scala	/^import org.apache.spark.rdd.PipedRDD$/;"	i
org.apache.spark.rdd.RDD	CacheManager.scala	/^import org.apache.spark.rdd.RDD$/;"	i
org.apache.spark.rdd.RDD	Dependency.scala	/^import org.apache.spark.rdd.RDD$/;"	i
org.apache.spark.rdd.RDD	FutureAction.scala	/^import org.apache.spark.rdd.RDD$/;"	i
org.apache.spark.rdd.RDD	Partitioner.scala	/^import org.apache.spark.rdd.RDD$/;"	i
org.apache.spark.rdd.RDD	api/java/JavaDoubleRDD.scala	/^import org.apache.spark.rdd.RDD$/;"	i
org.apache.spark.rdd.RDD	api/java/JavaPairRDD.scala	/^import org.apache.spark.rdd.RDD$/;"	i
org.apache.spark.rdd.RDD	api/java/JavaRDD.scala	/^import org.apache.spark.rdd.RDD$/;"	i
org.apache.spark.rdd.RDD	api/java/JavaRDDLike.scala	/^import org.apache.spark.rdd.RDD$/;"	i
org.apache.spark.rdd.RDD	api/java/JavaSparkContext.scala	/^import org.apache.spark.rdd.RDD$/;"	i
org.apache.spark.rdd.RDD	api/python/PythonRDD.scala	/^import org.apache.spark.rdd.RDD$/;"	i
org.apache.spark.rdd.RDD	partial/ApproximateActionListener.scala	/^import org.apache.spark.rdd.RDD$/;"	i
org.apache.spark.rdd.RDD	scheduler/DAGScheduler.scala	/^import org.apache.spark.rdd.RDD$/;"	i
org.apache.spark.rdd.RDD	scheduler/DAGSchedulerEvent.scala	/^import org.apache.spark.rdd.RDD$/;"	i
org.apache.spark.rdd.RDD	scheduler/JobLogger.scala	/^import org.apache.spark.rdd.RDD$/;"	i
org.apache.spark.rdd.RDD	scheduler/ResultTask.scala	/^import org.apache.spark.rdd.RDD$/;"	i
org.apache.spark.rdd.RDD	scheduler/ShuffleMapTask.scala	/^import org.apache.spark.rdd.RDD$/;"	i
org.apache.spark.rdd.RDD	scheduler/Stage.scala	/^import org.apache.spark.rdd.RDD$/;"	i
org.apache.spark.rdd.RDDCheckpointData	scheduler/ResultTask.scala	/^import org.apache.spark.rdd.RDDCheckpointData$/;"	i
org.apache.spark.rdd.RDDCheckpointData	scheduler/ShuffleMapTask.scala	/^import org.apache.spark.rdd.RDDCheckpointData$/;"	i
org.apache.spark.rdd._	SparkContext.scala	/^import org.apache.spark.rdd._$/;"	i
org.apache.spark.scheduler	scheduler/ActiveJob.scala	/^package org.apache.spark.scheduler$/;"	p
org.apache.spark.scheduler	scheduler/DAGScheduler.scala	/^package org.apache.spark.scheduler$/;"	p
org.apache.spark.scheduler	scheduler/DAGSchedulerEvent.scala	/^package org.apache.spark.scheduler$/;"	p
org.apache.spark.scheduler	scheduler/DAGSchedulerSource.scala	/^package org.apache.spark.scheduler$/;"	p
org.apache.spark.scheduler	scheduler/InputFormatInfo.scala	/^package org.apache.spark.scheduler$/;"	p
org.apache.spark.scheduler	scheduler/JobListener.scala	/^package org.apache.spark.scheduler$/;"	p
org.apache.spark.scheduler	scheduler/JobLogger.scala	/^package org.apache.spark.scheduler$/;"	p
org.apache.spark.scheduler	scheduler/JobResult.scala	/^package org.apache.spark.scheduler$/;"	p
org.apache.spark.scheduler	scheduler/JobWaiter.scala	/^package org.apache.spark.scheduler$/;"	p
org.apache.spark.scheduler	scheduler/MapStatus.scala	/^package org.apache.spark.scheduler$/;"	p
org.apache.spark.scheduler	scheduler/Pool.scala	/^package org.apache.spark.scheduler$/;"	p
org.apache.spark.scheduler	scheduler/ResultTask.scala	/^package org.apache.spark.scheduler$/;"	p
org.apache.spark.scheduler	scheduler/Schedulable.scala	/^package org.apache.spark.scheduler$/;"	p
org.apache.spark.scheduler	scheduler/SchedulableBuilder.scala	/^package org.apache.spark.scheduler$/;"	p
org.apache.spark.scheduler	scheduler/SchedulingAlgorithm.scala	/^package org.apache.spark.scheduler$/;"	p
org.apache.spark.scheduler	scheduler/SchedulingMode.scala	/^package org.apache.spark.scheduler$/;"	p
org.apache.spark.scheduler	scheduler/ShuffleMapTask.scala	/^package org.apache.spark.scheduler$/;"	p
org.apache.spark.scheduler	scheduler/SparkListener.scala	/^package org.apache.spark.scheduler$/;"	p
org.apache.spark.scheduler	scheduler/SparkListenerBus.scala	/^package org.apache.spark.scheduler$/;"	p
org.apache.spark.scheduler	scheduler/SplitInfo.scala	/^package org.apache.spark.scheduler$/;"	p
org.apache.spark.scheduler	scheduler/Stage.scala	/^package org.apache.spark.scheduler$/;"	p
org.apache.spark.scheduler	scheduler/StageInfo.scala	/^package org.apache.spark.scheduler$/;"	p
org.apache.spark.scheduler	scheduler/Task.scala	/^package org.apache.spark.scheduler$/;"	p
org.apache.spark.scheduler	scheduler/TaskDescription.scala	/^package org.apache.spark.scheduler$/;"	p
org.apache.spark.scheduler	scheduler/TaskInfo.scala	/^package org.apache.spark.scheduler$/;"	p
org.apache.spark.scheduler	scheduler/TaskLocality.scala	/^package org.apache.spark.scheduler$/;"	p
org.apache.spark.scheduler	scheduler/TaskLocation.scala	/^package org.apache.spark.scheduler$/;"	p
org.apache.spark.scheduler	scheduler/TaskResult.scala	/^package org.apache.spark.scheduler$/;"	p
org.apache.spark.scheduler	scheduler/TaskScheduler.scala	/^package org.apache.spark.scheduler$/;"	p
org.apache.spark.scheduler	scheduler/TaskSet.scala	/^package org.apache.spark.scheduler$/;"	p
org.apache.spark.scheduler	scheduler/TaskSetManager.scala	/^package org.apache.spark.scheduler$/;"	p
org.apache.spark.scheduler.JobFailed	FutureAction.scala	/^import org.apache.spark.scheduler.JobFailed$/;"	i
org.apache.spark.scheduler.JobListener	partial/ApproximateActionListener.scala	/^import org.apache.spark.scheduler.JobListener$/;"	i
org.apache.spark.scheduler.MapStatus	MapOutputTracker.scala	/^import org.apache.spark.scheduler.MapStatus$/;"	i
org.apache.spark.scheduler.MapStatus	serializer/KryoSerializer.scala	/^import org.apache.spark.scheduler.MapStatus$/;"	i
org.apache.spark.scheduler.MapStatus	util/TimeStampedHashMap.scala	/^import org.apache.spark.scheduler.MapStatus$/;"	i
org.apache.spark.scheduler.SchedulingMode	ui/UIWorkloadGenerator.scala	/^import org.apache.spark.scheduler.SchedulingMode$/;"	i
org.apache.spark.scheduler.SchedulingMode	ui/jobs/IndexPage.scala	/^import org.apache.spark.scheduler.SchedulingMode$/;"	i
org.apache.spark.scheduler.SchedulingMode	ui/jobs/JobProgressUI.scala	/^import org.apache.spark.scheduler.SchedulingMode$/;"	i
org.apache.spark.scheduler.SchedulingMode.SchedulingMode	scheduler/Pool.scala	/^import org.apache.spark.scheduler.SchedulingMode.SchedulingMode$/;"	i
org.apache.spark.scheduler.SchedulingMode.SchedulingMode	scheduler/Schedulable.scala	/^import org.apache.spark.scheduler.SchedulingMode.SchedulingMode$/;"	i
org.apache.spark.scheduler.SchedulingMode.SchedulingMode	scheduler/TaskScheduler.scala	/^import org.apache.spark.scheduler.SchedulingMode.SchedulingMode$/;"	i
org.apache.spark.scheduler.SchedulingMode.SchedulingMode	scheduler/cluster/ClusterScheduler.scala	/^import org.apache.spark.scheduler.SchedulingMode.SchedulingMode$/;"	i
org.apache.spark.scheduler.SchedulingMode.SchedulingMode	scheduler/local/LocalScheduler.scala	/^import org.apache.spark.scheduler.SchedulingMode.SchedulingMode$/;"	i
org.apache.spark.scheduler.SchedulingMode.SchedulingMode	ui/jobs/JobProgressUI.scala	/^import org.apache.spark.scheduler.SchedulingMode.SchedulingMode$/;"	i
org.apache.spark.scheduler.Stage	ui/jobs/PoolPage.scala	/^import org.apache.spark.scheduler.Stage$/;"	i
org.apache.spark.scheduler.StageInfo	SparkContext.scala	/^import org.apache.spark.scheduler.StageInfo$/;"	i
org.apache.spark.scheduler.StatsReportListener._	scheduler/SparkListener.scala	/^    import org.apache.spark.scheduler.StatsReportListener._$/;"	i
org.apache.spark.scheduler.TaskDescription	scheduler/cluster/CoarseGrainedClusterMessage.scala	/^import org.apache.spark.scheduler.TaskDescription$/;"	i
org.apache.spark.scheduler.TaskDescription	scheduler/cluster/CoarseGrainedSchedulerBackend.scala	/^import org.apache.spark.scheduler.TaskDescription$/;"	i
org.apache.spark.scheduler.TaskDescription	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^import org.apache.spark.scheduler.TaskDescription$/;"	i
org.apache.spark.scheduler.TaskInfo	ui/exec/ExecutorsUI.scala	/^import org.apache.spark.scheduler.TaskInfo$/;"	i
org.apache.spark.scheduler.TaskInfo	ui/jobs/StagePage.scala	/^import org.apache.spark.scheduler.TaskInfo$/;"	i
org.apache.spark.scheduler._	SparkContext.scala	/^import org.apache.spark.scheduler._$/;"	i
org.apache.spark.scheduler._	executor/Executor.scala	/^import org.apache.spark.scheduler._$/;"	i
org.apache.spark.scheduler._	scheduler/cluster/ClusterScheduler.scala	/^import org.apache.spark.scheduler._$/;"	i
org.apache.spark.scheduler._	scheduler/cluster/ClusterTaskSetManager.scala	/^import org.apache.spark.scheduler._$/;"	i
org.apache.spark.scheduler._	scheduler/local/LocalScheduler.scala	/^import org.apache.spark.scheduler._$/;"	i
org.apache.spark.scheduler._	ui/jobs/JobProgressListener.scala	/^import org.apache.spark.scheduler._$/;"	i
org.apache.spark.scheduler._	ui/jobs/JobProgressUI.scala	/^import org.apache.spark.scheduler._$/;"	i
org.apache.spark.scheduler.cluster	scheduler/cluster/ClusterScheduler.scala	/^package org.apache.spark.scheduler.cluster$/;"	p
org.apache.spark.scheduler.cluster	scheduler/cluster/ClusterTaskSetManager.scala	/^package org.apache.spark.scheduler.cluster$/;"	p
org.apache.spark.scheduler.cluster	scheduler/cluster/CoarseGrainedClusterMessage.scala	/^package org.apache.spark.scheduler.cluster$/;"	p
org.apache.spark.scheduler.cluster	scheduler/cluster/CoarseGrainedSchedulerBackend.scala	/^package org.apache.spark.scheduler.cluster$/;"	p
org.apache.spark.scheduler.cluster	scheduler/cluster/ExecutorLossReason.scala	/^package org.apache.spark.scheduler.cluster$/;"	p
org.apache.spark.scheduler.cluster	scheduler/cluster/SchedulerBackend.scala	/^package org.apache.spark.scheduler.cluster$/;"	p
org.apache.spark.scheduler.cluster	scheduler/cluster/SimrSchedulerBackend.scala	/^package org.apache.spark.scheduler.cluster$/;"	p
org.apache.spark.scheduler.cluster	scheduler/cluster/SparkDeploySchedulerBackend.scala	/^package org.apache.spark.scheduler.cluster$/;"	p
org.apache.spark.scheduler.cluster	scheduler/cluster/TaskResultGetter.scala	/^package org.apache.spark.scheduler.cluster$/;"	p
org.apache.spark.scheduler.cluster	scheduler/cluster/WorkerOffer.scala	/^package org.apache.spark.scheduler.cluster$/;"	p
org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages._	executor/CoarseGrainedExecutorBackend.scala	/^import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages._$/;"	i
org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages._	scheduler/cluster/CoarseGrainedSchedulerBackend.scala	/^import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages._$/;"	i
org.apache.spark.scheduler.cluster.mesos	scheduler/cluster/mesos/CoarseMesosSchedulerBackend.scala	/^package org.apache.spark.scheduler.cluster.mesos$/;"	p
org.apache.spark.scheduler.cluster.mesos	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^package org.apache.spark.scheduler.cluster.mesos$/;"	p
org.apache.spark.scheduler.cluster.mesos.{CoarseMesosSchedulerBackend, MesosSchedulerBackend}	SparkContext.scala	/^import org.apache.spark.scheduler.cluster.mesos.{CoarseMesosSchedulerBackend, MesosSchedulerBackend}$/;"	i
org.apache.spark.scheduler.cluster.{ClusterScheduler, CoarseGrainedSchedulerBackend}	scheduler/cluster/mesos/CoarseMesosSchedulerBackend.scala	/^import org.apache.spark.scheduler.cluster.{ClusterScheduler, CoarseGrainedSchedulerBackend}$/;"	i
org.apache.spark.scheduler.cluster.{ClusterScheduler, ExecutorExited, ExecutorLossReason}	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^import org.apache.spark.scheduler.cluster.{ClusterScheduler, ExecutorExited, ExecutorLossReason}$/;"	i
org.apache.spark.scheduler.cluster.{CoarseGrainedSchedulerBackend,	SparkContext.scala	/^import org.apache.spark.scheduler.cluster.{CoarseGrainedSchedulerBackend,$/;"	i
org.apache.spark.scheduler.cluster.{SchedulerBackend, SlaveLost, WorkerOffer}	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^import org.apache.spark.scheduler.cluster.{SchedulerBackend, SlaveLost, WorkerOffer}$/;"	i
org.apache.spark.scheduler.local	scheduler/local/LocalScheduler.scala	/^package org.apache.spark.scheduler.local$/;"	p
org.apache.spark.scheduler.local	scheduler/local/LocalTaskSetManager.scala	/^package org.apache.spark.scheduler.local$/;"	p
org.apache.spark.scheduler.local.LocalScheduler	SparkContext.scala	/^import org.apache.spark.scheduler.local.LocalScheduler$/;"	i
org.apache.spark.scheduler.{DirectTaskResult, IndirectTaskResult, Pool, Schedulable, Task,	scheduler/local/LocalTaskSetManager.scala	/^import org.apache.spark.scheduler.{DirectTaskResult, IndirectTaskResult, Pool, Schedulable, Task,$/;"	i
org.apache.spark.scheduler.{DirectTaskResult, IndirectTaskResult, TaskResult}	scheduler/cluster/TaskResultGetter.scala	/^import org.apache.spark.scheduler.{DirectTaskResult, IndirectTaskResult, TaskResult}$/;"	i
org.apache.spark.scheduler.{JobSucceeded, JobWaiter}	FutureAction.scala	/^import org.apache.spark.scheduler.{JobSucceeded, JobWaiter}$/;"	i
org.apache.spark.scheduler.{ResultTask, ShuffleMapTask}	rdd/RDDCheckpointData.scala	/^import org.apache.spark.scheduler.{ResultTask, ShuffleMapTask}$/;"	i
org.apache.spark.scheduler.{Schedulable, StageInfo}	ui/jobs/PoolTable.scala	/^import org.apache.spark.scheduler.{Schedulable, StageInfo}$/;"	i
org.apache.spark.scheduler.{SchedulingMode, StageInfo, TaskInfo}	ui/jobs/StageTable.scala	/^import org.apache.spark.scheduler.{SchedulingMode, StageInfo, TaskInfo}$/;"	i
org.apache.spark.scheduler.{SparkListenerTaskStart, SparkListenerTaskEnd, SparkListener}	ui/exec/ExecutorsUI.scala	/^import org.apache.spark.scheduler.{SparkListenerTaskStart, SparkListenerTaskEnd, SparkListener}$/;"	i
org.apache.spark.serializer	serializer/JavaSerializer.scala	/^package org.apache.spark.serializer$/;"	p
org.apache.spark.serializer	serializer/KryoSerializer.scala	/^package org.apache.spark.serializer$/;"	p
org.apache.spark.serializer	serializer/Serializer.scala	/^package org.apache.spark.serializer$/;"	p
org.apache.spark.serializer	serializer/SerializerManager.scala	/^package org.apache.spark.serializer$/;"	p
org.apache.spark.serializer.JavaSerializer	Accumulators.scala	/^import org.apache.spark.serializer.JavaSerializer$/;"	i
org.apache.spark.serializer.JavaSerializer	rdd/ParallelCollectionRDD.scala	/^import org.apache.spark.serializer.JavaSerializer$/;"	i
org.apache.spark.serializer.KryoSerializer	storage/StoragePerfTester.scala	/^import org.apache.spark.serializer.KryoSerializer$/;"	i
org.apache.spark.serializer.KryoSerializer	storage/ThreadingTest.scala	/^import org.apache.spark.serializer.KryoSerializer$/;"	i
org.apache.spark.serializer.Serializer	BlockStoreShuffleFetcher.scala	/^import org.apache.spark.serializer.Serializer$/;"	i
org.apache.spark.serializer.Serializer	ShuffleFetcher.scala	/^import org.apache.spark.serializer.Serializer$/;"	i
org.apache.spark.serializer.Serializer	storage/BlockFetcherIterator.scala	/^import org.apache.spark.serializer.Serializer$/;"	i
org.apache.spark.serializer.Serializer	storage/BlockManager.scala	/^import org.apache.spark.serializer.Serializer$/;"	i
org.apache.spark.serializer.Serializer	storage/DiskStore.scala	/^import org.apache.spark.serializer.Serializer$/;"	i
org.apache.spark.serializer.Serializer	storage/ShuffleBlockManager.scala	/^import org.apache.spark.serializer.Serializer$/;"	i
org.apache.spark.serializer.SerializerInstance	scheduler/Task.scala	/^import org.apache.spark.serializer.SerializerInstance$/;"	i
org.apache.spark.serializer.SerializerInstance	scheduler/cluster/TaskResultGetter.scala	/^import org.apache.spark.serializer.SerializerInstance$/;"	i
org.apache.spark.serializer.{DeserializationStream, SerializationStream, SerializerInstance}	util/Utils.scala	/^import org.apache.spark.serializer.{DeserializationStream, SerializationStream, SerializerInstance}$/;"	i
org.apache.spark.serializer.{SerializationStream, Serializer}	storage/BlockObjectWriter.scala	/^import org.apache.spark.serializer.{SerializationStream, Serializer}$/;"	i
org.apache.spark.serializer.{Serializer, SerializerManager}	SparkEnv.scala	/^import org.apache.spark.serializer.{Serializer, SerializerManager}$/;"	i
org.apache.spark.storage	storage/BlockException.scala	/^package org.apache.spark.storage$/;"	p
org.apache.spark.storage	storage/BlockFetchTracker.scala	/^package org.apache.spark.storage$/;"	p
org.apache.spark.storage	storage/BlockFetcherIterator.scala	/^package org.apache.spark.storage$/;"	p
org.apache.spark.storage	storage/BlockId.scala	/^package org.apache.spark.storage$/;"	p
org.apache.spark.storage	storage/BlockInfo.scala	/^package org.apache.spark.storage$/;"	p
org.apache.spark.storage	storage/BlockManager.scala	/^package org.apache.spark.storage$/;"	p
org.apache.spark.storage	storage/BlockManagerId.scala	/^package org.apache.spark.storage$/;"	p
org.apache.spark.storage	storage/BlockManagerMaster.scala	/^package org.apache.spark.storage$/;"	p
org.apache.spark.storage	storage/BlockManagerMasterActor.scala	/^package org.apache.spark.storage$/;"	p
org.apache.spark.storage	storage/BlockManagerMessages.scala	/^package org.apache.spark.storage$/;"	p
org.apache.spark.storage	storage/BlockManagerSlaveActor.scala	/^package org.apache.spark.storage$/;"	p
org.apache.spark.storage	storage/BlockManagerSource.scala	/^package org.apache.spark.storage$/;"	p
org.apache.spark.storage	storage/BlockManagerWorker.scala	/^package org.apache.spark.storage$/;"	p
org.apache.spark.storage	storage/BlockMessage.scala	/^package org.apache.spark.storage$/;"	p
org.apache.spark.storage	storage/BlockMessageArray.scala	/^package org.apache.spark.storage$/;"	p
org.apache.spark.storage	storage/BlockObjectWriter.scala	/^package org.apache.spark.storage$/;"	p
org.apache.spark.storage	storage/BlockStore.scala	/^package org.apache.spark.storage$/;"	p
org.apache.spark.storage	storage/DiskBlockManager.scala	/^package org.apache.spark.storage$/;"	p
org.apache.spark.storage	storage/DiskStore.scala	/^package org.apache.spark.storage$/;"	p
org.apache.spark.storage	storage/FileSegment.scala	/^package org.apache.spark.storage$/;"	p
org.apache.spark.storage	storage/MemoryStore.scala	/^package org.apache.spark.storage$/;"	p
org.apache.spark.storage	storage/PutResult.scala	/^package org.apache.spark.storage$/;"	p
org.apache.spark.storage	storage/ShuffleBlockManager.scala	/^package org.apache.spark.storage$/;"	p
org.apache.spark.storage	storage/StorageLevel.scala	/^package org.apache.spark.storage$/;"	p
org.apache.spark.storage	storage/StoragePerfTester.scala	/^package org.apache.spark.storage$/;"	p
org.apache.spark.storage	storage/StorageUtils.scala	/^package org.apache.spark.storage$/;"	p
org.apache.spark.storage	storage/ThreadingTest.scala	/^package org.apache.spark.storage$/;"	p
org.apache.spark.storage.BlockId	network/netty/ShuffleCopier.scala	/^import org.apache.spark.storage.BlockId$/;"	i
org.apache.spark.storage.BlockId	scheduler/TaskResult.scala	/^import org.apache.spark.storage.BlockId$/;"	i
org.apache.spark.storage.BlockManager	network/BufferMessage.scala	/^import org.apache.spark.storage.BlockManager$/;"	i
org.apache.spark.storage.BlockManager	util/ByteBufferInputStream.scala	/^import org.apache.spark.storage.BlockManager$/;"	i
org.apache.spark.storage.BlockManagerId	FetchFailedException.scala	/^import org.apache.spark.storage.BlockManagerId$/;"	i
org.apache.spark.storage.BlockManagerId	MapOutputTracker.scala	/^import org.apache.spark.storage.BlockManagerId$/;"	i
org.apache.spark.storage.BlockManagerId	TaskEndReason.scala	/^import org.apache.spark.storage.BlockManagerId$/;"	i
org.apache.spark.storage.BlockManagerId	scheduler/MapStatus.scala	/^import org.apache.spark.storage.BlockManagerId$/;"	i
org.apache.spark.storage.BlockManagerId	scheduler/Stage.scala	/^import org.apache.spark.storage.BlockManagerId$/;"	i
org.apache.spark.storage.BlockManagerMasterActor.BlockStatus	ui/storage/RDDPage.scala	/^import org.apache.spark.storage.BlockManagerMasterActor.BlockStatus$/;"	i
org.apache.spark.storage.BlockManagerMessages._	storage/BlockManagerMaster.scala	/^import org.apache.spark.storage.BlockManagerMessages._$/;"	i
org.apache.spark.storage.BlockManagerMessages._	storage/BlockManagerMasterActor.scala	/^import org.apache.spark.storage.BlockManagerMessages._$/;"	i
org.apache.spark.storage.BlockManagerMessages._	storage/BlockManagerSlaveActor.scala	/^import org.apache.spark.storage.BlockManagerMessages._$/;"	i
org.apache.spark.storage.ShuffleBlockManager.ShuffleFileGroup	storage/ShuffleBlockManager.scala	/^import org.apache.spark.storage.ShuffleBlockManager.ShuffleFileGroup$/;"	i
org.apache.spark.storage.StorageLevel	api/java/JavaDoubleRDD.scala	/^import org.apache.spark.storage.StorageLevel$/;"	i
org.apache.spark.storage.StorageLevel	api/java/JavaPairRDD.scala	/^import org.apache.spark.storage.StorageLevel$/;"	i
org.apache.spark.storage.StorageLevel	api/java/JavaRDD.scala	/^import org.apache.spark.storage.StorageLevel$/;"	i
org.apache.spark.storage.StorageLevel	api/java/JavaRDDLike.scala	/^import org.apache.spark.storage.StorageLevel$/;"	i
org.apache.spark.storage.StorageLevel	rdd/RDD.scala	/^import org.apache.spark.storage.StorageLevel$/;"	i
org.apache.spark.storage.StorageLevel	scheduler/JobLogger.scala	/^import org.apache.spark.storage.StorageLevel$/;"	i
org.apache.spark.storage._	scheduler/ShuffleMapTask.scala	/^import org.apache.spark.storage._$/;"	i
org.apache.spark.storage._	serializer/KryoSerializer.scala	/^import org.apache.spark.storage._$/;"	i
org.apache.spark.storage.{BlockId, BlockManager, BlockManagerMaster, RDDBlockId}	scheduler/DAGScheduler.scala	/^import org.apache.spark.storage.{BlockId, BlockManager, BlockManagerMaster, RDDBlockId}$/;"	i
org.apache.spark.storage.{BlockId, BlockManager, StorageLevel, RDDBlockId}	CacheManager.scala	/^import org.apache.spark.storage.{BlockId, BlockManager, StorageLevel, RDDBlockId}$/;"	i
org.apache.spark.storage.{BlockId, BlockManagerId, ShuffleBlockId}	BlockStoreShuffleFetcher.scala	/^import org.apache.spark.storage.{BlockId, BlockManagerId, ShuffleBlockId}$/;"	i
org.apache.spark.storage.{BlockId, BlockManager}	rdd/BlockRDD.scala	/^import org.apache.spark.storage.{BlockId, BlockManager}$/;"	i
org.apache.spark.storage.{BlockId, FileSegment}	network/netty/ShuffleSender.scala	/^import org.apache.spark.storage.{BlockId, FileSegment}$/;"	i
org.apache.spark.storage.{BlockId, StorageStatus, StorageUtils}	ui/storage/RDDPage.scala	/^import org.apache.spark.storage.{BlockId, StorageStatus, StorageUtils}$/;"	i
org.apache.spark.storage.{BlockManagerMasterActor, BlockManager, BlockManagerMaster}	SparkEnv.scala	/^import org.apache.spark.storage.{BlockManagerMasterActor, BlockManager, BlockManagerMaster}$/;"	i
org.apache.spark.storage.{BlockManagerSource, RDDInfo, StorageStatus, StorageUtils}	SparkContext.scala	/^import org.apache.spark.storage.{BlockManagerSource, RDDInfo, StorageStatus, StorageUtils}$/;"	i
org.apache.spark.storage.{BroadcastBlockId, BroadcastHelperBlockId, StorageLevel}	broadcast/TorrentBroadcast.scala	/^import org.apache.spark.storage.{BroadcastBlockId, BroadcastHelperBlockId, StorageLevel}$/;"	i
org.apache.spark.storage.{BroadcastBlockId, StorageLevel}	broadcast/BitTorrentBroadcast.scala	/^import org.apache.spark.storage.{BroadcastBlockId, StorageLevel}$/;"	i
org.apache.spark.storage.{BroadcastBlockId, StorageLevel}	broadcast/HttpBroadcast.scala	/^import org.apache.spark.storage.{BroadcastBlockId, StorageLevel}$/;"	i
org.apache.spark.storage.{BroadcastBlockId, StorageLevel}	broadcast/TreeBroadcast.scala	/^import org.apache.spark.storage.{BroadcastBlockId, StorageLevel}$/;"	i
org.apache.spark.storage.{RDDInfo, StorageUtils}	ui/storage/IndexPage.scala	/^import org.apache.spark.storage.{RDDInfo, StorageUtils}$/;"	i
org.apache.spark.storage.{StorageLevel, TaskResultBlockId}	executor/Executor.scala	/^import org.apache.spark.storage.{StorageLevel, TaskResultBlockId}$/;"	i
org.apache.spark.storage.{TestBlockId, BlockId}	network/netty/FileHeader.scala	/^import org.apache.spark.storage.{TestBlockId, BlockId}$/;"	i
org.apache.spark.ui	ui/JettyUtils.scala	/^package org.apache.spark.ui$/;"	p
org.apache.spark.ui	ui/Page.scala	/^package org.apache.spark.ui$/;"	p
org.apache.spark.ui	ui/SparkUI.scala	/^package org.apache.spark.ui$/;"	p
org.apache.spark.ui	ui/UIUtils.scala	/^package org.apache.spark.ui$/;"	p
org.apache.spark.ui	ui/UIWorkloadGenerator.scala	/^package org.apache.spark.ui$/;"	p
org.apache.spark.ui.JettyUtils	deploy/master/ui/MasterWebUI.scala	/^import org.apache.spark.ui.JettyUtils$/;"	i
org.apache.spark.ui.JettyUtils	deploy/worker/ui/WorkerWebUI.scala	/^import org.apache.spark.ui.JettyUtils$/;"	i
org.apache.spark.ui.JettyUtils	metrics/sink/MetricsServlet.scala	/^import org.apache.spark.ui.JettyUtils$/;"	i
org.apache.spark.ui.JettyUtils._	deploy/master/ui/MasterWebUI.scala	/^import org.apache.spark.ui.JettyUtils._$/;"	i
org.apache.spark.ui.JettyUtils._	deploy/worker/ui/WorkerWebUI.scala	/^import org.apache.spark.ui.JettyUtils._$/;"	i
org.apache.spark.ui.JettyUtils._	ui/SparkUI.scala	/^import org.apache.spark.ui.JettyUtils._$/;"	i
org.apache.spark.ui.JettyUtils._	ui/env/EnvironmentUI.scala	/^import org.apache.spark.ui.JettyUtils._$/;"	i
org.apache.spark.ui.JettyUtils._	ui/exec/ExecutorsUI.scala	/^import org.apache.spark.ui.JettyUtils._$/;"	i
org.apache.spark.ui.JettyUtils._	ui/jobs/JobProgressUI.scala	/^import org.apache.spark.ui.JettyUtils._$/;"	i
org.apache.spark.ui.JettyUtils._	ui/storage/BlockManagerUI.scala	/^import org.apache.spark.ui.JettyUtils._$/;"	i
org.apache.spark.ui.Page.Environment	ui/env/EnvironmentUI.scala	/^import org.apache.spark.ui.Page.Environment$/;"	i
org.apache.spark.ui.Page.Executors	ui/exec/ExecutorsUI.scala	/^import org.apache.spark.ui.Page.Executors$/;"	i
org.apache.spark.ui.Page._	ui/jobs/IndexPage.scala	/^import org.apache.spark.ui.Page._$/;"	i
org.apache.spark.ui.Page._	ui/jobs/PoolPage.scala	/^import org.apache.spark.ui.Page._$/;"	i
org.apache.spark.ui.Page._	ui/jobs/StagePage.scala	/^import org.apache.spark.ui.Page._$/;"	i
org.apache.spark.ui.Page._	ui/storage/IndexPage.scala	/^import org.apache.spark.ui.Page._$/;"	i
org.apache.spark.ui.Page._	ui/storage/RDDPage.scala	/^import org.apache.spark.ui.Page._$/;"	i
org.apache.spark.ui.SparkUI	SparkContext.scala	/^import org.apache.spark.ui.SparkUI$/;"	i
org.apache.spark.ui.UIUtils	deploy/master/ui/ApplicationPage.scala	/^import org.apache.spark.ui.UIUtils$/;"	i
org.apache.spark.ui.UIUtils	deploy/master/ui/IndexPage.scala	/^import org.apache.spark.ui.UIUtils$/;"	i
org.apache.spark.ui.UIUtils	deploy/worker/ui/IndexPage.scala	/^import org.apache.spark.ui.UIUtils$/;"	i
org.apache.spark.ui.UIUtils	deploy/worker/ui/WorkerWebUI.scala	/^import org.apache.spark.ui.UIUtils$/;"	i
org.apache.spark.ui.UIUtils	ui/env/EnvironmentUI.scala	/^import org.apache.spark.ui.UIUtils$/;"	i
org.apache.spark.ui.UIUtils	ui/exec/ExecutorsUI.scala	/^import org.apache.spark.ui.UIUtils$/;"	i
org.apache.spark.ui.UIUtils	ui/jobs/PoolTable.scala	/^import org.apache.spark.ui.UIUtils$/;"	i
org.apache.spark.ui.UIUtils	ui/jobs/StageTable.scala	/^import org.apache.spark.ui.UIUtils$/;"	i
org.apache.spark.ui.UIUtils._	ui/jobs/IndexPage.scala	/^import org.apache.spark.ui.UIUtils._$/;"	i
org.apache.spark.ui.UIUtils._	ui/jobs/PoolPage.scala	/^import org.apache.spark.ui.UIUtils._$/;"	i
org.apache.spark.ui.UIUtils._	ui/jobs/StagePage.scala	/^import org.apache.spark.ui.UIUtils._$/;"	i
org.apache.spark.ui.UIUtils._	ui/storage/IndexPage.scala	/^import org.apache.spark.ui.UIUtils._$/;"	i
org.apache.spark.ui.UIUtils._	ui/storage/RDDPage.scala	/^import org.apache.spark.ui.UIUtils._$/;"	i
org.apache.spark.ui.env	ui/env/EnvironmentUI.scala	/^package org.apache.spark.ui.env$/;"	p
org.apache.spark.ui.env.EnvironmentUI	ui/SparkUI.scala	/^import org.apache.spark.ui.env.EnvironmentUI$/;"	i
org.apache.spark.ui.exec	ui/exec/ExecutorsUI.scala	/^package org.apache.spark.ui.exec$/;"	p
org.apache.spark.ui.exec.ExecutorsUI	ui/SparkUI.scala	/^import org.apache.spark.ui.exec.ExecutorsUI$/;"	i
org.apache.spark.ui.jobs	ui/jobs/IndexPage.scala	/^package org.apache.spark.ui.jobs$/;"	p
org.apache.spark.ui.jobs	ui/jobs/JobProgressListener.scala	/^package org.apache.spark.ui.jobs$/;"	p
org.apache.spark.ui.jobs	ui/jobs/JobProgressUI.scala	/^package org.apache.spark.ui.jobs$/;"	p
org.apache.spark.ui.jobs	ui/jobs/PoolPage.scala	/^package org.apache.spark.ui.jobs$/;"	p
org.apache.spark.ui.jobs	ui/jobs/PoolTable.scala	/^package org.apache.spark.ui.jobs$/;"	p
org.apache.spark.ui.jobs	ui/jobs/StagePage.scala	/^package org.apache.spark.ui.jobs$/;"	p
org.apache.spark.ui.jobs	ui/jobs/StageTable.scala	/^package org.apache.spark.ui.jobs$/;"	p
org.apache.spark.ui.jobs.JobProgressUI	ui/SparkUI.scala	/^import org.apache.spark.ui.jobs.JobProgressUI$/;"	i
org.apache.spark.ui.storage	ui/storage/BlockManagerUI.scala	/^package org.apache.spark.ui.storage$/;"	p
org.apache.spark.ui.storage	ui/storage/IndexPage.scala	/^package org.apache.spark.ui.storage$/;"	p
org.apache.spark.ui.storage	ui/storage/RDDPage.scala	/^package org.apache.spark.ui.storage$/;"	p
org.apache.spark.ui.storage.BlockManagerUI	ui/SparkUI.scala	/^import org.apache.spark.ui.storage.BlockManagerUI$/;"	i
org.apache.spark.util	util/AkkaUtils.scala	/^package org.apache.spark.util$/;"	p
org.apache.spark.util	util/AppendOnlyMap.scala	/^package org.apache.spark.util$/;"	p
org.apache.spark.util	util/BoundedPriorityQueue.scala	/^package org.apache.spark.util$/;"	p
org.apache.spark.util	util/ByteBufferInputStream.scala	/^package org.apache.spark.util$/;"	p
org.apache.spark.util	util/Clock.scala	/^package org.apache.spark.util$/;"	p
org.apache.spark.util	util/ClosureCleaner.scala	/^package org.apache.spark.util$/;"	p
org.apache.spark.util	util/CompletionIterator.scala	/^package org.apache.spark.util$/;"	p
org.apache.spark.util	util/Distribution.scala	/^package org.apache.spark.util$/;"	p
org.apache.spark.util	util/IdGenerator.scala	/^package org.apache.spark.util$/;"	p
org.apache.spark.util	util/IntParam.scala	/^package org.apache.spark.util$/;"	p
org.apache.spark.util	util/MemoryParam.scala	/^package org.apache.spark.util$/;"	p
org.apache.spark.util	util/MetadataCleaner.scala	/^package org.apache.spark.util$/;"	p
org.apache.spark.util	util/MutablePair.scala	/^package org.apache.spark.util$/;"	p
org.apache.spark.util	util/NextIterator.scala	/^package org.apache.spark.util$/;"	p
org.apache.spark.util	util/RateLimitedOutputStream.scala	/^package org.apache.spark.util$/;"	p
org.apache.spark.util	util/SerializableBuffer.scala	/^package org.apache.spark.util$/;"	p
org.apache.spark.util	util/SizeEstimator.scala	/^package org.apache.spark.util$/;"	p
org.apache.spark.util	util/StatCounter.scala	/^package org.apache.spark.util$/;"	p
org.apache.spark.util	util/TimeStampedHashMap.scala	/^package org.apache.spark.util$/;"	p
org.apache.spark.util	util/TimeStampedHashSet.scala	/^package org.apache.spark.util$/;"	p
org.apache.spark.util	util/Utils.scala	/^package org.apache.spark.util$/;"	p
org.apache.spark.util	util/Vector.scala	/^package org.apache.spark.util$/;"	p
org.apache.spark.util.AppendOnlyMap	Aggregator.scala	/^import org.apache.spark.util.AppendOnlyMap$/;"	i
org.apache.spark.util.AppendOnlyMap	rdd/CoGroupedRDD.scala	/^import org.apache.spark.util.AppendOnlyMap$/;"	i
org.apache.spark.util.ByteBufferInputStream	scheduler/Task.scala	/^import org.apache.spark.util.ByteBufferInputStream$/;"	i
org.apache.spark.util.ByteBufferInputStream	serializer/JavaSerializer.scala	/^import org.apache.spark.util.ByteBufferInputStream$/;"	i
org.apache.spark.util.CompletionIterator	BlockStoreShuffleFetcher.scala	/^import org.apache.spark.util.CompletionIterator$/;"	i
org.apache.spark.util.NextIterator	rdd/HadoopRDD.scala	/^import org.apache.spark.util.NextIterator$/;"	i
org.apache.spark.util.NextIterator	rdd/JdbcRDD.scala	/^import org.apache.spark.util.NextIterator$/;"	i
org.apache.spark.util.SerializableBuffer	scheduler/TaskDescription.scala	/^import org.apache.spark.util.SerializableBuffer$/;"	i
org.apache.spark.util.StatCounter	api/java/JavaDoubleRDD.scala	/^import org.apache.spark.util.StatCounter$/;"	i
org.apache.spark.util.StatCounter	partial/GroupedMeanEvaluator.scala	/^import org.apache.spark.util.StatCounter$/;"	i
org.apache.spark.util.StatCounter	partial/GroupedSumEvaluator.scala	/^import org.apache.spark.util.StatCounter$/;"	i
org.apache.spark.util.StatCounter	partial/MeanEvaluator.scala	/^import org.apache.spark.util.StatCounter$/;"	i
org.apache.spark.util.StatCounter	partial/SumEvaluator.scala	/^import org.apache.spark.util.StatCounter$/;"	i
org.apache.spark.util.StatCounter	rdd/DoubleRDDFunctions.scala	/^import org.apache.spark.util.StatCounter$/;"	i
org.apache.spark.util.Utils	HttpFileServer.scala	/^import org.apache.spark.util.Utils$/;"	i
org.apache.spark.util.Utils	HttpServer.scala	/^import org.apache.spark.util.Utils$/;"	i
org.apache.spark.util.Utils	Partitioner.scala	/^import org.apache.spark.util.Utils$/;"	i
org.apache.spark.util.Utils	api/python/PythonPartitioner.scala	/^import org.apache.spark.util.Utils$/;"	i
org.apache.spark.util.Utils	api/python/PythonRDD.scala	/^import org.apache.spark.util.Utils$/;"	i
org.apache.spark.util.Utils	broadcast/BitTorrentBroadcast.scala	/^import org.apache.spark.util.Utils$/;"	i
org.apache.spark.util.Utils	broadcast/MultiTracker.scala	/^import org.apache.spark.util.Utils$/;"	i
org.apache.spark.util.Utils	broadcast/TorrentBroadcast.scala	/^import org.apache.spark.util.Utils$/;"	i
org.apache.spark.util.Utils	broadcast/TreeBroadcast.scala	/^import org.apache.spark.util.Utils$/;"	i
org.apache.spark.util.Utils	deploy/DeployMessage.scala	/^import org.apache.spark.util.Utils$/;"	i
org.apache.spark.util.Utils	deploy/LocalSparkCluster.scala	/^import org.apache.spark.util.Utils$/;"	i
org.apache.spark.util.Utils	deploy/master/WorkerInfo.scala	/^import org.apache.spark.util.Utils$/;"	i
org.apache.spark.util.Utils	deploy/master/ui/ApplicationPage.scala	/^import org.apache.spark.util.Utils$/;"	i
org.apache.spark.util.Utils	deploy/master/ui/IndexPage.scala	/^import org.apache.spark.util.Utils$/;"	i
org.apache.spark.util.Utils	deploy/master/ui/MasterWebUI.scala	/^import org.apache.spark.util.Utils$/;"	i
org.apache.spark.util.Utils	deploy/worker/ExecutorRunner.scala	/^import org.apache.spark.util.Utils$/;"	i
org.apache.spark.util.Utils	deploy/worker/ui/IndexPage.scala	/^import org.apache.spark.util.Utils$/;"	i
org.apache.spark.util.Utils	deploy/worker/ui/WorkerWebUI.scala	/^import org.apache.spark.util.Utils$/;"	i
org.apache.spark.util.Utils	executor/Executor.scala	/^import org.apache.spark.util.Utils$/;"	i
org.apache.spark.util.Utils	executor/MesosExecutorBackend.scala	/^import org.apache.spark.util.Utils$/;"	i
org.apache.spark.util.Utils	network/ConnectionManager.scala	/^import org.apache.spark.util.Utils$/;"	i
org.apache.spark.util.Utils	network/ConnectionManagerId.scala	/^import org.apache.spark.util.Utils$/;"	i
org.apache.spark.util.Utils	network/netty/ShuffleSender.scala	/^import org.apache.spark.util.Utils$/;"	i
org.apache.spark.util.Utils	rdd/ParallelCollectionRDD.scala	/^import org.apache.spark.util.Utils$/;"	i
org.apache.spark.util.Utils	scheduler/TaskInfo.scala	/^import org.apache.spark.util.Utils$/;"	i
org.apache.spark.util.Utils	scheduler/TaskResult.scala	/^import org.apache.spark.util.Utils$/;"	i
org.apache.spark.util.Utils	scheduler/cluster/CoarseGrainedSchedulerBackend.scala	/^import org.apache.spark.util.Utils$/;"	i
org.apache.spark.util.Utils	scheduler/cluster/SparkDeploySchedulerBackend.scala	/^import org.apache.spark.util.Utils$/;"	i
org.apache.spark.util.Utils	scheduler/cluster/TaskResultGetter.scala	/^import org.apache.spark.util.Utils$/;"	i
org.apache.spark.util.Utils	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^import org.apache.spark.util.Utils$/;"	i
org.apache.spark.util.Utils	storage/BlockFetcherIterator.scala	/^import org.apache.spark.util.Utils$/;"	i
org.apache.spark.util.Utils	storage/BlockManagerId.scala	/^import org.apache.spark.util.Utils$/;"	i
org.apache.spark.util.Utils	storage/BlockManagerMasterActor.scala	/^import org.apache.spark.util.Utils$/;"	i
org.apache.spark.util.Utils	storage/BlockManagerWorker.scala	/^import org.apache.spark.util.Utils$/;"	i
org.apache.spark.util.Utils	storage/DiskBlockManager.scala	/^import org.apache.spark.util.Utils$/;"	i
org.apache.spark.util.Utils	storage/DiskStore.scala	/^import org.apache.spark.util.Utils$/;"	i
org.apache.spark.util.Utils	storage/StoragePerfTester.scala	/^import org.apache.spark.util.Utils$/;"	i
org.apache.spark.util.Utils	storage/StorageUtils.scala	/^import org.apache.spark.util.Utils$/;"	i
org.apache.spark.util.Utils	ui/SparkUI.scala	/^import org.apache.spark.util.Utils$/;"	i
org.apache.spark.util.Utils	ui/exec/ExecutorsUI.scala	/^import org.apache.spark.util.Utils$/;"	i
org.apache.spark.util.Utils	ui/jobs/JobProgressUI.scala	/^import org.apache.spark.util.Utils$/;"	i
org.apache.spark.util.Utils	ui/jobs/StageTable.scala	/^import org.apache.spark.util.Utils$/;"	i
org.apache.spark.util.Utils	ui/storage/IndexPage.scala	/^import org.apache.spark.util.Utils$/;"	i
org.apache.spark.util.Utils	ui/storage/RDDPage.scala	/^import org.apache.spark.util.Utils$/;"	i
org.apache.spark.util._	storage/BlockManager.scala	/^import org.apache.spark.util._$/;"	i
org.apache.spark.util.collection	util/collection/BitSet.scala	/^package org.apache.spark.util.collection$/;"	p
org.apache.spark.util.collection	util/collection/OpenHashMap.scala	/^package org.apache.spark.util.collection$/;"	p
org.apache.spark.util.collection	util/collection/OpenHashSet.scala	/^package org.apache.spark.util.collection$/;"	p
org.apache.spark.util.collection	util/collection/PrimitiveKeyOpenHashMap.scala	/^package org.apache.spark.util.collection$/;"	p
org.apache.spark.util.collection	util/collection/PrimitiveVector.scala	/^package org.apache.spark.util.collection$/;"	p
org.apache.spark.util.collection.{PrimitiveKeyOpenHashMap, PrimitiveVector}	storage/ShuffleBlockManager.scala	/^import org.apache.spark.util.collection.{PrimitiveKeyOpenHashMap, PrimitiveVector}$/;"	i
org.apache.spark.util.{AkkaUtils, Utils}	deploy/master/Master.scala	/^import org.apache.spark.util.{AkkaUtils, Utils}$/;"	i
org.apache.spark.util.{ClosureCleaner, MetadataCleaner, MetadataCleanerType,	SparkContext.scala	/^import org.apache.spark.util.{ClosureCleaner, MetadataCleaner, MetadataCleanerType,$/;"	i
org.apache.spark.util.{MetadataCleaner, MetadataCleanerType, TimeStampedHashMap}	scheduler/DAGScheduler.scala	/^import org.apache.spark.util.{MetadataCleaner, MetadataCleanerType, TimeStampedHashMap}$/;"	i
org.apache.spark.util.{MetadataCleaner, MetadataCleanerType, TimeStampedHashSet, Utils}	broadcast/HttpBroadcast.scala	/^import org.apache.spark.util.{MetadataCleaner, MetadataCleanerType, TimeStampedHashSet, Utils}$/;"	i
org.apache.spark.util.{MetadataCleanerType, MetadataCleaner, TimeStampedHashMap}	scheduler/ResultTask.scala	/^import org.apache.spark.util.{MetadataCleanerType, MetadataCleaner, TimeStampedHashMap}$/;"	i
org.apache.spark.util.{MetadataCleanerType, MetadataCleaner, TimeStampedHashMap}	storage/ShuffleBlockManager.scala	/^import org.apache.spark.util.{MetadataCleanerType, MetadataCleaner, TimeStampedHashMap}$/;"	i
org.apache.spark.util.{MetadataCleanerType, TimeStampedHashMap, MetadataCleaner}	scheduler/ShuffleMapTask.scala	/^import org.apache.spark.util.{MetadataCleanerType, TimeStampedHashMap, MetadataCleaner}$/;"	i
org.apache.spark.util.{MetadataCleanerType, Utils, MetadataCleaner, TimeStampedHashMap}	MapOutputTracker.scala	/^import org.apache.spark.util.{MetadataCleanerType, Utils, MetadataCleaner, TimeStampedHashMap}$/;"	i
org.apache.spark.util.{NextIterator, ByteBufferInputStream}	serializer/Serializer.scala	/^import org.apache.spark.util.{NextIterator, ByteBufferInputStream}$/;"	i
org.apache.spark.util.{SizeEstimator, Utils}	storage/MemoryStore.scala	/^import org.apache.spark.util.{SizeEstimator, Utils}$/;"	i
org.apache.spark.util.{SystemClock, Clock}	scheduler/cluster/ClusterTaskSetManager.scala	/^import org.apache.spark.util.{SystemClock, Clock}$/;"	i
org.apache.spark.util.{Utils, AkkaUtils}	SparkEnv.scala	/^import org.apache.spark.util.{Utils, AkkaUtils}$/;"	i
org.apache.spark.util.{Utils, AkkaUtils}	deploy/client/TestClient.scala	/^import org.apache.spark.util.{Utils, AkkaUtils}$/;"	i
org.apache.spark.util.{Utils, AkkaUtils}	deploy/worker/Worker.scala	/^import org.apache.spark.util.{Utils, AkkaUtils}$/;"	i
org.apache.spark.util.{Utils, AkkaUtils}	executor/CoarseGrainedExecutorBackend.scala	/^import org.apache.spark.util.{Utils, AkkaUtils}$/;"	i
org.apache.spark.util.{Utils, BoundedPriorityQueue}	rdd/RDD.scala	/^import org.apache.spark.util.{Utils, BoundedPriorityQueue}$/;"	i
org.apache.spark.util.{Utils, Distribution}	scheduler/SparkListener.scala	/^import org.apache.spark.util.{Utils, Distribution}$/;"	i
org.apache.spark.util.{Utils, Distribution}	ui/jobs/StagePage.scala	/^import org.apache.spark.util.{Utils, Distribution}$/;"	i
org.apache.spark.util.{Utils, IntParam, MemoryParam}	deploy/worker/WorkerArguments.scala	/^import org.apache.spark.util.{Utils, IntParam, MemoryParam}$/;"	i
org.apache.spark.util.{Utils, IntParam}	deploy/master/MasterArguments.scala	/^import org.apache.spark.util.{Utils, IntParam}$/;"	i
org.apache.spark.util.{Utils, SerializableBuffer}	scheduler/cluster/CoarseGrainedClusterMessage.scala	/^import org.apache.spark.util.{Utils, SerializableBuffer}$/;"	i
org.apache.spark.{Accumulable, AccumulableParam, Accumulator, AccumulatorParam, SparkContext}	api/java/JavaSparkContext.scala	/^import org.apache.spark.{Accumulable, AccumulableParam, Accumulator, AccumulatorParam, SparkContext}$/;"	i
org.apache.spark.{ComplexFutureAction, FutureAction, Logging}	rdd/AsyncRDDActions.scala	/^import org.apache.spark.{ComplexFutureAction, FutureAction, Logging}$/;"	i
org.apache.spark.{Dependency, OneToOneDependency, ShuffleDependency}	rdd/CoGroupedRDD.scala	/^import org.apache.spark.{Dependency, OneToOneDependency, ShuffleDependency}$/;"	i
org.apache.spark.{Dependency, Partitioner, SparkEnv, ShuffleDependency, Partition, TaskContext}	rdd/ShuffledRDD.scala	/^import org.apache.spark.{Dependency, Partitioner, SparkEnv, ShuffleDependency, Partition, TaskContext}$/;"	i
org.apache.spark.{Dependency, RangeDependency, SparkContext, Partition, TaskContext}	rdd/UnionRDD.scala	/^import org.apache.spark.{Dependency, RangeDependency, SparkContext, Partition, TaskContext}$/;"	i
org.apache.spark.{ExceptionFailure, FetchFailed, Logging, Resubmitted, SparkEnv,	scheduler/cluster/ClusterTaskSetManager.scala	/^import org.apache.spark.{ExceptionFailure, FetchFailed, Logging, Resubmitted, SparkEnv,$/;"	i
org.apache.spark.{ExceptionFailure, Logging, SparkContext}	ui/exec/ExecutorsUI.scala	/^import org.apache.spark.{ExceptionFailure, Logging, SparkContext}$/;"	i
org.apache.spark.{ExceptionFailure, Logging, SparkEnv, SparkException, Success, TaskState}	scheduler/local/LocalTaskSetManager.scala	/^import org.apache.spark.{ExceptionFailure, Logging, SparkEnv, SparkException, Success, TaskState}$/;"	i
org.apache.spark.{ExceptionFailure, SparkContext, Success}	ui/jobs/JobProgressListener.scala	/^import org.apache.spark.{ExceptionFailure, SparkContext, Success}$/;"	i
org.apache.spark.{ExceptionFailure, SparkContext, Success}	ui/jobs/JobProgressUI.scala	/^import org.apache.spark.{ExceptionFailure, SparkContext, Success}$/;"	i
org.apache.spark.{ExceptionFailure}	ui/jobs/StagePage.scala	/^import org.apache.spark.{ExceptionFailure}$/;"	i
org.apache.spark.{HttpServer, Logging, SparkEnv}	broadcast/HttpBroadcast.scala	/^import org.apache.spark.{HttpServer, Logging, SparkEnv}$/;"	i
org.apache.spark.{InterruptibleIterator, Logging, Partition, SerializableWritable, SparkContext, TaskContext}	rdd/NewHadoopRDD.scala	/^import org.apache.spark.{InterruptibleIterator, Logging, Partition, SerializableWritable, SparkContext, TaskContext}$/;"	i
org.apache.spark.{InterruptibleIterator, Partition, Partitioner, SparkEnv, TaskContext}	rdd/CoGroupedRDD.scala	/^import org.apache.spark.{InterruptibleIterator, Partition, Partitioner, SparkEnv, TaskContext}$/;"	i
org.apache.spark.{Logging, Partition, SparkContext, TaskContext}	rdd/JdbcRDD.scala	/^import org.apache.spark.{Logging, Partition, SparkContext, TaskContext}$/;"	i
org.apache.spark.{Logging, SparkContext, SparkEnv}	ui/SparkUI.scala	/^import org.apache.spark.{Logging, SparkContext, SparkEnv}$/;"	i
org.apache.spark.{Logging, SparkContext, TaskEndReason}	scheduler/SparkListener.scala	/^import org.apache.spark.{Logging, SparkContext, TaskEndReason}$/;"	i
org.apache.spark.{Logging, SparkContext}	deploy/FaultToleranceTest.scala	/^import org.apache.spark.{Logging, SparkContext}$/;"	i
org.apache.spark.{Logging, SparkContext}	scheduler/cluster/SimrSchedulerBackend.scala	/^import org.apache.spark.{Logging, SparkContext}$/;"	i
org.apache.spark.{Logging, SparkContext}	scheduler/cluster/SparkDeploySchedulerBackend.scala	/^import org.apache.spark.{Logging, SparkContext}$/;"	i
org.apache.spark.{Logging, SparkContext}	ui/storage/BlockManagerUI.scala	/^import org.apache.spark.{Logging, SparkContext}$/;"	i
org.apache.spark.{Logging, SparkEnv, SparkException}	storage/BlockManager.scala	/^import org.apache.spark.{Logging, SparkEnv, SparkException}$/;"	i
org.apache.spark.{Logging, SparkEnv}	scheduler/InputFormatInfo.scala	/^import org.apache.spark.{Logging, SparkEnv}$/;"	i
org.apache.spark.{Logging, SparkException, SparkContext, TaskState}	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^import org.apache.spark.{Logging, SparkException, SparkContext, TaskState}$/;"	i
org.apache.spark.{Logging, SparkException}	deploy/master/Master.scala	/^import org.apache.spark.{Logging, SparkException}$/;"	i
org.apache.spark.{Logging, SparkException}	storage/BlockManagerMaster.scala	/^import org.apache.spark.{Logging, SparkException}$/;"	i
org.apache.spark.{Logging, SparkException}	storage/BlockManagerMasterActor.scala	/^import org.apache.spark.{Logging, SparkException}$/;"	i
org.apache.spark.{Logging}	deploy/client/TestClient.scala	/^import org.apache.spark.{Logging}$/;"	i
org.apache.spark.{Logging}	deploy/master/ui/MasterWebUI.scala	/^import org.apache.spark.{Logging}$/;"	i
org.apache.spark.{Logging}	deploy/worker/ExecutorRunner.scala	/^import org.apache.spark.{Logging}$/;"	i
org.apache.spark.{Logging}	deploy/worker/ui/WorkerWebUI.scala	/^import org.apache.spark.{Logging}$/;"	i
org.apache.spark.{Logging}	storage/BlockManagerWorker.scala	/^import org.apache.spark.{Logging}$/;"	i
org.apache.spark.{NarrowDependency, SparkEnv, Partition, TaskContext}	rdd/PartitionPruningRDD.scala	/^import org.apache.spark.{NarrowDependency, SparkEnv, Partition, TaskContext}$/;"	i
org.apache.spark.{OneToOneDependency, Partition, TaskContext}	rdd/FilteredRDD.scala	/^import org.apache.spark.{OneToOneDependency, Partition, TaskContext}$/;"	i
org.apache.spark.{OneToOneDependency, SparkContext, Partition, TaskContext}	rdd/ZippedPartitionsRDD.scala	/^import org.apache.spark.{OneToOneDependency, SparkContext, Partition, TaskContext}$/;"	i
org.apache.spark.{OneToOneDependency, SparkContext, Partition, TaskContext}	rdd/ZippedRDD.scala	/^import org.apache.spark.{OneToOneDependency, SparkContext, Partition, TaskContext}$/;"	i
org.apache.spark.{Partition, SparkException, Logging}	rdd/RDDCheckpointData.scala	/^import org.apache.spark.{Partition, SparkException, Logging}$/;"	i
org.apache.spark.{Partition, TaskContext}	rdd/FlatMappedRDD.scala	/^import org.apache.spark.{Partition, TaskContext}$/;"	i
org.apache.spark.{Partition, TaskContext}	rdd/GlommedRDD.scala	/^import org.apache.spark.{Partition, TaskContext}$/;"	i
org.apache.spark.{Partition, TaskContext}	rdd/MapPartitionsRDD.scala	/^import org.apache.spark.{Partition, TaskContext}$/;"	i
org.apache.spark.{Partition, TaskContext}	rdd/MappedRDD.scala	/^import org.apache.spark.{Partition, TaskContext}$/;"	i
org.apache.spark.{Partition, TaskContext}	rdd/SampledRDD.scala	/^import org.apache.spark.{Partition, TaskContext}$/;"	i
org.apache.spark.{RangePartitioner, Logging}	rdd/OrderedRDDFunctions.scala	/^import org.apache.spark.{RangePartitioner, Logging}$/;"	i
org.apache.spark.{SerializableWritable, Logging}	serializer/KryoSerializer.scala	/^import org.apache.spark.{SerializableWritable, Logging}$/;"	i
org.apache.spark.{SparkContext, Partition, TaskContext}	api/java/JavaRDDLike.scala	/^import org.apache.spark.{SparkContext, Partition, TaskContext}$/;"	i
org.apache.spark.{SparkContext, SparkEnv, Partition, TaskContext}	rdd/BlockRDD.scala	/^import org.apache.spark.{SparkContext, SparkEnv, Partition, TaskContext}$/;"	i
org.apache.spark.{SparkContext, SparkEnv, Partition, TaskContext}	rdd/EmptyRDD.scala	/^import org.apache.spark.{SparkContext, SparkEnv, Partition, TaskContext}$/;"	i
org.apache.spark.{SparkContext, SparkException}	deploy/SparkHadoopUtil.scala	/^import org.apache.spark.{SparkContext, SparkException}$/;"	i
org.apache.spark.{SparkContext}	storage/StorageUtils.scala	/^import org.apache.spark.{SparkContext}$/;"	i
org.apache.spark.{SparkEnv, Partition, TaskContext}	rdd/PipedRDD.scala	/^import org.apache.spark.{SparkEnv, Partition, TaskContext}$/;"	i
org.apache.spark.{SparkEnv}	scheduler/TaskResult.scala	/^import org.apache.spark.{SparkEnv}$/;"	i
org.apache.spark.{SparkException, Logging, SparkContext, TaskState}	scheduler/cluster/mesos/CoarseMesosSchedulerBackend.scala	/^import org.apache.spark.{SparkException, Logging, SparkContext, TaskState}$/;"	i
org.apache.spark.{SparkException, Logging, TaskState}	scheduler/cluster/CoarseGrainedSchedulerBackend.scala	/^import org.apache.spark.{SparkException, Logging, TaskState}$/;"	i
org.apache.spark.{SparkException, Logging}	util/Utils.scala	/^import org.apache.spark.{SparkException, Logging}$/;"	i
org.apache.spark.{TaskContext, Logging}	rdd/DoubleRDDFunctions.scala	/^import org.apache.spark.{TaskContext, Logging}$/;"	i
org.apache.spark.{TaskContext, Partition}	rdd/FlatMappedValuesRDD.scala	/^import org.apache.spark.{TaskContext, Partition}$/;"	i
org.apache.spark.{TaskContext, Partition}	rdd/MappedValuesRDD.scala	/^import org.apache.spark.{TaskContext, Partition}$/;"	i
org.apache.zookeeper.Watcher.Event.EventType	deploy/master/ZooKeeperLeaderElectionAgent.scala	/^import org.apache.zookeeper.Watcher.Event.EventType$/;"	i
org.apache.zookeeper.Watcher.Event.KeeperState	deploy/master/SparkZooKeeperSession.scala	/^import org.apache.zookeeper.Watcher.Event.KeeperState$/;"	i
org.apache.zookeeper._	deploy/master/SparkZooKeeperSession.scala	/^import org.apache.zookeeper._$/;"	i
org.apache.zookeeper._	deploy/master/ZooKeeperLeaderElectionAgent.scala	/^import org.apache.zookeeper._$/;"	i
org.apache.zookeeper._	deploy/master/ZooKeeperPersistenceEngine.scala	/^import org.apache.zookeeper._$/;"	i
org.apache.zookeeper.data.Stat	deploy/master/SparkZooKeeperSession.scala	/^import org.apache.zookeeper.data.Stat$/;"	i
org.eclipse.jetty.server.Handler	metrics/sink/MetricsServlet.scala	/^import org.eclipse.jetty.server.Handler$/;"	i
org.eclipse.jetty.server.Handler	ui/env/EnvironmentUI.scala	/^import org.eclipse.jetty.server.Handler$/;"	i
org.eclipse.jetty.server.Handler	ui/exec/ExecutorsUI.scala	/^import org.eclipse.jetty.server.Handler$/;"	i
org.eclipse.jetty.server.Handler	ui/jobs/JobProgressUI.scala	/^import org.eclipse.jetty.server.Handler$/;"	i
org.eclipse.jetty.server.Handler	ui/storage/BlockManagerUI.scala	/^import org.eclipse.jetty.server.Handler$/;"	i
org.eclipse.jetty.server.Server	HttpServer.scala	/^import org.eclipse.jetty.server.Server$/;"	i
org.eclipse.jetty.server.bio.SocketConnector	HttpServer.scala	/^import org.eclipse.jetty.server.bio.SocketConnector$/;"	i
org.eclipse.jetty.server.handler.DefaultHandler	HttpServer.scala	/^import org.eclipse.jetty.server.handler.DefaultHandler$/;"	i
org.eclipse.jetty.server.handler.HandlerList	HttpServer.scala	/^import org.eclipse.jetty.server.handler.HandlerList$/;"	i
org.eclipse.jetty.server.handler.ResourceHandler	HttpServer.scala	/^import org.eclipse.jetty.server.handler.ResourceHandler$/;"	i
org.eclipse.jetty.server.handler.{ResourceHandler, HandlerList, ContextHandler, AbstractHandler}	ui/JettyUtils.scala	/^import org.eclipse.jetty.server.handler.{ResourceHandler, HandlerList, ContextHandler, AbstractHandler}$/;"	i
org.eclipse.jetty.server.{Handler, Server}	deploy/master/ui/MasterWebUI.scala	/^import org.eclipse.jetty.server.{Handler, Server}$/;"	i
org.eclipse.jetty.server.{Handler, Server}	deploy/worker/ui/WorkerWebUI.scala	/^import org.eclipse.jetty.server.{Handler, Server}$/;"	i
org.eclipse.jetty.server.{Handler, Server}	ui/SparkUI.scala	/^import org.eclipse.jetty.server.{Handler, Server}$/;"	i
org.eclipse.jetty.server.{Server, Request, Handler}	ui/JettyUtils.scala	/^import org.eclipse.jetty.server.{Server, Request, Handler}$/;"	i
org.eclipse.jetty.util.thread.QueuedThreadPool	HttpServer.scala	/^import org.eclipse.jetty.util.thread.QueuedThreadPool$/;"	i
org.eclipse.jetty.util.thread.QueuedThreadPool	ui/JettyUtils.scala	/^import org.eclipse.jetty.util.thread.QueuedThreadPool$/;"	i
org.objectweb.asm.Opcodes._	util/ClosureCleaner.scala	/^import org.objectweb.asm.Opcodes._$/;"	i
org.objectweb.asm.{ClassReader, ClassVisitor, MethodVisitor, Type}	util/ClosureCleaner.scala	/^import org.objectweb.asm.{ClassReader, ClassVisitor, MethodVisitor, Type}$/;"	i
org.slf4j.Logger	Logging.scala	/^import org.slf4j.Logger$/;"	i
org.slf4j.LoggerFactory	Logging.scala	/^import org.slf4j.LoggerFactory$/;"	i
org.xerial.snappy.{SnappyInputStream, SnappyOutputStream}	io/CompressionCodec.scala	/^import org.xerial.snappy.{SnappyInputStream, SnappyOutputStream}$/;"	i
origin	rdd/RDD.scala	/^  @transient private[spark] val origin = Utils.formatSparkCallSite$/;"	V
originalLevel	storage/BlockManagerMasterActor.scala	/^        val originalLevel: StorageLevel = _blocks.get(blockId).storageLevel$/;"	V
originals	Accumulators.scala	/^  val originals = Map[Long, Accumulable[_, _]]()$/;"	V
osWrapper	util/Utils.scala	/^    val osWrapper = ser.serializeStream(new OutputStream {$/;"	V
other	network/MessageChunkHeader.scala	/^    val other = buffer.getInt()$/;"	V
other	network/MessageChunkHeader.scala	/^    val other: Int,$/;"	V
other	scheduler/SparkListener.scala	/^    val other = 1.0 - (exec + fetch.getOrElse(0d))$/;"	V
other	util/StatCounter.scala	/^    val other = new StatCounter$/;"	V
otherProperties	ui/env/EnvironmentUI.scala	/^    val otherProperties = properties.diff(sparkProperties :+ classPathProperty).sorted$/;"	V
otherPropertyTable	ui/env/EnvironmentUI.scala	/^    val otherPropertyTable =$/;"	V
out	MapOutputTracker.scala	/^    val out = new ByteArrayOutputStream$/;"	V
out	api/python/PythonRDD.scala	/^      val out = new DataOutputStream(new BufferedOutputStream(socket.getOutputStream, bufferSize))$/;"	V
out	api/python/PythonWorkerFactory.scala	/^      val out = new OutputStreamWriter(worker.getOutputStream)$/;"	V
out	broadcast/HttpBroadcast.scala	/^    val out: OutputStream = {$/;"	V
out	deploy/master/FileSystemPersistenceEngine.scala	/^    val out = new FileOutputStream(file)$/;"	V
out	deploy/worker/ExecutorRunner.scala	/^    val out = new FileOutputStream(file, true)$/;"	V
out	rdd/PipedRDD.scala	/^        val out = new PrintWriter(proc.getOutputStream)$/;"	V
out	scheduler/ResultTask.scala	/^        val out = new ByteArrayOutputStream$/;"	V
out	scheduler/ShuffleMapTask.scala	/^        val out = new ByteArrayOutputStream$/;"	V
out	scheduler/Task.scala	/^    val out = new FastByteArrayOutputStream(4096)$/;"	V
out	serializer/JavaSerializer.scala	/^    val out = serializeStream(bos)$/;"	V
out	util/Utils.scala	/^        val out = new FileOutputStream(tempFile)$/;"	V
outFile	deploy/FaultToleranceTest.scala	/^    val outFile = File.createTempFile("fault-tolerance-test", "")$/;"	V
outStream	deploy/FaultToleranceTest.scala	/^    val outStream: FileWriter = new FileWriter(outFile)$/;"	V
outbox	network/Connection.scala	/^  private val outbox = new Outbox(1)$/;"	V
outer	util/ClosureCleaner.scala	/^    var outer: AnyRef = null$/;"	v
outerClasses	util/ClosureCleaner.scala	/^    val outerClasses = getOuterClasses(func)$/;"	V
outerObjects	util/ClosureCleaner.scala	/^    val outerObjects = getOuterObjects(func)$/;"	V
outerPairs	util/ClosureCleaner.scala	/^    var outerPairs: List[(Class[_], AnyRef)] = (outerClasses zip outerObjects).reverse$/;"	v
output	serializer/KryoSerializer.scala	/^  lazy val output = ks.newKryoOutput()$/;"	V
output	serializer/KryoSerializer.scala	/^  val output = new KryoOutput(outStream)$/;"	V
output	util/Utils.scala	/^    val output = new StringBuffer$/;"	V
outputDir	rdd/CheckpointRDD.scala	/^    val outputDir = new Path(path)$/;"	V
outputFormatClass	rdd/PairRDDFunctions.scala	/^    val outputFormatClass = conf.getOutputFormat$/;"	V
outputId	scheduler/ResultTask.scala	/^    var outputId: Int)$/;"	v
outputLocs	scheduler/Stage.scala	/^  val outputLocs = Array.fill[List[MapStatus]](numPartitions)(Nil)$/;"	V
outputName	SparkHadoopWriter.scala	/^    val outputName = "part-"  + numfmt.format(splitID)$/;"	V
outputPath	SparkHadoopWriter.scala	/^    var outputPath = new Path(path)$/;"	v
outputStream	storage/DiskStore.scala	/^    val outputStream = new FileOutputStream(file)$/;"	V
outputsMerged	partial/CountEvaluator.scala	/^  var outputsMerged = 0$/;"	v
outputsMerged	partial/GroupedCountEvaluator.scala	/^  var outputsMerged = 0$/;"	v
outputsMerged	partial/GroupedMeanEvaluator.scala	/^  var outputsMerged = 0$/;"	v
outputsMerged	partial/GroupedSumEvaluator.scala	/^  var outputsMerged = 0$/;"	v
outputsMerged	partial/MeanEvaluator.scala	/^  var outputsMerged = 0$/;"	v
outputsMerged	partial/SumEvaluator.scala	/^  var outputsMerged = 0$/;"	v
ow	SerializableWritable.scala	/^    val ow = new ObjectWritable()$/;"	V
p	FutureAction.scala	/^  private val p = promise[T]()$/;"	V
p	partial/CountEvaluator.scala	/^      val p = outputsMerged.toDouble \/ totalOutputs$/;"	V
p	partial/GroupedCountEvaluator.scala	/^      val p = outputsMerged.toDouble \/ totalOutputs$/;"	V
p	partial/GroupedMeanEvaluator.scala	/^      val p = outputsMerged.toDouble \/ totalOutputs$/;"	V
p	partial/GroupedSumEvaluator.scala	/^      val p = outputsMerged.toDouble \/ totalOutputs$/;"	V
p	partial/SumEvaluator.scala	/^      val p = outputsMerged.toDouble \/ totalOutputs$/;"	V
p	rdd/AsyncRDDActions.scala	/^        val p = partsScanned until math.min(partsScanned + numPartsToTry, totalParts)$/;"	V
p	rdd/RDD.scala	/^      val p = partsScanned until math.min(partsScanned + numPartsToTry, totalParts)$/;"	V
p2	rdd/RDD.scala	/^      val p2 = new Partitioner() {$/;"	V
pB	storage/BlockManagerWorker.scala	/^        val pB = PutBlock(blockMessage.getId, blockMessage.getData, blockMessage.getLevel)$/;"	V
pB	storage/BlockMessageArray.scala	/^          val pB = PutBlock(blockMessage.getId, blockMessage.getData, blockMessage.getLevel)$/;"	V
pResolver	network/netty/ShuffleSender.scala	/^private[spark] class ShuffleSender(portIn: Int, val pResolver: PathResolver) extends Logging {$/;"	V
pResovler	network/netty/ShuffleSender.scala	/^    val pResovler = new PathResolver {$/;"	V
pair	scheduler/ShuffleMapTask.scala	/^        val pair = elem.asInstanceOf[Product2[Any, Any]]$/;"	V
pair	storage/MemoryStore.scala	/^          val pair = iterator.next()$/;"	V
pair	util/collection/OpenHashMap.scala	/^      val pair = nextPair$/;"	V
pair	util/collection/PrimitiveKeyOpenHashMap.scala	/^      val pair = nextPair$/;"	V
parallelize	SparkContext.scala	/^  def parallelize[T: ClassManifest](seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] = {$/;"	m
parallelize	api/java/JavaSparkContext.scala	/^  def parallelize[T](list: java.util.List[T]): JavaRDD[T] =$/;"	m
parallelize	api/java/JavaSparkContext.scala	/^  def parallelize[T](list: java.util.List[T], numSlices: Int): JavaRDD[T] = {$/;"	m
parallelizeDoubles	api/java/JavaSparkContext.scala	/^  def parallelizeDoubles(list: java.util.List[java.lang.Double]): JavaDoubleRDD =$/;"	m
parallelizeDoubles	api/java/JavaSparkContext.scala	/^  def parallelizeDoubles(list: java.util.List[java.lang.Double], numSlices: Int): JavaDoubleRDD =$/;"	m
parallelizePairs	api/java/JavaSparkContext.scala	/^  def parallelizePairs[K, V](list: java.util.List[Tuple2[K, V]]): JavaPairRDD[K, V] =$/;"	m
parallelizePairs	api/java/JavaSparkContext.scala	/^  def parallelizePairs[K, V](list: java.util.List[Tuple2[K, V]], numSlices: Int)$/;"	m
param	SparkContext.scala	/^    val param = new GrowableAccumulableParam[R,T]$/;"	V
params	util/ClosureCleaner.scala	/^      val params = cons.getParameterTypes.map(createNullValue).toArray$/;"	V
parent	scheduler/Pool.scala	/^  var parent: Pool = null$/;"	v
parent	scheduler/Schedulable.scala	/^  var parent: Pool$/;"	v
parent	scheduler/cluster/ClusterTaskSetManager.scala	/^  var parent: Pool = null$/;"	v
parent	scheduler/local/LocalTaskSetManager.scala	/^  var parent: Pool = null$/;"	v
parent	util/SizeEstimator.scala	/^    val parent = getClassInfo(cls.getSuperclass)$/;"	V
parentCtor	util/ClosureCleaner.scala	/^      val parentCtor = classOf[java.lang.Object].getDeclaredConstructor()$/;"	V
parentPool	scheduler/SchedulableBuilder.scala	/^    var parentPool = rootPool.getSchedulableByName(poolName)$/;"	v
parentSplit	rdd/PartitionPruningRDD.scala	/^class PartitionPruningRDDPartition(idx: Int, val parentSplit: Partition) extends Partition {$/;"	V
parents	rdd/CoalescedRDD.scala	/^  var parents: Seq[Partition] = parentsIndices.map(rdd.partitions(_))$/;"	v
parents	scheduler/DAGScheduler.scala	/^    val parents = new HashSet[Stage]$/;"	V
parents	scheduler/Stage.scala	/^    val parents: List[Stage],$/;"	V
parse	deploy/master/MasterArguments.scala	/^  def parse(args: List[String]): Unit = args match {$/;"	m
parse	deploy/worker/WorkerArguments.scala	/^  def parse(args: List[String]): Unit = args match {$/;"	m
parseHostPort	util/Utils.scala	/^  def parseHostPort(hostPort: String): (String,  Int) = {$/;"	m
part	rdd/JdbcRDD.scala	/^    val part = thePart.asInstanceOf[JdbcPartition]$/;"	V
part	rdd/OrderedRDDFunctions.scala	/^    val part = new RangePartitioner(numPartitions, self, ascending)$/;"	V
partition	Partitioner.scala	/^    var partition = 0$/;"	v
partition	rdd/SubtractedRDD.scala	/^    val partition = p.asInstanceOf[CoGroupPartition]$/;"	V
partition	scheduler/DAGScheduler.scala	/^        val partition = job.partitions(id)$/;"	V
partition1	rdd/ZippedRDD.scala	/^  var partition1 = rdd1.partitions(idx)$/;"	v
partition2	rdd/ZippedRDD.scala	/^  var partition2 = rdd2.partitions(idx)$/;"	v
partitionBy	api/java/JavaPairRDD.scala	/^  def partitionBy(partitioner: Partitioner): JavaPairRDD[K, V] =$/;"	m
partitionBy	rdd/PairRDDFunctions.scala	/^  def partitionBy(partitioner: Partitioner): RDD[(K, V)] = {$/;"	m
partitionFiles	rdd/CheckpointRDD.scala	/^      val partitionFiles = dirContents.map(_.getPath.toString).filter(_.contains("part-")).sorted$/;"	V
partitionId	TaskContext.scala	/^  val partitionId: Int,$/;"	V
partitionId	scheduler/Task.scala	/^private[spark] abstract class Task[T](val stageId: Int, var partitionId: Int) extends Serializable {$/;"	v
partitionValues	rdd/ZippedPartitionsRDD.scala	/^  var partitionValues = rdds.map(rdd => rdd.partitions(idx))$/;"	v
partitioned	rdd/PairRDDFunctions.scala	/^      val partitioned = new ShuffledRDD[K, C, (K, C)](combined, partitioner)$/;"	V
partitioner	Dependency.scala	/^    val partitioner: Partitioner,$/;"	V
partitioner	api/python/PythonRDD.scala	/^  override val partitioner = if (preservePartitoning) parent.partitioner else None$/;"	V
partitioner	rdd/CoGroupedRDD.scala	/^  override val partitioner = Some(part)$/;"	V
partitioner	rdd/FilteredRDD.scala	/^  override val partitioner = prev.partitioner    \/\/ Since filter cannot change a partition's keys$/;"	V
partitioner	rdd/FlatMappedValuesRDD.scala	/^  override val partitioner = firstParent[Product2[K, V]].partitioner$/;"	V
partitioner	rdd/MapPartitionsRDD.scala	/^  override val partitioner = if (preservesPartitioning) firstParent[T].partitioner else None$/;"	V
partitioner	rdd/MappedValuesRDD.scala	/^  override val partitioner = firstParent[Product2[K, U]].partitioner$/;"	V
partitioner	rdd/RDD.scala	/^  @transient val partitioner: Option[Partitioner] = None$/;"	V
partitioner	rdd/ShuffledRDD.scala	/^  override val partitioner = Some(part)$/;"	V
partitioner	rdd/SubtractedRDD.scala	/^  override val partitioner = Some(part)$/;"	V
partitions	rdd/PartitionPruningRDD.scala	/^  val partitions: Array[Partition] = rdd.partitions$/;"	V
partitions	rdd/ZippedPartitionsRDD.scala	/^    val partitions = s.asInstanceOf[ZippedPartitionsPartition].partitions$/;"	V
partitions	rdd/ZippedPartitionsRDD.scala	/^  def partitions = partitionValues$/;"	m
partitions	rdd/ZippedRDD.scala	/^  def partitions = (partition1, partition2)$/;"	m
partitions	scheduler/ActiveJob.scala	/^    val partitions: Array[Int],$/;"	V
partitions	scheduler/DAGScheduler.scala	/^    val partitions = (0 until rdd.partitions.size).toArray$/;"	V
partitions_	rdd/RDD.scala	/^  @transient private var partitions_ : Array[Partition] = null$/;"	v
parts	rdd/ZippedPartitionsRDD.scala	/^    val parts = s.asInstanceOf[ZippedPartitionsPartition].partitions$/;"	V
partsScanned	rdd/AsyncRDDActions.scala	/^      var partsScanned = 0$/;"	v
partsScanned	rdd/RDD.scala	/^    var partsScanned = 0$/;"	v
path	SparkContext.scala	/^    val path = new Path(dir)$/;"	V
path	SparkHadoopWriter.scala	/^    val path = FileOutputFormat.getOutputPath(conf.value)$/;"	V
path	deploy/worker/ui/WorkerWebUI.scala	/^    val path = "%s\/%s\/%s\/%s".format(workDir.getPath, appId, executorId, logType)$/;"	V
path	rdd/CheckpointRDD.scala	/^    val path = new Path(hdfsPath, "temp")$/;"	V
path	rdd/RDDCheckpointData.scala	/^    val path = new Path(rdd.context.checkpointDir.get, "rdd-" + rdd.id)$/;"	V
path	scheduler/InputFormatInfo.scala	/^                      val path: String) extends Logging {$/;"	V
pb	api/python/PythonWorkerFactory.scala	/^        val pb = new ProcessBuilder(Seq(pythonExec, sparkHome + "\/python\/pyspark\/daemon.py"))$/;"	V
pb	api/python/PythonWorkerFactory.scala	/^      val pb = new ProcessBuilder(Seq(pythonExec, sparkHome + "\/python\/pyspark\/worker.py"))$/;"	V
pb	rdd/PipedRDD.scala	/^    val pb = new ProcessBuilder(command)$/;"	V
pc	rdd/CoalescedRDD.scala	/^    val pc = new PartitionCoalescer(maxPartitions, prev, balanceSlack)$/;"	V
pcController	broadcast/BitTorrentBroadcast.scala	/^    var pcController = new PeerChatterController$/;"	v
peerIter	broadcast/BitTorrentBroadcast.scala	/^              var peerIter = listOfSources.iterator$/;"	v
peerIter	broadcast/BitTorrentBroadcast.scala	/^        var peerIter = peersNotInUse.iterator$/;"	v
peerSocketToSource	broadcast/BitTorrentBroadcast.scala	/^      private var peerSocketToSource: Socket = null$/;"	v
peerToTalkTo	broadcast/BitTorrentBroadcast.scala	/^          var peerToTalkTo = pickPeerToTalkToRandom$/;"	v
peers	storage/BlockManagerMasterActor.scala	/^    val peers: Array[BlockManagerId] = blockManagerInfo.keySet.toArray$/;"	V
peersNotInUse	broadcast/BitTorrentBroadcast.scala	/^      var peersNotInUse = ListBuffer[SourceInfo]()$/;"	v
peersNowTalking	broadcast/BitTorrentBroadcast.scala	/^    private var peersNowTalking = ListBuffer[SourceInfo]()$/;"	v
peersWithRareBlocks	broadcast/BitTorrentBroadcast.scala	/^      var peersWithRareBlocks = ListBuffer[(SourceInfo, Int)]()$/;"	v
pendingTasks	scheduler/DAGScheduler.scala	/^  val pendingTasks = new TimeStampedHashMap[Stage, HashSet[Task[_]]] \/\/ Missing tasks from each stage$/;"	V
pendingTasksForExecutor	scheduler/cluster/ClusterTaskSetManager.scala	/^  private val pendingTasksForExecutor = new HashMap[String, ArrayBuffer[Int]]$/;"	V
pendingTasksForHost	scheduler/cluster/ClusterTaskSetManager.scala	/^  private val pendingTasksForHost = new HashMap[String, ArrayBuffer[Int]]$/;"	V
pendingTasksForRack	scheduler/cluster/ClusterTaskSetManager.scala	/^  private val pendingTasksForRack = new HashMap[String, ArrayBuffer[Int]]$/;"	V
pendingTasksWithNoPrefs	scheduler/cluster/ClusterTaskSetManager.scala	/^  val pendingTasksWithNoPrefs = new ArrayBuffer[Int]$/;"	V
percentiles	scheduler/SparkListener.scala	/^  val percentiles = Array[Int](0,5,10,25,50,75,90,95,100)$/;"	V
percentilesHeader	scheduler/SparkListener.scala	/^  val percentilesHeader = "\\t" + percentiles.mkString("%\\t") + "%"$/;"	V
period	metrics/MetricsSystem.scala	/^    val period = MINIMAL_POLL_UNIT.convert(pollPeriod, pollUnit)$/;"	V
periodSeconds	util/MetadataCleaner.scala	/^  private val periodSeconds = math.max(10, delaySeconds \/ 10)$/;"	V
persist	api/java/JavaDoubleRDD.scala	/^  def persist(newLevel: StorageLevel): JavaDoubleRDD = fromRDD(srdd.persist(newLevel))$/;"	m
persist	api/java/JavaPairRDD.scala	/^  def persist(newLevel: StorageLevel): JavaPairRDD[K, V] =$/;"	m
persist	api/java/JavaRDD.scala	/^  def persist(newLevel: StorageLevel): JavaRDD[T] = wrapRDD(rdd.persist(newLevel))$/;"	m
persist	rdd/RDD.scala	/^  def persist(): RDD[T] = persist(StorageLevel.MEMORY_ONLY)$/;"	m
persist	rdd/RDD.scala	/^  def persist(newLevel: StorageLevel): RDD[T] = {$/;"	m
persistenceEngine	deploy/master/Master.scala	/^  var persistenceEngine: PersistenceEngine = _$/;"	v
persistentRdds	SparkContext.scala	/^  private[spark] val persistentRdds = new TimeStampedHashMap[Int, RDD[_]]$/;"	V
pgroup	rdd/CoalescedRDD.scala	/^        val pgroup = PartitionGroup(nxt_replica)$/;"	V
pgroup	rdd/CoalescedRDD.scala	/^      val pgroup = PartitionGroup(nxt_replica)$/;"	V
pickBin	rdd/CoalescedRDD.scala	/^  def pickBin(p: Partition): PartitionGroup = {$/;"	m
pickedBlockIndex	broadcast/BitTorrentBroadcast.scala	/^          var pickedBlockIndex = needBlocksBitVector.nextSetBit(0)$/;"	v
picksLeft	broadcast/BitTorrentBroadcast.scala	/^            var picksLeft = MultiTracker.MaxPeersInGuideResponse$/;"	v
pieceId	broadcast/TorrentBroadcast.scala	/^      val pieceId = BroadcastHelperBlockId(broadcastId, "piece" + i)$/;"	V
pieceId	broadcast/TorrentBroadcast.scala	/^      val pieceId = BroadcastHelperBlockId(broadcastId, "piece" + pid)$/;"	V
pipe	api/java/JavaRDDLike.scala	/^  def pipe(command: JList[String]): JavaRDD[String] =$/;"	m
pipe	api/java/JavaRDDLike.scala	/^  def pipe(command: JList[String], env: java.util.Map[String, String]): JavaRDD[String] =$/;"	m
pipe	api/java/JavaRDDLike.scala	/^  def pipe(command: String): JavaRDD[String] = rdd.pipe(command)$/;"	m
pipe	rdd/RDD.scala	/^  def pipe(command: String): RDD[String] = new PipedRDD(this, command)$/;"	m
pipe	rdd/RDD.scala	/^  def pipe(command: String, env: Map[String, String]): RDD[String] =$/;"	m
plusDot	util/Vector.scala	/^  def plusDot(plus: Vector, other: Vector): Double = {$/;"	m
pointerFields	util/SizeEstimator.scala	/^    val pointerFields: List[Field]) {}$/;"	V
pointerFields	util/SizeEstimator.scala	/^    var pointerFields = parent.pointerFields$/;"	v
pointerSize	util/SizeEstimator.scala	/^  private var pointerSize = 4$/;"	v
poisson	rdd/SampledRDD.scala	/^      val poisson = new Poisson(frac, new DRand(split.seed))$/;"	V
pollDir	metrics/sink/CsvSink.scala	/^  val pollDir = Option(property.getProperty(CSV_KEY_DIR)) match {$/;"	V
pollPeriod	metrics/sink/ConsoleSink.scala	/^  val pollPeriod = Option(property.getProperty(CONSOLE_KEY_PERIOD)) match {$/;"	V
pollPeriod	metrics/sink/CsvSink.scala	/^  val pollPeriod = Option(property.getProperty(CSV_KEY_PERIOD)) match {$/;"	V
pollPeriod	metrics/sink/GangliaSink.scala	/^  val pollPeriod = propertyToOption(GANGLIA_KEY_PERIOD).map(_.toInt)$/;"	V
pollUnit	metrics/sink/ConsoleSink.scala	/^  val pollUnit = Option(property.getProperty(CONSOLE_KEY_UNIT)) match {$/;"	V
pollUnit	metrics/sink/CsvSink.scala	/^  val pollUnit = Option(property.getProperty(CSV_KEY_UNIT)) match {$/;"	V
pollUnit	metrics/sink/GangliaSink.scala	/^  val pollUnit = propertyToOption(GANGLIA_KEY_UNIT).map(u => TimeUnit.valueOf(u.toUpperCase))$/;"	V
pool	scheduler/SchedulableBuilder.scala	/^      val pool = new Pool(DEFAULT_POOL_NAME, DEFAULT_SCHEDULING_MODE,$/;"	V
pool	scheduler/SchedulableBuilder.scala	/^      val pool = new Pool(poolName, schedulingMode, minShare, weight)$/;"	V
pool	ui/JettyUtils.scala	/^      val pool = new QueuedThreadPool$/;"	V
pool	ui/jobs/PoolPage.scala	/^      val pool = listener.sc.getPoolForName(poolName).get$/;"	V
poolName	scheduler/Pool.scala	/^    val poolName: String,$/;"	V
poolName	scheduler/SchedulableBuilder.scala	/^      val poolName = (poolNode \\ POOL_NAME_PROPERTY).text$/;"	V
poolName	scheduler/SchedulableBuilder.scala	/^    var poolName = DEFAULT_POOL_NAME$/;"	v
poolName	ui/jobs/JobProgressListener.scala	/^    val poolName = Option(stageSubmitted.properties).map {$/;"	V
poolName	ui/jobs/PoolPage.scala	/^      val poolName = request.getParameter("poolname")$/;"	V
poolName	ui/jobs/StageTable.scala	/^    val poolName = listener.stageIdToPool.get(s.stageId)$/;"	V
poolPage	ui/jobs/JobProgressUI.scala	/^  private val poolPage = new PoolPage(this)$/;"	V
poolTable	ui/jobs/IndexPage.scala	/^      val poolTable = new PoolTable(pools, listener)$/;"	V
poolTable	ui/jobs/PoolPage.scala	/^      val poolTable = new PoolTable(Seq(pool), listener)$/;"	V
poolToActiveStages	ui/jobs/JobProgressListener.scala	/^  val poolToActiveStages = new HashMap[String, HashSet[StageInfo]]()$/;"	V
poolToActiveStages	ui/jobs/PoolPage.scala	/^      val poolToActiveStages = listener.poolToActiveStages$/;"	V
poolToActiveStages	ui/jobs/PoolTable.scala	/^  var poolToActiveStages: HashMap[String, HashSet[StageInfo]] = listener.poolToActiveStages$/;"	v
pools	ui/jobs/IndexPage.scala	/^      val pools = listener.sc.getAllPools$/;"	V
port	HttpServer.scala	/^  private var port: Int = -1$/;"	v
port	deploy/master/MasterArguments.scala	/^  var port = 7077$/;"	v
port	deploy/master/WorkerInfo.scala	/^    val port: Int,$/;"	V
port	deploy/master/ui/MasterWebUI.scala	/^  val port = requestedPort$/;"	V
port	deploy/worker/WorkerArguments.scala	/^  var port = 0$/;"	v
port	deploy/worker/ui/WorkerWebUI.scala	/^  val port = requestedPort.getOrElse($/;"	V
port	metrics/sink/GangliaSink.scala	/^  val port = propertyToOption(GANGLIA_KEY_PORT).get.toInt$/;"	V
port	network/MessageChunkHeader.scala	/^    val port = address.getPort()$/;"	V
port	network/MessageChunkHeader.scala	/^    val port = buffer.getInt()$/;"	V
port	network/netty/ShuffleCopier.scala	/^    val port = args(1).toInt$/;"	V
port	network/netty/ShuffleSender.scala	/^    val port = args(0).toInt$/;"	V
port	network/netty/ShuffleSender.scala	/^  def port: Int = server.getPort()$/;"	m
port	storage/BlockManagerId.scala	/^  def port: Int = port_$/;"	m
port	ui/SparkUI.scala	/^  val port = Option(System.getProperty("spark.ui.port")).getOrElse(SparkUI.DEFAULT_PORT).toInt$/;"	V
port_	storage/BlockManagerId.scala	/^    private var port_ : Int,$/;"	v
pos	deploy/master/Master.scala	/^        var pos = 0$/;"	v
pos	rdd/UnionRDD.scala	/^    var pos = 0$/;"	v
pos	util/AppendOnlyMap.scala	/^    var pos = -1$/;"	v
pos	util/AppendOnlyMap.scala	/^    var pos = rehash(k.hashCode) & mask$/;"	v
pos	util/AppendOnlyMap.scala	/^    var pos = rehash(key.hashCode) & mask$/;"	v
pos	util/collection/OpenHashMap.scala	/^      val pos = _keySet.addWithoutResize(k) & OpenHashSet.POSITION_MASK$/;"	V
pos	util/collection/OpenHashMap.scala	/^      val pos = _keySet.addWithoutResize(k)$/;"	V
pos	util/collection/OpenHashMap.scala	/^      val pos = _keySet.getPos(k)$/;"	V
pos	util/collection/OpenHashMap.scala	/^    var pos = -1$/;"	v
pos	util/collection/OpenHashSet.scala	/^    var pos = 0$/;"	v
pos	util/collection/OpenHashSet.scala	/^    var pos = hashcode(hasher.hash(k)) & _mask$/;"	v
pos	util/collection/OpenHashSet.scala	/^    var pos = hashcode(hasher.hash(k)) & mask$/;"	v
pos	util/collection/PrimitiveKeyOpenHashMap.scala	/^    val pos = _keySet.addWithoutResize(k) & OpenHashSet.POSITION_MASK$/;"	V
pos	util/collection/PrimitiveKeyOpenHashMap.scala	/^    val pos = _keySet.addWithoutResize(k)$/;"	V
pos	util/collection/PrimitiveKeyOpenHashMap.scala	/^    val pos = _keySet.getPos(k)$/;"	V
pos	util/collection/PrimitiveKeyOpenHashMap.scala	/^    var pos = 0$/;"	v
post	scheduler/SparkListenerBus.scala	/^  def post(event: SparkListenerEvents) {$/;"	m
pre	deploy/worker/ui/WorkerWebUI.scala	/^    val pre = "==== Bytes %s-%s of %s of %s\/%s\/%s ====\\n"$/;"	V
pref	rdd/CoalescedRDD.scala	/^    val pref = currPrefLocs(p).map(getLeastGroupHash(_)).sortWith(compare) \/\/ least loaded pref locs$/;"	V
pref1	rdd/ZippedRDD.scala	/^    val pref1 = rdd1.preferredLocations(partition1)$/;"	V
pref2	rdd/ZippedRDD.scala	/^    val pref2 = rdd2.preferredLocations(partition2)$/;"	V
prefPart	rdd/CoalescedRDD.scala	/^    val prefPart = if (pref == Nil) None else pref.head$/;"	V
prefPartActual	rdd/CoalescedRDD.scala	/^    val prefPartActual = prefPart.get$/;"	V
preferredLocations	rdd/UnionRDD.scala	/^  def preferredLocations() = rdd.preferredLocations(split)$/;"	m
preferredLocations	scheduler/Task.scala	/^  def preferredLocations: Seq[TaskLocation] = Nil$/;"	m
preferredLocs	scheduler/ResultTask.scala	/^  @transient private val preferredLocs: Seq[TaskLocation] = {$/;"	V
preferredLocs	scheduler/ShuffleMapTask.scala	/^  @transient private val preferredLocs: Seq[TaskLocation] = {$/;"	V
preferredNodeLocationData	SparkContext.scala	/^    val preferredNodeLocationData: scala.collection.Map[String, scala.collection.Set[SplitInfo]] =$/;"	V
prefs	rdd/ZippedPartitionsRDD.scala	/^    val prefs = rdds.zip(parts).map { case (rdd, p) => rdd.preferredLocations(p) }$/;"	V
prefs	scheduler/cluster/ClusterTaskSetManager.scala	/^        val prefs = tasks(index).preferredLocations$/;"	V
prependBaseUri	ui/UIUtils.scala	/^  def prependBaseUri(resource: String = "") = uiRoot + resource$/;"	m
prev	rdd/CoalescedRDD.scala	/^                                      @transient var prev: RDD[T],$/;"	v
prev	rdd/SampledRDD.scala	/^class SampledRDDPartition(val prev: Partition, val seed: Int) extends Partition with Serializable {$/;"	V
prev	rdd/ShuffledRDD.scala	/^    @transient var prev: RDD[P],$/;"	v
prev	util/TimeStampedHashMap.scala	/^    val prev = internalMap.putIfAbsent(key, (value, currentTime))$/;"	V
prevList	scheduler/Stage.scala	/^      val prevList = outputLocs(partition)$/;"	V
prevList	scheduler/Stage.scala	/^    val prevList = outputLocs(partition)$/;"	V
prevPos	storage/BlockObjectWriter.scala	/^      val prevPos = lastValidPosition$/;"	V
prfs	rdd/PairRDDFunctions.scala	/^    val prfs = new PairRDDFunctions[K, Seq[Seq[_]]](cg)(classManifest[K], Manifests.seqSeqManifest)$/;"	V
printBuffer	network/Connection.scala	/^  def printBuffer(buffer: ByteBuffer, position: Int, length: Int) {$/;"	m
printOut	api/python/PythonRDD.scala	/^          val printOut = new PrintWriter(stream)$/;"	V
printRemainingBuffer	network/Connection.scala	/^  def printRemainingBuffer(buffer: ByteBuffer) {$/;"	m
printUsageAndExit	deploy/master/MasterArguments.scala	/^  def printUsageAndExit(exitCode: Int) {$/;"	m
printUsageAndExit	deploy/worker/WorkerArguments.scala	/^  def printUsageAndExit(exitCode: Int) {$/;"	m
prioritizeContainers	scheduler/cluster/ClusterScheduler.scala	/^  def prioritizeContainers[K, T] (map: HashMap[K, ArrayBuffer[T]]): List[T] = {$/;"	m
priority	scheduler/Pool.scala	/^  var priority = 0$/;"	v
priority	scheduler/Schedulable.scala	/^  def priority: Int$/;"	m
priority	scheduler/TaskSet.scala	/^    val priority: Int,$/;"	V
priority	scheduler/cluster/ClusterTaskSetManager.scala	/^  var priority = taskSet.priority$/;"	v
priority	scheduler/local/LocalTaskSetManager.scala	/^  var priority: Int = taskSet.priority$/;"	v
priority1	scheduler/SchedulingAlgorithm.scala	/^    val priority1 = s1.priority$/;"	V
priority2	scheduler/SchedulingAlgorithm.scala	/^    val priority2 = s2.priority$/;"	V
probFailure	ui/UIWorkloadGenerator.scala	/^          val probFailure = (4.0 \/ NUM_PARTITIONS)$/;"	V
probabilities	scheduler/SparkListener.scala	/^  val probabilities = percentiles.map{_ \/ 100.0}$/;"	V
proc	rdd/PipedRDD.scala	/^    val proc = pb.start()$/;"	V
process	deploy/master/SparkZooKeeperSession.scala	/^    def process(event: WatchedEvent) {$/;"	m
process	deploy/master/ZooKeeperLeaderElectionAgent.scala	/^    def process(event: WatchedEvent) {$/;"	m
process	deploy/worker/ExecutorRunner.scala	/^  var process: Process = null$/;"	v
process	rdd/PairRDDFunctions.scala	/^        def process(it: Iterator[(K, V)]): Seq[V] = {$/;"	m
process	util/Utils.scala	/^    val process = builder.start()$/;"	V
process	util/Utils.scala	/^    val process = new ProcessBuilder(command: _*)$/;"	V
processBlockMessage	storage/BlockManagerWorker.scala	/^  def processBlockMessage(blockMessage: BlockMessage): Option[BlockMessage] = {$/;"	m
processFunc	SparkContext.scala	/^    val processFunc = (context: TaskContext, iter: Iterator[T]) => processPartition(iter)$/;"	V
processPartition	rdd/DoubleRDDFunctions.scala	/^    val processPartition = (ctx: TaskContext, ns: Iterator[Double]) => StatCounter(ns)$/;"	V
producers	storage/ThreadingTest.scala	/^    val producers = (1 to numProducers).map(i => new ProducerThread(blockManager, i))$/;"	V
promise	network/ConnectionManager.scala	/^    val promise = Promise[Option[Message]]$/;"	V
properties	executor/MesosExecutorBackend.scala	/^    val properties = Utils.deserialize[Array[(String, String)]](executorInfo.getData.toByteArray)$/;"	V
properties	metrics/MetricsConfig.scala	/^  val properties = new Properties()$/;"	V
properties	scheduler/ActiveJob.scala	/^    val properties: Properties) {$/;"	V
properties	scheduler/DAGScheduler.scala	/^    val properties = if (idToActiveJob.contains(stage.jobId)) {$/;"	V
properties	scheduler/JobLogger.scala	/^    val properties = jobStart.properties$/;"	V
properties	scheduler/TaskSet.scala	/^    val properties: Properties) {$/;"	V
properties	scheduler/cluster/CoarseGrainedSchedulerBackend.scala	/^    val properties = new ArrayBuffer[(String, String)]$/;"	V
properties	ui/env/EnvironmentUI.scala	/^    val properties = System.getProperties.iterator.toSeq$/;"	V
property	metrics/sink/ConsoleSink.scala	/^class ConsoleSink(val property: Properties, val registry: MetricRegistry) extends Sink {$/;"	V
property	metrics/sink/CsvSink.scala	/^class CsvSink(val property: Properties, val registry: MetricRegistry) extends Sink {$/;"	V
property	metrics/sink/GangliaSink.scala	/^class GangliaSink(val property: Properties, val registry: MetricRegistry) extends Sink {$/;"	V
property	metrics/sink/JmxSink.scala	/^class JmxSink(val property: Properties, val registry: MetricRegistry) extends Sink {$/;"	V
property	metrics/sink/MetricsServlet.scala	/^class MetricsServlet(val property: Properties, val registry: MetricRegistry) extends Sink {$/;"	V
propertyCategories	metrics/MetricsConfig.scala	/^  var propertyCategories: mutable.HashMap[String, Properties] = null$/;"	v
propertyHeaders	ui/env/EnvironmentUI.scala	/^    val propertyHeaders = Seq("Name", "Value")$/;"	V
propertyRow	ui/env/EnvironmentUI.scala	/^    def propertyRow(kv: (String, String)) = <tr><td>{kv._1}<\/td><td>{kv._2}<\/td><\/tr>$/;"	m
propertyToOption	metrics/sink/GangliaSink.scala	/^  def propertyToOption(prop: String) = Option(property.getProperty(prop))$/;"	m
props	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^      val props = new HashMap[String, String]$/;"	V
provider	util/AkkaUtils.scala	/^    val provider = actorSystem.asInstanceOf[ExtendedActorSystem].provider$/;"	V
publicAddress	deploy/master/WorkerInfo.scala	/^    val publicAddress: String)$/;"	V
publicAddress	deploy/worker/Worker.scala	/^  val publicAddress = {$/;"	V
put	storage/BlockManager.scala	/^  def put(blockId: BlockId, values: ArrayBuffer[Any], level: StorageLevel,$/;"	m
put	storage/BlockManager.scala	/^  def put(blockId: BlockId, values: Iterator[Any], level: StorageLevel, tellMaster: Boolean)$/;"	m
putBytes	storage/BlockManager.scala	/^  def putBytes(blockId: BlockId, bytes: ByteBuffer, level: StorageLevel,$/;"	m
putBytes	storage/BlockStore.scala	/^  def putBytes(blockId: BlockId, bytes: ByteBuffer, level: StorageLevel)$/;"	m
putCachedMetadata	rdd/HadoopRDD.scala	/^  def putCachedMetadata(key: String, value: Any) =$/;"	m
putIfAbsent	util/TimeStampedHashMap.scala	/^  def putIfAbsent(key: A, value: B): Option[B] = {$/;"	m
putLock	storage/MemoryStore.scala	/^  private val putLock = new Object()$/;"	V
putResult	storage/BlockFetcherIterator.scala	/^      def putResult(blockId: BlockId, blockSize: Long, blockData: ByteBuf) {$/;"	m
putSingle	storage/BlockManager.scala	/^  def putSingle(blockId: BlockId, value: Any, level: StorageLevel, tellMaster: Boolean = true) {$/;"	m
putValues	storage/BlockStore.scala	/^  def putValues(blockId: BlockId, values: ArrayBuffer[Any], level: StorageLevel,$/;"	m
pyPartitionFunctionId	api/python/PythonPartitioner.scala	/^  val pyPartitionFunctionId: Long)$/;"	V
pythonPath	api/python/PythonWorkerFactory.scala	/^        val pythonPath = sparkHome + "\/python\/" + File.pathSeparator + workerEnv.get("PYTHONPATH")$/;"	V
pythonPath	api/python/PythonWorkerFactory.scala	/^      val pythonPath = sparkHome + "\/python\/" + File.pathSeparator + workerEnv.get("PYTHONPATH")$/;"	V
pythonWorkers	SparkEnv.scala	/^  private val pythonWorkers = mutable.HashMap[(String, Map[String, String]), PythonWorkerFactory]()$/;"	V
quantileHeaders	ui/jobs/StagePage.scala	/^          val quantileHeaders = Seq("Metric", "Min", "25th percentile",$/;"	V
quantileRow	ui/jobs/StagePage.scala	/^          def quantileRow(data: Seq[String]): Seq[Node] = <tr> {data.map(d => <td>{d}<\/td>)} <\/tr>$/;"	m
quantiles	scheduler/SparkListener.scala	/^    val quantiles = d.getQuantiles(probabilities).map{formatNumber}$/;"	V
queue	rdd/RDD.scala	/^      val queue = new BoundedPriorityQueue[T](num)$/;"	V
queue	storage/ThreadingTest.scala	/^    val queue = new ArrayBlockingQueue[(BlockId, Seq[Int])](100)$/;"	V
queueFullErrorMessageLogged	scheduler/SparkListenerBus.scala	/^  private var queueFullErrorMessageLogged = false$/;"	v
r	rdd/ParallelCollectionRDD.scala	/^        var r = nr$/;"	v
r	util/CompletionIterator.scala	/^    val r = sub.hasNext$/;"	V
r	util/collection/OpenHashSet.scala	/^    val r = h ^ (h >>> 20) ^ (h >>> 12)$/;"	V
r1	rdd/CoalescedRDD.scala	/^    val r1 = rnd.nextInt(groupArr.size)$/;"	V
r2	rdd/CoalescedRDD.scala	/^    val r2 = rnd.nextInt(groupArr.size)$/;"	V
racks	scheduler/cluster/ClusterTaskSetManager.scala	/^            val racks = tasks(index).preferredLocations.map(_.host).map(sched.getRackForHost)$/;"	V
ranGen	broadcast/MultiTracker.scala	/^  var ranGen = new Random$/;"	v
rand	rdd/RDD.scala	/^    val rand = new Random(seed)$/;"	V
rand	rdd/SampledRDD.scala	/^      val rand = new Random(split.seed)$/;"	V
rand	storage/DiskBlockManager.scala	/^      val rand = new Random()$/;"	V
rand	util/SizeEstimator.scala	/^        val rand = new Random(42)$/;"	V
randomLevel	storage/ThreadingTest.scala	/^    def randomLevel(): StorageLevel = {$/;"	m
randomNumber	broadcast/BitTorrentBroadcast.scala	/^        var randomNumber = MultiTracker.ranGen.nextDouble$/;"	v
randomize	util/Utils.scala	/^  def randomize[T: ClassManifest](seq: TraversableOnce[T]): Seq[T] = {$/;"	m
randomizeInPlace	util/Utils.scala	/^  def randomizeInPlace[T](arr: Array[T], rand: Random = new Random): Array[T] = {$/;"	m
range	deploy/worker/ui/WorkerWebUI.scala	/^    val range = <span>Bytes {startByte.toString} - {endByte.toString} of {logLength}<\/span>$/;"	V
rangeBounds	Partitioner.scala	/^  private val rangeBounds: Array[K] = {$/;"	V
rangeEnd	rdd/CoalescedRDD.scala	/^          val rangeEnd = (((i.toLong + 1) * prev.partitions.length) \/ maxPartitions).toInt$/;"	V
rangeStart	rdd/CoalescedRDD.scala	/^          val rangeStart = ((i.toLong * prev.partitions.length) \/ maxPartitions).toInt$/;"	V
rangeToSend	broadcast/TreeBroadcast.scala	/^          var rangeToSend = ois.readObject.asInstanceOf[(Int, Int)]$/;"	v
rareBlocksIndices	broadcast/BitTorrentBroadcast.scala	/^      var rareBlocksIndices = ListBuffer[Int]()$/;"	v
rate	util/RateLimitedOutputStream.scala	/^    val rate = bytesWrittenSinceSync.toDouble \/ elapsedSecs$/;"	V
rawMod	util/Utils.scala	/^    val rawMod = x % mod$/;"	V
rawSplits	rdd/NewHadoopRDD.scala	/^    val rawSplits = inputFormat.getSplits(jobContext).toArray$/;"	V
rdd	Dependency.scala	/^abstract class Dependency[T](val rdd: RDD[T]) extends Serializable$/;"	V
rdd	api/java/JavaDoubleRDD.scala	/^  override val rdd: RDD[Double] = srdd.map(x => Double.valueOf(x))$/;"	V
rdd	api/java/JavaPairRDD.scala	/^class JavaPairRDD[K, V](val rdd: RDD[(K, V)])(implicit val kManifest: ClassManifest[K],$/;"	V
rdd	api/java/JavaRDD.scala	/^class JavaRDD[T](val rdd: RDD[T])(implicit val classManifest: ClassManifest[T]) extends$/;"	V
rdd	api/java/JavaRDDLike.scala	/^  def rdd: RDD[T]$/;"	m
rdd	rdd/CheckpointRDD.scala	/^    val rdd = sc.makeRDD(1 to 10, 10).flatMap(x => 1 to 10000)$/;"	V
rdd	scheduler/DAGScheduler.scala	/^      val rdd = job.finalStage.rdd$/;"	V
rdd	scheduler/ResultTask.scala	/^    val rdd = objIn.readObject().asInstanceOf[RDD[_]]$/;"	V
rdd	scheduler/ResultTask.scala	/^    var rdd: RDD[T],$/;"	v
rdd	scheduler/ShuffleMapTask.scala	/^      val rdd = objIn.readObject().asInstanceOf[RDD[_]]$/;"	V
rdd	scheduler/ShuffleMapTask.scala	/^    var rdd: RDD[_],$/;"	v
rdd	scheduler/Stage.scala	/^    val rdd: RDD[_],$/;"	V
rdd1	rdd/CartesianRDD.scala	/^    var rdd1 : RDD[T],$/;"	v
rdd1	rdd/SubtractedRDD.scala	/^    @transient var rdd1: RDD[_ <: Product2[K, V]],$/;"	v
rdd1	rdd/ZippedPartitionsRDD.scala	/^    var rdd1: RDD[A],$/;"	v
rdd1	rdd/ZippedRDD.scala	/^    var rdd1: RDD[T],$/;"	v
rdd2	rdd/CartesianRDD.scala	/^    var rdd2 : RDD[U])$/;"	v
rdd2	rdd/SubtractedRDD.scala	/^    @transient var rdd2: RDD[_ <: Product2[K, W]],$/;"	v
rdd2	rdd/ZippedPartitionsRDD.scala	/^    var rdd2: RDD[B])$/;"	v
rdd2	rdd/ZippedPartitionsRDD.scala	/^    var rdd2: RDD[B],$/;"	v
rdd2	rdd/ZippedRDD.scala	/^    var rdd2: RDD[U])$/;"	v
rdd3	rdd/ZippedPartitionsRDD.scala	/^    var rdd3: RDD[C])$/;"	v
rdd3	rdd/ZippedPartitionsRDD.scala	/^    var rdd3: RDD[C],$/;"	v
rdd4	rdd/ZippedPartitionsRDD.scala	/^    var rdd4: RDD[D])$/;"	v
rddBlocks	storage/StorageUtils.scala	/^  def rddBlocks = blocks.flatMap {$/;"	m
rddBlocks	ui/exec/ExecutorsUI.scala	/^    val rddBlocks = status.blocks.size.toString$/;"	V
rddHeaders	ui/storage/IndexPage.scala	/^    val rddHeaders = Seq($/;"	V
rddId	rdd/ParallelCollectionRDD.scala	/^    var rddId: Long,$/;"	v
rddInfo	scheduler/JobLogger.scala	/^    val rddInfo =$/;"	V
rddInfo	ui/storage/RDDPage.scala	/^    val rddInfo = StorageUtils.rddInfoFromStorageStatus(filteredStorageStatusList, sc).head$/;"	V
rddInfoFromBlockStatusList	storage/StorageUtils.scala	/^  def rddInfoFromBlockStatusList(infos: Map[RDDBlockId, BlockStatus],$/;"	m
rddInfoFromStorageStatus	storage/StorageUtils.scala	/^  def rddInfoFromStorageStatus(storageStatusList: Seq[StorageStatus],$/;"	m
rddInfos	storage/StorageUtils.scala	/^    val rddInfos = groupedRddBlocks.map { case (rddId, rddBlocks) =>$/;"	V
rddList	scheduler/JobLogger.scala	/^      var rddList = new ListBuffer[RDD[_]]$/;"	v
rddName	scheduler/JobLogger.scala	/^    var rddName = rdd.getClass.getSimpleName$/;"	v
rddName	scheduler/StageInfo.scala	/^  val rddName = stage.rdd.name$/;"	V
rddName	storage/StorageUtils.scala	/^        val rddName = Option(r.name).getOrElse(rddId.toString)$/;"	V
rddPage	ui/storage/BlockManagerUI.scala	/^  val rddPage = new RDDPage(this)$/;"	V
rddPrefs	scheduler/DAGScheduler.scala	/^    val rddPrefs = rdd.preferredLocations(rdd.partitions(partition)).toList$/;"	V
rddRow	ui/storage/IndexPage.scala	/^  def rddRow(rdd: RDDInfo): Seq[Node] = {$/;"	m
rddSample	Partitioner.scala	/^      val rddSample = rdd.sample(false, frac, 1).map(_._1).collect().sortWith(_ < _)$/;"	V
rddSize	Partitioner.scala	/^      val rddSize = rdd.count()$/;"	V
rddStorageLevel	storage/StorageUtils.scala	/^        val rddStorageLevel = r.getStorageLevel$/;"	V
rddToAdd	storage/MemoryStore.scala	/^      val rddToAdd = getRddId(blockIdToAdd)$/;"	V
rdds	api/java/JavaSparkContext.scala	/^    val rdds: Seq[RDD[(K, V)]] = (Seq(first) ++ asScalaBuffer(rest)).map(_.rdd)$/;"	V
rdds	api/java/JavaSparkContext.scala	/^    val rdds: Seq[RDD[Double]] = (Seq(first) ++ asScalaBuffer(rest)).map(_.srdd)$/;"	V
rdds	api/java/JavaSparkContext.scala	/^    val rdds: Seq[RDD[T]] = (Seq(first) ++ asScalaBuffer(rest)).map(_.rdd)$/;"	V
rdds	rdd/CoGroupedRDD.scala	/^class CoGroupedRDD[K](@transient var rdds: Seq[RDD[_ <: Product2[K, _]]], part: Partitioner)$/;"	v
rdds	rdd/UnionRDD.scala	/^    @transient var rdds: Seq[RDD[T]])$/;"	v
rdds	rdd/ZippedPartitionsRDD.scala	/^    var rdds: Seq[RDD[_]])$/;"	v
rdds	ui/storage/IndexPage.scala	/^    val rdds = StorageUtils.rddInfoFromStorageStatus(storageStatusList, sc)$/;"	V
read	broadcast/HttpBroadcast.scala	/^  def read[T](id: Long): T = {$/;"	m
read	network/Connection.scala	/^  def read(): Boolean = {$/;"	m
read	util/Utils.scala	/^      def read(): Int = is.read()$/;"	m
readExternal	scheduler/MapStatus.scala	/^  def readExternal(in: ObjectInput) {$/;"	m
readFromFile	rdd/CheckpointRDD.scala	/^  def readFromFile[T](path: Path, context: TaskContext): Iterator[T] = {$/;"	m
readMetrics	scheduler/JobLogger.scala	/^    val readMetrics = taskMetrics.shuffleReadMetrics match {$/;"	V
readObject	serializer/JavaSerializer.scala	/^  def readObject[T](): T = objIn.readObject().asInstanceOf[T]$/;"	m
readObject	serializer/KryoSerializer.scala	/^  def readObject[T](): T = {$/;"	m
readObject	serializer/Serializer.scala	/^  def readObject[T](): T$/;"	m
readPersistedData	deploy/master/PersistenceEngine.scala	/^  def readPersistedData(): (Seq[ApplicationInfo], Seq[WorkerInfo])$/;"	m
readRDDFromPickleFile	api/python/PythonRDD.scala	/^  def readRDDFromPickleFile(sc: JavaSparkContext, filename: String, parallelism: Int) :$/;"	m
readRunnableStarted	network/ConnectionManager.scala	/^  private val readRunnableStarted: HashSet[SelectionKey] = new HashSet[SelectionKey]()$/;"	V
reader	rdd/HadoopRDD.scala	/^      var reader: RecordReader[K, V] = null$/;"	v
reader	rdd/NewHadoopRDD.scala	/^      val reader = format.createRecordReader($/;"	V
reason	executor/Executor.scala	/^          val reason = ExceptionFailure(t.getClass.getName, t.toString, t.getStackTrace, metrics)$/;"	V
reason	executor/Executor.scala	/^          val reason = ffe.toTaskEndReason$/;"	V
reason	scheduler/cluster/SparkDeploySchedulerBackend.scala	/^    val reason: ExecutorLossReason = exitStatus match {$/;"	V
reason	scheduler/cluster/TaskResultGetter.scala	/^    var reason: Option[TaskEndReason] = None$/;"	v
reason	scheduler/local/LocalTaskSetManager.scala	/^    val reason: ExceptionFailure = ser.deserialize[ExceptionFailure]($/;"	V
receive	MapOutputTracker.scala	/^  def receive = {$/;"	m
receive	scheduler/cluster/CoarseGrainedSchedulerBackend.scala	/^    def receive = {$/;"	m
receive	scheduler/local/LocalScheduler.scala	/^  def receive = {$/;"	m
receive	storage/BlockManagerMasterActor.scala	/^  def receive = {$/;"	m
receiveBroadcast	broadcast/BitTorrentBroadcast.scala	/^  def receiveBroadcast(variableID: Long): Boolean = {$/;"	m
receiveBroadcast	broadcast/TorrentBroadcast.scala	/^  def receiveBroadcast(variableID: Long): Boolean = {$/;"	m
receiveBroadcast	broadcast/TreeBroadcast.scala	/^  def receiveBroadcast(variableID: Long): Boolean = {$/;"	m
receiveMessage	network/ConnectionManager.scala	/^  def receiveMessage(connection: Connection, message: Message) {$/;"	m
receivingConnection	network/ConnectionManager.scala	/^        val receivingConnection = connection.asInstanceOf[ReceivingConnection]$/;"	V
recentExceptions	scheduler/cluster/ClusterTaskSetManager.scala	/^  val recentExceptions = HashMap[String, (Int, Long)]()$/;"	V
receptionFailed	broadcast/SourceInfo.scala	/^  var receptionFailed = false$/;"	v
receptionSucceeded	broadcast/BitTorrentBroadcast.scala	/^          val receptionSucceeded = receiveBroadcast(id)$/;"	V
receptionSucceeded	broadcast/TreeBroadcast.scala	/^          val receptionSucceeded = receiveBroadcast(id)$/;"	V
receptionSucceeded	broadcast/TreeBroadcast.scala	/^      val receptionSucceeded = receiveSingleTransmission(sourceInfo)$/;"	V
receptionSucceeded	broadcast/TreeBroadcast.scala	/^    var receptionSucceeded = false$/;"	v
receptionTime	broadcast/BitTorrentBroadcast.scala	/^              val receptionTime = (System.currentTimeMillis - recvStartTime)$/;"	V
receptionTime	broadcast/TreeBroadcast.scala	/^        val receptionTime = (System.currentTimeMillis - recvStartTime)$/;"	V
reconnectAttempts	deploy/master/SparkZooKeeperSession.scala	/^  private var reconnectAttempts = 0$/;"	v
record	rdd/PairRDDFunctions.scala	/^        val record = iter.next()$/;"	V
recordLength	storage/StoragePerfTester.scala	/^    val recordLength = 1000 \/\/ ~1KB records$/;"	V
recordMapOutput	storage/ShuffleBlockManager.scala	/^    def recordMapOutput(mapId: Int, offsets: Array[Long]) {$/;"	m
recordsPerMap	storage/StoragePerfTester.scala	/^    val recordsPerMap = totalRecords \/ numMaps$/;"	V
recvOrder	broadcast/TorrentBroadcast.scala	/^    val recvOrder = new Random().shuffle(Array.iterate(0, totalBlocks)(_ + 1).toList)$/;"	V
recvStartTime	broadcast/BitTorrentBroadcast.scala	/^              val recvStartTime = System.currentTimeMillis$/;"	V
recvStartTime	broadcast/TreeBroadcast.scala	/^        val recvStartTime = System.currentTimeMillis$/;"	V
redirectStream	deploy/worker/ExecutorRunner.scala	/^  def redirectStream(in: InputStream, file: File) {$/;"	m
reduce	api/java/JavaRDDLike.scala	/^  def reduce(f: JFunction2[T, T, T]): T = rdd.reduce(f)$/;"	m
reduce	rdd/RDD.scala	/^  def reduce(f: (T, T) => T): T = {$/;"	m
reduceByKey	api/java/JavaPairRDD.scala	/^  def reduceByKey(func: JFunction2[V, V, V]): JavaPairRDD[K, V] = {$/;"	m
reduceByKey	api/java/JavaPairRDD.scala	/^  def reduceByKey(func: JFunction2[V, V, V], numPartitions: Int): JavaPairRDD[K, V] =$/;"	m
reduceByKey	api/java/JavaPairRDD.scala	/^  def reduceByKey(partitioner: Partitioner, func: JFunction2[V, V, V]): JavaPairRDD[K, V] =$/;"	m
reduceByKey	rdd/PairRDDFunctions.scala	/^  def reduceByKey(func: (V, V) => V): RDD[(K, V)] = {$/;"	m
reduceByKey	rdd/PairRDDFunctions.scala	/^  def reduceByKey(func: (V, V) => V, numPartitions: Int): RDD[(K, V)] = {$/;"	m
reduceByKey	rdd/PairRDDFunctions.scala	/^  def reduceByKey(partitioner: Partitioner, func: (V, V) => V): RDD[(K, V)] = {$/;"	m
reduceByKeyLocally	api/java/JavaPairRDD.scala	/^  def reduceByKeyLocally(func: JFunction2[V, V, V]): java.util.Map[K, V] =$/;"	m
reduceByKeyLocally	rdd/PairRDDFunctions.scala	/^  def reduceByKeyLocally(func: (V, V) => V): Map[K, V] = {$/;"	m
reduceByKeyToDriver	rdd/PairRDDFunctions.scala	/^  def reduceByKeyToDriver(func: (V, V) => V): Map[K, V] = reduceByKeyLocally(func)$/;"	m
reducePartition	rdd/PairRDDFunctions.scala	/^    def reducePartition(iter: Iterator[(K, V)]): Iterator[JHashMap[K, V]] = {$/;"	m
reducePartition	rdd/RDD.scala	/^    val reducePartition: Iterator[T] => Option[T] = iter => {$/;"	V
reg	serializer/KryoSerializer.scala	/^        val reg = Class.forName(regCls, true, classLoader).newInstance().asInstanceOf[KryoRegistrator]$/;"	V
register	Accumulators.scala	/^  def register(a: Accumulable[_, _], original: Boolean): Unit = synchronized {$/;"	m
register	network/ConnectionManager.scala	/^        var register: Boolean = false$/;"	v
registerApplication	deploy/master/Master.scala	/^  def registerApplication(app: ApplicationInfo): Unit = {$/;"	m
registerBroadcast	broadcast/MultiTracker.scala	/^  def registerBroadcast(id: Long, gInfo: SourceInfo) {$/;"	m
registerClasses	serializer/KryoSerializer.scala	/^  def registerClasses(kryo: Kryo)$/;"	m
registerMapOutput	MapOutputTracker.scala	/^  def registerMapOutput(shuffleId: Int, mapId: Int, status: MapStatus) {$/;"	m
registerOrLookup	SparkEnv.scala	/^    def registerOrLookup(name: String, newActor: => Actor): ActorRef = {$/;"	m
registerRequests	network/ConnectionManager.scala	/^  private val registerRequests = new SynchronizedQueue[SendingConnection]$/;"	V
registerShuffle	MapOutputTracker.scala	/^  def registerShuffle(shuffleId: Int, numMaps: Int) {$/;"	m
registerShutdownDeleteDir	util/Utils.scala	/^  def registerShutdownDeleteDir(file: File) {$/;"	m
registerSource	metrics/MetricsSystem.scala	/^  def registerSource(source: Source) {$/;"	m
registerWorker	deploy/master/Master.scala	/^  def registerWorker(worker: WorkerInfo): Unit = {$/;"	m
registered	deploy/client/Client.scala	/^  var registered = false$/;"	v
registered	deploy/worker/Worker.scala	/^  @volatile var registered = false$/;"	v
registeredLock	scheduler/cluster/mesos/CoarseMesosSchedulerBackend.scala	/^  val registeredLock = new Object()$/;"	V
registeredLock	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^  val registeredLock = new Object()$/;"	V
registry	metrics/MetricsSystem.scala	/^  val registry = new MetricRegistry()$/;"	V
rehashIfNeeded	util/collection/OpenHashSet.scala	/^  def rehashIfNeeded(k: T, allocateFunc: (Int) => Unit, moveFunc: (Int, Int) => Unit) {$/;"	m
releaseWriters	storage/ShuffleBlockManager.scala	/^  def releaseWriters(success: Boolean)$/;"	m
rem	util/SizeEstimator.scala	/^    val rem = size % ALIGN_SIZE$/;"	V
remainingBytes	network/Connection.scala	/^          val remainingBytes = buffer.remaining$/;"	V
remainingMem	storage/BlockManagerMasterActor.scala	/^    def remainingMem: Long = _remainingMem$/;"	m
remainingMem	storage/BlockManagerSource.scala	/^      val remainingMem = storageStatusList.map(_.memRemaining).reduce(_ + _)$/;"	V
remote	storage/BlockManager.scala	/^    val remote = getRemote(blockId)$/;"	V
remoteAddress	network/Connection.scala	/^  val remoteAddress = getRemoteAddress()$/;"	V
remoteBlocksFetched	executor/TaskMetrics.scala	/^  var remoteBlocksFetched: Int = _$/;"	v
remoteBlocksToFetch	storage/BlockFetcherIterator.scala	/^    protected val remoteBlocksToFetch = new HashSet[BlockId]()$/;"	V
remoteBytesRead	executor/TaskMetrics.scala	/^  var remoteBytesRead: Long = _$/;"	v
remoteBytesRead	storage/BlockFetchTracker.scala	/^  def remoteBytesRead : Long$/;"	m
remoteConnectionManagerId	network/ConnectionManager.scala	/^        val remoteConnectionManagerId = receivingConnection.getRemoteConnectionManagerId()$/;"	V
remoteFetchTime	executor/TaskMetrics.scala	/^  var remoteFetchTime: Long = _$/;"	v
remoteFetchTime	storage/BlockFetchTracker.scala	/^  def remoteFetchTime : Long$/;"	m
remoteRequests	storage/BlockFetcherIterator.scala	/^      val remoteRequests = new ArrayBuffer[FetchRequest]$/;"	V
remoteRequests	storage/BlockFetcherIterator.scala	/^      val remoteRequests = splitLocalRemoteBlocks()$/;"	V
remoteStartTime	storage/BlockManager.scala	/^          val remoteStartTime = System.currentTimeMillis$/;"	V
remove	storage/BlockStore.scala	/^  def remove(blockId: BlockId): Boolean$/;"	m
removeApplication	deploy/master/Master.scala	/^  def removeApplication(app: ApplicationInfo, state: ApplicationState.Value) {$/;"	m
removeApplication	deploy/master/PersistenceEngine.scala	/^  def removeApplication(app: ApplicationInfo)$/;"	m
removeBlock	storage/BlockManager.scala	/^  def removeBlock(blockId: BlockId, tellMaster: Boolean = true) {$/;"	m
removeBlock	storage/BlockManagerMaster.scala	/^  def removeBlock(blockId: BlockId) {$/;"	m
removeBlock	storage/BlockManagerMasterActor.scala	/^    def removeBlock(blockId: BlockId) {$/;"	m
removeConnection	network/ConnectionManager.scala	/^  def removeConnection(connection: Connection) {$/;"	m
removeExecutor	deploy/master/ApplicationInfo.scala	/^  def removeExecutor(exec: ExecutorInfo) {$/;"	m
removeExecutor	deploy/master/WorkerInfo.scala	/^  def removeExecutor(exec: ExecutorInfo) {$/;"	m
removeExecutor	scheduler/cluster/CoarseGrainedSchedulerBackend.scala	/^    def removeExecutor(executorId: String, reason: String) {$/;"	m
removeExecutor	scheduler/cluster/CoarseGrainedSchedulerBackend.scala	/^  def removeExecutor(executorId: String, reason: String) {$/;"	m
removeExecutor	storage/BlockManagerMaster.scala	/^  def removeExecutor(execId: String) {$/;"	m
removeMessage	network/Connection.scala	/^    def removeMessage(message: Message) {$/;"	m
removeMsg	storage/BlockManagerMasterActor.scala	/^    val removeMsg = RemoveRdd(rddId)$/;"	V
removeOutputLoc	scheduler/Stage.scala	/^  def removeOutputLoc(partition: Int, bmAddress: BlockManagerId) {$/;"	m
removeOutputsOnExecutor	scheduler/Stage.scala	/^  def removeOutputsOnExecutor(execId: String) {$/;"	m
removeRdd	storage/BlockManager.scala	/^  def removeRdd(rddId: Int): Int = {$/;"	m
removeRdd	storage/BlockManagerMaster.scala	/^  def removeRdd(rddId: Int, blocking: Boolean) {$/;"	m
removeRunningTask	scheduler/cluster/ClusterTaskSetManager.scala	/^  def removeRunningTask(tid: Long) {$/;"	m
removeSchedulable	scheduler/Schedulable.scala	/^  def removeSchedulable(schedulable: Schedulable): Unit$/;"	m
removeSource	metrics/MetricsSystem.scala	/^  def removeSource(source: Source) {$/;"	m
removeWorker	deploy/master/Master.scala	/^  def removeWorker(worker: WorkerInfo) {$/;"	m
removeWorker	deploy/master/PersistenceEngine.scala	/^  def removeWorker(worker: WorkerInfo)$/;"	m
removedFromDisk	storage/BlockManager.scala	/^      val removedFromDisk = diskStore.remove(blockId)$/;"	V
removedFromMemory	storage/BlockManager.scala	/^      val removedFromMemory = memoryStore.remove(blockId)$/;"	V
render	deploy/master/ui/ApplicationPage.scala	/^  def render(request: HttpServletRequest): Seq[Node] = {$/;"	m
render	deploy/master/ui/IndexPage.scala	/^  def render(request: HttpServletRequest): Seq[Node] = {$/;"	m
render	deploy/worker/ui/IndexPage.scala	/^  def render(request: HttpServletRequest): Seq[Node] = {$/;"	m
render	ui/exec/ExecutorsUI.scala	/^  def render(request: HttpServletRequest): Seq[Node] = {$/;"	m
render	ui/jobs/IndexPage.scala	/^  def render(request: HttpServletRequest): Seq[Node] = {$/;"	m
render	ui/jobs/PoolPage.scala	/^  def render(request: HttpServletRequest): Seq[Node] = {$/;"	m
render	ui/jobs/StagePage.scala	/^  def render(request: HttpServletRequest): Seq[Node] = {$/;"	m
render	ui/storage/IndexPage.scala	/^  def render(request: HttpServletRequest): Seq[Node] = {$/;"	m
render	ui/storage/RDDPage.scala	/^  def render(request: HttpServletRequest): Seq[Node] = {$/;"	m
renderJson	deploy/master/ui/ApplicationPage.scala	/^  def renderJson(request: HttpServletRequest): JValue = {$/;"	m
renderJson	deploy/master/ui/IndexPage.scala	/^  def renderJson(request: HttpServletRequest): JValue = {$/;"	m
renderJson	deploy/worker/ui/IndexPage.scala	/^  def renderJson(request: HttpServletRequest): JValue = {$/;"	m
repartition	api/java/JavaDoubleRDD.scala	/^  def repartition(numPartitions: Int): JavaDoubleRDD = fromRDD(srdd.repartition(numPartitions))$/;"	m
repartition	api/java/JavaPairRDD.scala	/^  def repartition(numPartitions: Int): JavaPairRDD[K, V] = fromRDD(rdd.repartition(numPartitions))$/;"	m
repartition	api/java/JavaRDD.scala	/^  def repartition(numPartitions: Int): JavaRDD[T] = rdd.repartition(numPartitions)$/;"	m
repartition	rdd/RDD.scala	/^  def repartition(numPartitions: Int): RDD[T] = {$/;"	m
replClassLoader	executor/Executor.scala	/^  private val replClassLoader = addReplClassLoaderIfNeeded(urlClassLoader)$/;"	V
replication	storage/BlockMessage.scala	/^      val replication = buffer.getInt()$/;"	V
replication	storage/StorageLevel.scala	/^  def replication = replication_$/;"	m
replicationFuture	storage/BlockManager.scala	/^    val replicationFuture = if (data.isRight && level.replication > 1) {$/;"	V
replication_	storage/StorageLevel.scala	/^    private var replication_ : Int = 1)$/;"	v
reportBlockStatus	storage/BlockManager.scala	/^  def reportBlockStatus(blockId: BlockId, info: BlockInfo, droppedMemorySize: Long = 0L) {$/;"	m
reporter	metrics/sink/ConsoleSink.scala	/^  val reporter: ConsoleReporter = ConsoleReporter.forRegistry(registry)$/;"	V
reporter	metrics/sink/CsvSink.scala	/^  val reporter: CsvReporter = CsvReporter.forRegistry(registry)$/;"	V
reporter	metrics/sink/GangliaSink.scala	/^  val reporter: GangliaReporter = GangliaReporter.forRegistry(registry)$/;"	V
reporter	metrics/sink/JmxSink.scala	/^  val reporter: JmxReporter = JmxReporter.forRegistry(registry).build()$/;"	V
res	deploy/FaultToleranceTest.scala	/^        val res = sc.parallelize(0 until 10).collect()$/;"	V
res	rdd/PairRDDFunctions.scala	/^        val res = self.context.runJob(self, process _, Array(index), false)$/;"	V
res	rdd/RDD.scala	/^      val res = sc.runJob(this, (it: Iterator[T]) => it.take(left).toArray, p, allowLocal = true)$/;"	V
res	scheduler/SchedulingAlgorithm.scala	/^    var res = math.signum(priority1 - priority2)$/;"	v
res	scheduler/SchedulingAlgorithm.scala	/^    var res:Boolean = true$/;"	v
res	storage/BlockManager.scala	/^              val res = diskStore.putValues(blockId, values, level, askForBytes)$/;"	V
res	storage/BlockManager.scala	/^              val res = memoryStore.putValues(blockId, values, level, true)$/;"	V
res	storage/BlockManagerMaster.scala	/^    val res = askDriverWithReply[Boolean]($/;"	V
resHandler	HttpServer.scala	/^      val resHandler = new ResourceHandler$/;"	V
resetForceReregister	network/Connection.scala	/^  def resetForceReregister(): Boolean = {$/;"	m
resetForceReregister	network/Connection.scala	/^  def resetForceReregister(): Boolean$/;"	m
resetIterator	rdd/CoalescedRDD.scala	/^    def resetIterator() = {$/;"	m
resize	util/collection/PrimitiveVector.scala	/^  def resize(newLength: Int): PrimitiveVector[V] = {$/;"	m
resourceOffer	scheduler/local/LocalScheduler.scala	/^  def resourceOffer(freeCores: Int): Seq[TaskDescription] = {$/;"	m
resourceOffers	scheduler/cluster/ClusterScheduler.scala	/^  def resourceOffers(offers: Seq[WorkerOffer]): Seq[Seq[TaskDescription]] = synchronized {$/;"	m
resourceStream	util/ClosureCleaner.scala	/^    val resourceStream = cls.getResourceAsStream(className)$/;"	V
resp	deploy/master/Master.scala	/^    val resp = Await.result(respFuture, timeoutDuration).asInstanceOf[WebUIPortResponse]$/;"	V
respFuture	deploy/master/Master.scala	/^    val respFuture = actor ? RequestWebUIPort   \/\/ ask pattern$/;"	V
responseMessage	storage/BlockManagerWorker.scala	/^    val responseMessage = connectionManager.sendMessageReliablySync($/;"	V
responseMessages	storage/BlockManagerWorker.scala	/^          val responseMessages = blockMessages.map(processBlockMessage).filter(_ != None).map(_.get)$/;"	V
responseStr	network/SenderTest.scala	/^      val responseStr = manager.sendMessageReliablySync(targetConnectionManagerId, dataMessage) match {$/;"	V
result	SparkContext.scala	/^    val result = dagScheduler.runApproximateJob(rdd, func, evaluator, callSite, timeout,$/;"	V
result	network/Connection.scala	/^      val result = needForceReregister$/;"	V
result	partial/GroupedCountEvaluator.scala	/^      val result = new JHashMap[T, BoundedDouble](sums.size)$/;"	V
result	partial/GroupedMeanEvaluator.scala	/^      val result = new JHashMap[T, BoundedDouble](sums.size)$/;"	V
result	partial/GroupedSumEvaluator.scala	/^      val result = new JHashMap[T, BoundedDouble](sums.size)$/;"	V
result	rdd/AsyncRDDActions.scala	/^        var result = 0L$/;"	v
result	rdd/NewHadoopRDD.scala	/^    val result = new Array[Partition](rawSplits.size)$/;"	V
result	rdd/RDD.scala	/^      var result = 0L$/;"	v
result	scheduler/DAGScheduler.scala	/^        val result = job.func(taskContext, rdd.iterator(split, taskContext))$/;"	V
result	scheduler/cluster/TaskResultGetter.scala	/^          val result = serializer.get().deserialize[TaskResult[_]](serializedData) match {$/;"	V
result	scheduler/local/LocalTaskSetManager.scala	/^    val result = ser.deserialize[TaskResult[_]](serializedData, getClass.getClassLoader) match {$/;"	V
result	storage/BlockFetcherIterator.scala	/^      val result = results.take()$/;"	V
result	storage/BlockManager.scala	/^          val result = if (asValues) {$/;"	V
result	storage/BlockManagerMaster.scala	/^        val result = Await.result(future, timeout)$/;"	V
result	storage/BlockManagerMaster.scala	/^    val result = askDriverWithReply[Seq[BlockManagerId]](GetPeers(blockManagerId, numPeers))$/;"	V
result	storage/StorageLevel.scala	/^    var result = ""$/;"	v
result	ui/JettyUtils.scala	/^        val result = responder(request)$/;"	V
result	util/Utils.scala	/^    var result = bytes(7) & 0xFFL$/;"	v
resultMessage	storage/BlockManagerWorker.scala	/^    val resultMessage = connectionManager.sendMessageReliablySync($/;"	V
resultObject	partial/ApproximateActionListener.scala	/^  var resultObject: Option[PartialResult[R]] = None \/\/ Set if we've already returned a PartialResult$/;"	v
resultSetToObjectArray	rdd/JdbcRDD.scala	/^  def resultSetToObjectArray(rs: ResultSet) = {$/;"	m
resultSize	executor/TaskMetrics.scala	/^  var resultSize: Long = _$/;"	v
resultStageToJob	scheduler/DAGScheduler.scala	/^  val resultStageToJob = new HashMap[Stage, ActiveJob]$/;"	V
resultStr	network/ConnectionManagerTest.scala	/^        val resultStr = thisConnManagerId + " Sent " + mb + " MB in " + ms + " ms at " + (mb \/ ms * 1000.0) + " MB\/s"$/;"	V
resultStr	network/SenderTest.scala	/^      \/*val resultStr = "Sent " + mb + " MB " + targetServer + " in " + ms + " ms at " + (mb \/ ms * 1000.0) + " MB\/s"*\/$/;"	V
resultStr	network/SenderTest.scala	/^      val resultStr = "Sent " + mb + " MB " + targetServer + " in " + ms + " ms (" +  (mb \/ ms * 1000.0).toInt + "MB\/s) | Response = " + responseStr$/;"	V
resultStrs	network/ConnectionManagerTest.scala	/^      val resultStrs = sc.parallelize(0 until tasknum, tasknum).map(i => {$/;"	V
results	SparkContext.scala	/^    val results = new Array[U](partitions.size)$/;"	V
results	network/ConnectionManagerTest.scala	/^        val results = futures.map(f => Await.result(f, awaitTime))$/;"	V
results	rdd/AsyncRDDActions.scala	/^      val results = new ArrayBuffer[T](num)$/;"	V
results	rdd/AsyncRDDActions.scala	/^    val results = new Array[Array[T]](self.partitions.size)$/;"	V
results	rdd/RDD.scala	/^    val results = sc.runJob(this, (iter: Iterator[T]) => iter.toArray)$/;"	V
results	storage/BlockFetcherIterator.scala	/^    protected val results = new LinkedBlockingQueue[FetchResult]$/;"	V
resultsGotten	storage/BlockFetcherIterator.scala	/^    @volatile protected var resultsGotten = 0$/;"	v
ret	Accumulators.scala	/^    val ret = Map[Long, Any]()$/;"	V
ret	scheduler/cluster/mesos/CoarseMesosSchedulerBackend.scala	/^            val ret = driver.run()$/;"	V
ret	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^            val ret = driver.run()$/;"	V
ret	storage/StorageLevel.scala	/^    var ret = 0$/;"	v
ret	util/SerializableBuffer.scala	/^      val ret = channel.read(buffer)$/;"	V
ret	util/collection/OpenHashMap.scala	/^        val ret = (_keySet.getValue(pos), _values(pos))$/;"	V
ret	util/collection/PrimitiveKeyOpenHashMap.scala	/^        val ret = (_keySet.getValue(pos), _values(pos))$/;"	V
retByteArray	broadcast/MultiTracker.scala	/^    var retByteArray = new Array[Byte](totalBytes)$/;"	v
retByteArray	broadcast/TorrentBroadcast.scala	/^    var retByteArray = new Array[Byte](totalBytes)$/;"	v
retVal	broadcast/MultiTracker.scala	/^    val retVal = in.readObject.asInstanceOf[OUT]$/;"	V
retVal	broadcast/MultiTracker.scala	/^    var retVal = new Array[BroadcastBlock](blockNum)$/;"	v
retVal	broadcast/TorrentBroadcast.scala	/^    var retVal = new Array[TorrentBlock](blockNum)$/;"	v
retries	deploy/client/Client.scala	/^      var retries = 0$/;"	v
retries	deploy/worker/Worker.scala	/^    var retries = 0$/;"	v
retriesLeft	broadcast/MultiTracker.scala	/^    var retriesLeft = MultiTracker.MaxRetryCount$/;"	v
retriesLeft	broadcast/TreeBroadcast.scala	/^    var retriesLeft = MultiTracker.MaxRetryCount$/;"	v
retry	deploy/master/SparkZooKeeperSession.scala	/^  def retry[T](fn: => T, n: Int = MAX_RECONNECT_ATTEMPTS): T = {$/;"	m
retryCount	deploy/master/ApplicationInfo.scala	/^  def retryCount = _retryCount$/;"	m
retryTimer	deploy/client/Client.scala	/^      lazy val retryTimer: Cancellable =$/;"	V
retryTimer	deploy/worker/Worker.scala	/^    lazy val retryTimer: Cancellable =$/;"	V
returnType	api/java/function/Function.java	/^  public ClassManifest<R> returnType() {$/;"	m	class:Function
returnType	api/java/function/Function2.java	/^  public ClassManifest<R> returnType() {$/;"	m	class:Function2
returnType	api/java/function/Function3.java	/^  public ClassManifest<R> returnType() {$/;"	m	class:Function3
returnType	api/java/function/Function4.java	/^  public ClassManifest<R> returnType() {$/;"	m	class:Function4
retval	scheduler/InputFormatInfo.scala	/^    val retval = new ArrayBuffer[SplitInfo]()$/;"	V
retval	scheduler/SplitInfo.scala	/^    val retval = new ArrayBuffer[SplitInfo]()$/;"	V
retval	scheduler/cluster/ClusterScheduler.scala	/^    val retval = new ArrayBuffer[T](keyList.size * 2)$/;"	V
retval	util/Utils.scala	/^      val retval = (hostPort, 0)$/;"	V
retval	util/Utils.scala	/^    val retval = (hostPort.substring(0, indx).trim(), hostPort.substring(indx + 1).trim().toInt)$/;"	V
retval	util/Utils.scala	/^    val retval = System.getProperty("spark.hostPort", null)$/;"	V
retval	util/Utils.scala	/^    val retval = shutdownDeletePaths.synchronized {$/;"	V
reviveInterval	scheduler/cluster/CoarseGrainedSchedulerBackend.scala	/^      val reviveInterval = System.getProperty("spark.scheduler.revive.interval", "1000").toLong$/;"	V
reviveOffers	scheduler/cluster/SchedulerBackend.scala	/^  def reviveOffers(): Unit$/;"	m
rf	util/ClosureCleaner.scala	/^      val rf = sun.reflect.ReflectionFactory.getReflectionFactory()$/;"	V
rg	rdd/SampledRDD.scala	/^    val rg = new Random(seed)$/;"	V
rightOuterJoin	api/java/JavaPairRDD.scala	/^  def rightOuterJoin[W](other: JavaPairRDD[K, W]): JavaPairRDD[K, (Optional[V], W)] = {$/;"	m
rightOuterJoin	api/java/JavaPairRDD.scala	/^  def rightOuterJoin[W](other: JavaPairRDD[K, W], numPartitions: Int): JavaPairRDD[K, (Optional[V], W)] = {$/;"	m
rightOuterJoin	api/java/JavaPairRDD.scala	/^  def rightOuterJoin[W](other: JavaPairRDD[K, W], partitioner: Partitioner)$/;"	m
rightOuterJoin	rdd/PairRDDFunctions.scala	/^  def rightOuterJoin[W](other: RDD[(K, W)]): RDD[(K, (Option[V], W))] = {$/;"	m
rightOuterJoin	rdd/PairRDDFunctions.scala	/^  def rightOuterJoin[W](other: RDD[(K, W)], numPartitions: Int): RDD[(K, (Option[V], W))] = {$/;"	m
rightOuterJoin	rdd/PairRDDFunctions.scala	/^  def rightOuterJoin[W](other: RDD[(K, W)], partitioner: Partitioner)$/;"	m
rnd	rdd/CoalescedRDD.scala	/^  val rnd = new scala.util.Random(7919) \/\/ keep this class deterministic$/;"	V
rootPool	scheduler/SchedulableBuilder.scala	/^  def rootPool: Pool$/;"	m
rootPool	scheduler/SchedulableBuilder.scala	/^private[spark] class FIFOSchedulableBuilder(val rootPool: Pool)$/;"	V
rootPool	scheduler/SchedulableBuilder.scala	/^private[spark] class FairSchedulableBuilder(val rootPool: Pool)$/;"	V
rootPool	scheduler/TaskScheduler.scala	/^  def rootPool: Pool$/;"	m
rootPool	scheduler/cluster/ClusterScheduler.scala	/^  var rootPool: Pool = null$/;"	v
rootPool	scheduler/local/LocalScheduler.scala	/^  var rootPool: Pool = null$/;"	v
rotIt	rdd/CoalescedRDD.scala	/^    val rotIt = new LocationIterator(prev)$/;"	V
rs	rdd/JdbcRDD.scala	/^    val rs = stmt.executeQuery()$/;"	V
run	FutureAction.scala	/^  def run(func: => T)(implicit executor: ExecutionContext): this.type = {$/;"	m
run	deploy/SparkHadoopUtil.scala	/^        def run: Unit = func()$/;"	m
run	executor/CoarseGrainedExecutorBackend.scala	/^  def run(driverUrl: String, executorId: String, hostname: String, cores: Int) {$/;"	m
run	rdd/CoalescedRDD.scala	/^  def run(): Array[PartitionGroup] = {$/;"	m
runAsUser	deploy/SparkHadoopUtil.scala	/^  def runAsUser(user: String)(func: () => Unit) {$/;"	m
runJob	SparkContext.scala	/^  def runJob[T, U: ClassManifest]($/;"	m
runJob	SparkContext.scala	/^  def runJob[T, U: ClassManifest](rdd: RDD[T], func: (TaskContext, Iterator[T]) => U): Array[U] = {$/;"	m
runJob	SparkContext.scala	/^  def runJob[T, U: ClassManifest](rdd: RDD[T], func: Iterator[T] => U): Array[U] = {$/;"	m
runJob	scheduler/DAGScheduler.scala	/^  def runJob[T, U: ClassManifest]($/;"	m
runScript	scheduler/cluster/mesos/CoarseMesosSchedulerBackend.scala	/^      val runScript = new File(sparkHome, "spark-class").getCanonicalPath$/;"	V
runTask	scheduler/Task.scala	/^  def runTask(context: TaskContext): T$/;"	m
runnable	network/ConnectionManager.scala	/^    val runnable = new Runnable() {$/;"	V
runner	deploy/worker/ExecutorRunner.scala	/^    val runner = getAppEnv("JAVA_HOME").map(_ + "\/bin\/java").getOrElse("java")$/;"	V
runner	executor/MesosExecutorBackend.scala	/^    val runner = new MesosExecutorBackend()$/;"	V
running	scheduler/DAGScheduler.scala	/^  val running = new HashSet[Stage] \/\/ Stages we are running right now$/;"	V
running	scheduler/TaskInfo.scala	/^  def running: Boolean = !finished$/;"	m
runningExecutorTable	deploy/worker/ui/IndexPage.scala	/^    val runningExecutorTable =$/;"	V
runningLocally	TaskContext.scala	/^  val runningLocally: Boolean = false,$/;"	V
runningTasks	executor/Executor.scala	/^  private val runningTasks = new ConcurrentHashMap[Long, TaskRunner]$/;"	V
runningTasks	scheduler/Pool.scala	/^  var runningTasks = 0$/;"	v
runningTasks	scheduler/Schedulable.scala	/^  def runningTasks: Int$/;"	m
runningTasks	scheduler/cluster/ClusterTaskSetManager.scala	/^  var runningTasks = 0$/;"	v
runningTasks	scheduler/local/LocalTaskSetManager.scala	/^  var runningTasks: Int = 0$/;"	v
runningTasks1	scheduler/SchedulingAlgorithm.scala	/^    val runningTasks1 = s1.runningTasks$/;"	V
runningTasks2	scheduler/SchedulingAlgorithm.scala	/^    val runningTasks2 = s2.runningTasks$/;"	V
runningTasksSet	scheduler/cluster/ClusterTaskSetManager.scala	/^  private val runningTasksSet = new HashSet[Long]$/;"	V
runtimePcts	scheduler/SparkListener.scala	/^    val runtimePcts = stageCompleted.stage.taskInfos.map{$/;"	V
rxSourceInfo	broadcast/BitTorrentBroadcast.scala	/^          var rxSourceInfo = ois.readObject.asInstanceOf[SourceInfo]$/;"	v
s	api/python/PythonRDD.scala	/^      val s = elem.asInstanceOf[String].getBytes("UTF-8")$/;"	V
s1	rdd/CartesianRDD.scala	/^  var s1 = rdd1.partitions(s1Index)$/;"	v
s1Needy	scheduler/SchedulingAlgorithm.scala	/^    val s1Needy = runningTasks1 < minShare1$/;"	V
s2	rdd/CartesianRDD.scala	/^  var s2 = rdd2.partitions(s2Index)$/;"	v
s2Needy	scheduler/SchedulingAlgorithm.scala	/^    val s2Needy = runningTasks2 < minShare2$/;"	V
sample	api/java/JavaDoubleRDD.scala	/^  def sample(withReplacement: Boolean, fraction: Double, seed: Int): JavaDoubleRDD =$/;"	m
sample	api/java/JavaPairRDD.scala	/^  def sample(withReplacement: Boolean, fraction: Double, seed: Int): JavaPairRDD[K, V] =$/;"	m
sample	api/java/JavaRDD.scala	/^  def sample(withReplacement: Boolean, fraction: Double, seed: Int): JavaRDD[T] =$/;"	m
sample	rdd/RDD.scala	/^  def sample(withReplacement: Boolean, fraction: Double, seed: Int): RDD[T] =$/;"	m
sampleStdev	api/java/JavaDoubleRDD.scala	/^  def sampleStdev(): Double = srdd.sampleStdev()$/;"	m
sampleStdev	rdd/DoubleRDDFunctions.scala	/^  def sampleStdev(): Double = stats().sampleStdev$/;"	m
sampleStdev	util/StatCounter.scala	/^  def sampleStdev: Double = math.sqrt(sampleVariance)$/;"	m
sampleVariance	api/java/JavaDoubleRDD.scala	/^  def sampleVariance(): Double = srdd.sampleVariance()$/;"	m
sampleVariance	rdd/DoubleRDDFunctions.scala	/^  def sampleVariance(): Double = stats().sampleVariance$/;"	m
sampleVariance	util/StatCounter.scala	/^  def sampleVariance: Double = {$/;"	m
samples	rdd/RDD.scala	/^    var samples = this.sample(withReplacement, fraction, rand.nextInt()).collect()$/;"	v
saveAsHadoopDataset	api/java/JavaPairRDD.scala	/^  def saveAsHadoopDataset(conf: JobConf) {$/;"	m
saveAsHadoopDataset	rdd/PairRDDFunctions.scala	/^  def saveAsHadoopDataset(conf: JobConf) {$/;"	m
saveAsHadoopFile	api/java/JavaPairRDD.scala	/^  def saveAsHadoopFile[F <: OutputFormat[_, _]]($/;"	m
saveAsHadoopFile	rdd/PairRDDFunctions.scala	/^  def saveAsHadoopFile[F <: OutputFormat[K, V]]($/;"	m
saveAsHadoopFile	rdd/PairRDDFunctions.scala	/^  def saveAsHadoopFile[F <: OutputFormat[K, V]](path: String)(implicit fm: ClassManifest[F]) {$/;"	m
saveAsNewAPIHadoopFile	api/java/JavaPairRDD.scala	/^  def saveAsNewAPIHadoopFile[F <: NewOutputFormat[_, _]]($/;"	m
saveAsNewAPIHadoopFile	rdd/PairRDDFunctions.scala	/^  def saveAsNewAPIHadoopFile[F <: NewOutputFormat[K, V]](path: String)(implicit fm: ClassManifest[F]) {$/;"	m
saveAsObjectFile	api/java/JavaRDDLike.scala	/^  def saveAsObjectFile(path: String) = rdd.saveAsObjectFile(path)$/;"	m
saveAsObjectFile	rdd/RDD.scala	/^  def saveAsObjectFile(path: String) {$/;"	m
saveAsSequenceFile	rdd/SequenceFileRDDFunctions.scala	/^  def saveAsSequenceFile(path: String, codec: Option[Class[_ <: CompressionCodec]] = None) {$/;"	m
saveAsTextFile	api/java/JavaRDDLike.scala	/^  def saveAsTextFile(path: String) = rdd.saveAsTextFile(path)$/;"	m
saveAsTextFile	api/java/JavaRDDLike.scala	/^  def saveAsTextFile(path: String, codec: Class[_ <: CompressionCodec]) =$/;"	m
saveAsTextFile	rdd/RDD.scala	/^  def saveAsTextFile(path: String) {$/;"	m
saveAsTextFile	rdd/RDD.scala	/^  def saveAsTextFile(path: String, codec: Class[_ <: CompressionCodec]) {$/;"	m
sb	scheduler/JobLogger.scala	/^    val sb = new StringBuilder()$/;"	V
sc	api/java/JavaSparkContext.scala	/^class JavaSparkContext(val sc: SparkContext) extends JavaSparkContextVarargsWorkaround {$/;"	V
sc	deploy/FaultToleranceTest.scala	/^  var sc: SparkContext = _$/;"	v
sc	network/ConnectionManagerTest.scala	/^    val sc = new SparkContext(args(0), "ConnectionManagerTest")$/;"	V
sc	rdd/CheckpointRDD.scala	/^    val sc = new SparkContext(cluster, "CheckpointRDD Test")$/;"	V
sc	rdd/RDD.scala	/^    @transient private var sc: SparkContext,$/;"	v
sc	scheduler/SparkListener.scala	/^    implicit val sc = stageCompleted$/;"	V
sc	scheduler/cluster/ClusterScheduler.scala	/^private[spark] class ClusterScheduler(val sc: SparkContext)$/;"	V
sc	storage/StoragePerfTester.scala	/^    val sc = new SparkContext("local[4]", "Write Tester")$/;"	V
sc	ui/UIWorkloadGenerator.scala	/^    val sc = new SparkContext(master, appName)$/;"	V
sc	ui/exec/ExecutorsUI.scala	/^private[spark] class ExecutorsUI(val sc: SparkContext) {$/;"	V
sc	ui/jobs/JobProgressListener.scala	/^private[spark] class JobProgressListener(val sc: SparkContext) extends SparkListener {$/;"	V
sc	ui/jobs/JobProgressUI.scala	/^private[spark] class JobProgressUI(val sc: SparkContext) {$/;"	V
sc	ui/storage/BlockManagerUI.scala	/^private[spark] class BlockManagerUI(val sc: SparkContext) extends Logging {$/;"	V
sc	ui/storage/IndexPage.scala	/^  val sc = parent.sc$/;"	V
sc	ui/storage/RDDPage.scala	/^  val sc = parent.sc$/;"	V
scala.Seq	ui/jobs/JobProgressListener.scala	/^import scala.Seq$/;"	i
scala.Seq	ui/jobs/JobProgressUI.scala	/^import scala.Seq$/;"	i
scala.Serializable	deploy/master/FileSystemPersistenceEngine.scala	/^import scala.Serializable$/;"	i
scala.Serializable	rdd/ParallelCollectionRDD.scala	/^import scala.Serializable$/;"	i
scala.Some	rdd/CoalescedRDD.scala	/^import scala.Some$/;"	i
scala.Tuple2	api/java/JavaPairRDD.scala	/^import scala.Tuple2$/;"	i
scala.Tuple2	api/java/JavaRDDLike.scala	/^import scala.Tuple2$/;"	i
scala.annotation.tailrec	ui/JettyUtils.scala	/^import scala.annotation.tailrec$/;"	i
scala.annotation.tailrec	util/RateLimitedOutputStream.scala	/^import scala.annotation.tailrec$/;"	i
scala.collection.JavaConversions	api/java/JavaSparkContext.scala	/^import scala.collection.JavaConversions$/;"	i
scala.collection.JavaConversions	util/TimeStampedHashMap.scala	/^import scala.collection.JavaConversions$/;"	i
scala.collection.JavaConversions	util/TimeStampedHashSet.scala	/^import scala.collection.JavaConversions$/;"	i
scala.collection.JavaConversions._	api/java/JavaPairRDD.scala	/^import scala.collection.JavaConversions._$/;"	i
scala.collection.JavaConversions._	api/java/JavaRDDLike.scala	/^    import scala.collection.JavaConversions._$/;"	i
scala.collection.JavaConversions._	api/java/JavaRDDLike.scala	/^import scala.collection.JavaConversions._$/;"	i
scala.collection.JavaConversions._	api/java/JavaSparkContext.scala	/^import scala.collection.JavaConversions._$/;"	i
scala.collection.JavaConversions._	api/python/PythonRDD.scala	/^import scala.collection.JavaConversions._$/;"	i
scala.collection.JavaConversions._	api/python/PythonWorkerFactory.scala	/^import scala.collection.JavaConversions._$/;"	i
scala.collection.JavaConversions._	deploy/master/SparkZooKeeperSession.scala	/^import scala.collection.JavaConversions._$/;"	i
scala.collection.JavaConversions._	executor/Executor.scala	/^import scala.collection.JavaConversions._$/;"	i
scala.collection.JavaConversions._	executor/ExecutorSource.scala	/^import scala.collection.JavaConversions._$/;"	i
scala.collection.JavaConversions._	metrics/MetricsConfig.scala	/^      import scala.collection.JavaConversions._$/;"	i
scala.collection.JavaConversions._	metrics/MetricsConfig.scala	/^    import scala.collection.JavaConversions._$/;"	i
scala.collection.JavaConversions._	rdd/PairRDDFunctions.scala	/^import scala.collection.JavaConversions._$/;"	i
scala.collection.JavaConversions._	rdd/PipedRDD.scala	/^import scala.collection.JavaConversions._$/;"	i
scala.collection.JavaConversions._	rdd/SubtractedRDD.scala	/^import scala.collection.JavaConversions._$/;"	i
scala.collection.JavaConversions._	scheduler/InputFormatInfo.scala	/^import scala.collection.JavaConversions._$/;"	i
scala.collection.JavaConversions._	scheduler/cluster/mesos/CoarseMesosSchedulerBackend.scala	/^import scala.collection.JavaConversions._$/;"	i
scala.collection.JavaConversions._	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^import scala.collection.JavaConversions._$/;"	i
scala.collection.JavaConversions._	storage/BlockManagerMasterActor.scala	/^import scala.collection.JavaConversions._$/;"	i
scala.collection.JavaConversions._	storage/ShuffleBlockManager.scala	/^import scala.collection.JavaConversions._$/;"	i
scala.collection.JavaConversions._	ui/env/EnvironmentUI.scala	/^import scala.collection.JavaConversions._$/;"	i
scala.collection.JavaConversions._	util/Utils.scala	/^import scala.collection.JavaConversions._$/;"	i
scala.collection.JavaConversions.mapAsScalaMap	partial/GroupedCountEvaluator.scala	/^import scala.collection.JavaConversions.mapAsScalaMap$/;"	i
scala.collection.JavaConversions.mapAsScalaMap	partial/GroupedMeanEvaluator.scala	/^import scala.collection.JavaConversions.mapAsScalaMap$/;"	i
scala.collection.JavaConversions.mapAsScalaMap	partial/GroupedSumEvaluator.scala	/^import scala.collection.JavaConversions.mapAsScalaMap$/;"	i
scala.collection.JavaConversions.mapAsScalaMap	rdd/RDD.scala	/^import scala.collection.JavaConversions.mapAsScalaMap$/;"	i
scala.collection.JavaConverters._	api/java/JavaPairRDD.scala	/^    import scala.collection.JavaConverters._$/;"	i
scala.collection.JavaConverters._	api/java/JavaRDDLike.scala	/^    import scala.collection.JavaConverters._$/;"	i
scala.collection.JavaConverters._	api/python/PythonRDD.scala	/^    import scala.collection.JavaConverters._$/;"	i
scala.collection.JavaConverters._	network/netty/ShuffleCopier.scala	/^import scala.collection.JavaConverters._$/;"	i
scala.collection.JavaConverters._	util/BoundedPriorityQueue.scala	/^import scala.collection.JavaConverters._$/;"	i
scala.collection.Map	SparkContext.scala	/^import scala.collection.Map$/;"	i
scala.collection.Map	deploy/Command.scala	/^import scala.collection.Map$/;"	i
scala.collection.Map	partial/GroupedCountEvaluator.scala	/^import scala.collection.Map$/;"	i
scala.collection.Map	partial/GroupedMeanEvaluator.scala	/^import scala.collection.Map$/;"	i
scala.collection.Map	partial/GroupedSumEvaluator.scala	/^import scala.collection.Map$/;"	i
scala.collection.Map	rdd/ParallelCollectionRDD.scala	/^import scala.collection.Map$/;"	i
scala.collection.Map	rdd/PipedRDD.scala	/^import scala.collection.Map$/;"	i
scala.collection.Map	rdd/RDD.scala	/^import scala.collection.Map$/;"	i
scala.collection.Map	util/Utils.scala	/^import scala.collection.Map$/;"	i
scala.collection._	scheduler/StageInfo.scala	/^import scala.collection._$/;"	i
scala.collection.generic.Growable	Accumulators.scala	/^import scala.collection.generic.Growable$/;"	i
scala.collection.generic.Growable	SparkContext.scala	/^import scala.collection.generic.Growable$/;"	i
scala.collection.generic.Growable	util/BoundedPriorityQueue.scala	/^import scala.collection.generic.Growable$/;"	i
scala.collection.immutable	util/TimeStampedHashMap.scala	/^import scala.collection.immutable$/;"	i
scala.collection.immutable.List	deploy/DeployMessage.scala	/^import scala.collection.immutable.List$/;"	i
scala.collection.immutable.NumericRange	rdd/ParallelCollectionRDD.scala	/^import scala.collection.immutable.NumericRange$/;"	i
scala.collection.immutable.Set	scheduler/InputFormatInfo.scala	/^import scala.collection.immutable.Set$/;"	i
scala.collection.mutable	deploy/master/ApplicationInfo.scala	/^import scala.collection.mutable$/;"	i
scala.collection.mutable	deploy/master/WorkerInfo.scala	/^import scala.collection.mutable$/;"	i
scala.collection.mutable	metrics/MetricsConfig.scala	/^import scala.collection.mutable$/;"	i
scala.collection.mutable	metrics/MetricsSystem.scala	/^import scala.collection.mutable$/;"	i
scala.collection.mutable	rdd/CoalescedRDD.scala	/^import scala.collection.mutable$/;"	i
scala.collection.mutable	storage/BlockManagerMasterActor.scala	/^import scala.collection.mutable$/;"	i
scala.collection.mutable.ArrayBuffer	BlockStoreShuffleFetcher.scala	/^import scala.collection.mutable.ArrayBuffer$/;"	i
scala.collection.mutable.ArrayBuffer	SparkContext.scala	/^import scala.collection.mutable.ArrayBuffer$/;"	i
scala.collection.mutable.ArrayBuffer	TaskContext.scala	/^import scala.collection.mutable.ArrayBuffer$/;"	i
scala.collection.mutable.ArrayBuffer	deploy/LocalSparkCluster.scala	/^import scala.collection.mutable.ArrayBuffer$/;"	i
scala.collection.mutable.ArrayBuffer	network/BufferMessage.scala	/^import scala.collection.mutable.ArrayBuffer$/;"	i
scala.collection.mutable.ArrayBuffer	network/ConnectionManager.scala	/^import scala.collection.mutable.ArrayBuffer$/;"	i
scala.collection.mutable.ArrayBuffer	network/Message.scala	/^import scala.collection.mutable.ArrayBuffer$/;"	i
scala.collection.mutable.ArrayBuffer	network/MessageChunk.scala	/^import scala.collection.mutable.ArrayBuffer$/;"	i
scala.collection.mutable.ArrayBuffer	rdd/AsyncRDDActions.scala	/^import scala.collection.mutable.ArrayBuffer$/;"	i
scala.collection.mutable.ArrayBuffer	rdd/CoGroupedRDD.scala	/^import scala.collection.mutable.ArrayBuffer$/;"	i
scala.collection.mutable.ArrayBuffer	rdd/CoalescedRDD.scala	/^import scala.collection.mutable.ArrayBuffer$/;"	i
scala.collection.mutable.ArrayBuffer	rdd/PairRDDFunctions.scala	/^import scala.collection.mutable.ArrayBuffer$/;"	i
scala.collection.mutable.ArrayBuffer	rdd/ParallelCollectionRDD.scala	/^import scala.collection.mutable.ArrayBuffer$/;"	i
scala.collection.mutable.ArrayBuffer	rdd/PipedRDD.scala	/^import scala.collection.mutable.ArrayBuffer$/;"	i
scala.collection.mutable.ArrayBuffer	rdd/RDD.scala	/^import scala.collection.mutable.ArrayBuffer$/;"	i
scala.collection.mutable.ArrayBuffer	rdd/SubtractedRDD.scala	/^import scala.collection.mutable.ArrayBuffer$/;"	i
scala.collection.mutable.ArrayBuffer	rdd/UnionRDD.scala	/^import scala.collection.mutable.ArrayBuffer$/;"	i
scala.collection.mutable.ArrayBuffer	scheduler/Pool.scala	/^import scala.collection.mutable.ArrayBuffer$/;"	i
scala.collection.mutable.ArrayBuffer	scheduler/Schedulable.scala	/^import scala.collection.mutable.ArrayBuffer$/;"	i
scala.collection.mutable.ArrayBuffer	scheduler/cluster/ClusterScheduler.scala	/^import scala.collection.mutable.ArrayBuffer$/;"	i
scala.collection.mutable.ArrayBuffer	scheduler/cluster/ClusterTaskSetManager.scala	/^import scala.collection.mutable.ArrayBuffer$/;"	i
scala.collection.mutable.ArrayBuffer	scheduler/local/LocalTaskSetManager.scala	/^import scala.collection.mutable.ArrayBuffer$/;"	i
scala.collection.mutable.ArrayBuffer	storage/BlockFetcherIterator.scala	/^import scala.collection.mutable.ArrayBuffer$/;"	i
scala.collection.mutable.ArrayBuffer	storage/BlockMessage.scala	/^import scala.collection.mutable.ArrayBuffer$/;"	i
scala.collection.mutable.ArrayBuffer	storage/BlockMessageArray.scala	/^import scala.collection.mutable.ArrayBuffer$/;"	i
scala.collection.mutable.ArrayBuffer	storage/BlockStore.scala	/^import scala.collection.mutable.ArrayBuffer$/;"	i
scala.collection.mutable.ArrayBuffer	storage/DiskStore.scala	/^import scala.collection.mutable.ArrayBuffer$/;"	i
scala.collection.mutable.ArrayBuffer	util/SizeEstimator.scala	/^import scala.collection.mutable.ArrayBuffer$/;"	i
scala.collection.mutable.ArrayBuffer	util/Utils.scala	/^import scala.collection.mutable.ArrayBuffer$/;"	i
scala.collection.mutable.HashMap	BlockStoreShuffleFetcher.scala	/^import scala.collection.mutable.HashMap$/;"	i
scala.collection.mutable.HashMap	MapOutputTracker.scala	/^import scala.collection.mutable.HashMap$/;"	i
scala.collection.mutable.HashMap	SparkContext.scala	/^import scala.collection.mutable.HashMap$/;"	i
scala.collection.mutable.HashMap	deploy/worker/Worker.scala	/^import scala.collection.mutable.HashMap$/;"	i
scala.collection.mutable.HashMap	executor/Executor.scala	/^import scala.collection.mutable.HashMap$/;"	i
scala.collection.mutable.HashMap	network/ConnectionManager.scala	/^import scala.collection.mutable.HashMap$/;"	i
scala.collection.mutable.HashMap	partial/GroupedCountEvaluator.scala	/^import scala.collection.mutable.HashMap$/;"	i
scala.collection.mutable.HashMap	partial/GroupedMeanEvaluator.scala	/^import scala.collection.mutable.HashMap$/;"	i
scala.collection.mutable.HashMap	partial/GroupedSumEvaluator.scala	/^import scala.collection.mutable.HashMap$/;"	i
scala.collection.mutable.HashMap	scheduler/Pool.scala	/^import scala.collection.mutable.HashMap$/;"	i
scala.collection.mutable.HashMap	scheduler/ShuffleMapTask.scala	/^import scala.collection.mutable.HashMap$/;"	i
scala.collection.mutable.HashMap	scheduler/Task.scala	/^import scala.collection.mutable.HashMap$/;"	i
scala.collection.mutable.HashMap	scheduler/cluster/ClusterScheduler.scala	/^import scala.collection.mutable.HashMap$/;"	i
scala.collection.mutable.HashMap	scheduler/cluster/ClusterTaskSetManager.scala	/^import scala.collection.mutable.HashMap$/;"	i
scala.collection.mutable.HashMap	scheduler/cluster/SparkDeploySchedulerBackend.scala	/^import scala.collection.mutable.HashMap$/;"	i
scala.collection.mutable.HashMap	scheduler/local/LocalTaskSetManager.scala	/^import scala.collection.mutable.HashMap$/;"	i
scala.collection.mutable.HashMap	ui/jobs/PoolTable.scala	/^import scala.collection.mutable.HashMap$/;"	i
scala.collection.mutable.HashSet	MapOutputTracker.scala	/^import scala.collection.mutable.HashSet$/;"	i
scala.collection.mutable.HashSet	network/ConnectionManager.scala	/^import scala.collection.mutable.HashSet$/;"	i
scala.collection.mutable.HashSet	scheduler/cluster/ClusterScheduler.scala	/^import scala.collection.mutable.HashSet$/;"	i
scala.collection.mutable.HashSet	scheduler/cluster/ClusterTaskSetManager.scala	/^import scala.collection.mutable.HashSet$/;"	i
scala.collection.mutable.HashSet	storage/BlockFetcherIterator.scala	/^import scala.collection.mutable.HashSet$/;"	i
scala.collection.mutable.HashSet	ui/jobs/PoolPage.scala	/^import scala.collection.mutable.HashSet$/;"	i
scala.collection.mutable.HashSet	ui/jobs/PoolTable.scala	/^import scala.collection.mutable.HashSet$/;"	i
scala.collection.mutable.HashSet	ui/jobs/StageTable.scala	/^import scala.collection.mutable.HashSet$/;"	i
scala.collection.mutable.ListBuffer	deploy/FaultToleranceTest.scala	/^import scala.collection.mutable.ListBuffer$/;"	i
scala.collection.mutable.Map	Accumulators.scala	/^import scala.collection.mutable.Map$/;"	i
scala.collection.mutable.Map	broadcast/MultiTracker.scala	/^import scala.collection.mutable.Map$/;"	i
scala.collection.mutable.Map	scheduler/DAGSchedulerEvent.scala	/^import scala.collection.mutable.Map$/;"	i
scala.collection.mutable.Map	scheduler/TaskResult.scala	/^import scala.collection.mutable.Map$/;"	i
scala.collection.mutable.Map	util/ClosureCleaner.scala	/^import scala.collection.mutable.Map$/;"	i
scala.collection.mutable.Map	util/TimeStampedHashMap.scala	/^import scala.collection.mutable.Map$/;"	i
scala.collection.mutable.Queue	storage/BlockFetcherIterator.scala	/^import scala.collection.mutable.Queue$/;"	i
scala.collection.mutable.Set	util/ClosureCleaner.scala	/^import scala.collection.mutable.Set$/;"	i
scala.collection.mutable.Set	util/TimeStampedHashSet.scala	/^import scala.collection.mutable.Set$/;"	i
scala.collection.mutable.StringBuilder	storage/BlockMessage.scala	/^import scala.collection.mutable.StringBuilder$/;"	i
scala.collection.mutable.SynchronizedMap	network/ConnectionManager.scala	/^import scala.collection.mutable.SynchronizedMap$/;"	i
scala.collection.mutable.SynchronizedQueue	network/ConnectionManager.scala	/^import scala.collection.mutable.SynchronizedQueue$/;"	i
scala.collection.mutable.{ArrayBuffer, HashMap, HashSet, Map}	scheduler/DAGScheduler.scala	/^import scala.collection.mutable.{ArrayBuffer, HashMap, HashSet, Map}$/;"	i
scala.collection.mutable.{ArrayBuffer, HashMap, HashSet}	deploy/master/Master.scala	/^import scala.collection.mutable.{ArrayBuffer, HashMap, HashSet}$/;"	i
scala.collection.mutable.{ArrayBuffer, HashMap, HashSet}	scheduler/InputFormatInfo.scala	/^import scala.collection.mutable.{ArrayBuffer, HashMap, HashSet}$/;"	i
scala.collection.mutable.{ArrayBuffer, HashMap, HashSet}	scheduler/cluster/CoarseGrainedSchedulerBackend.scala	/^import scala.collection.mutable.{ArrayBuffer, HashMap, HashSet}$/;"	i
scala.collection.mutable.{ArrayBuffer, HashMap, HashSet}	scheduler/cluster/mesos/CoarseMesosSchedulerBackend.scala	/^import scala.collection.mutable.{ArrayBuffer, HashMap, HashSet}$/;"	i
scala.collection.mutable.{ArrayBuffer, HashMap, HashSet}	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^import scala.collection.mutable.{ArrayBuffer, HashMap, HashSet}$/;"	i
scala.collection.mutable.{ArrayBuffer, HashMap, HashSet}	scheduler/local/LocalScheduler.scala	/^import scala.collection.mutable.{ArrayBuffer, HashMap, HashSet}$/;"	i
scala.collection.mutable.{ArrayBuffer, HashSet}	CacheManager.scala	/^import scala.collection.mutable.{ArrayBuffer, HashSet}$/;"	i
scala.collection.mutable.{ArrayBuffer, SynchronizedBuffer}	scheduler/SparkListenerBus.scala	/^import scala.collection.mutable.{ArrayBuffer, SynchronizedBuffer}$/;"	i
scala.collection.mutable.{HashMap, ArrayBuffer}	storage/BlockManager.scala	/^import scala.collection.mutable.{HashMap, ArrayBuffer}$/;"	i
scala.collection.mutable.{HashMap, HashSet, ListBuffer}	scheduler/JobLogger.scala	/^import scala.collection.mutable.{HashMap, HashSet, ListBuffer}$/;"	i
scala.collection.mutable.{HashMap, HashSet}	ui/exec/ExecutorsUI.scala	/^import scala.collection.mutable.{HashMap, HashSet}$/;"	i
scala.collection.mutable.{HashMap, Queue, ArrayBuffer}	network/Connection.scala	/^import scala.collection.mutable.{HashMap, Queue, ArrayBuffer}$/;"	i
scala.collection.mutable.{HashSet, ListBuffer, HashMap, ArrayBuffer}	ui/jobs/JobProgressUI.scala	/^import scala.collection.mutable.{HashSet, ListBuffer, HashMap, ArrayBuffer}$/;"	i
scala.collection.mutable.{ListBuffer, HashMap, HashSet}	ui/jobs/JobProgressListener.scala	/^import scala.collection.mutable.{ListBuffer, HashMap, HashSet}$/;"	i
scala.collection.mutable.{ListBuffer, Map, Set}	broadcast/BitTorrentBroadcast.scala	/^import scala.collection.mutable.{ListBuffer, Map, Set}$/;"	i
scala.collection.mutable.{ListBuffer, Set}	broadcast/TreeBroadcast.scala	/^import scala.collection.mutable.{ListBuffer, Set}$/;"	i
scala.collection.{mutable, Map}	rdd/PairRDDFunctions.scala	/^import scala.collection.{mutable, Map}$/;"	i
scala.concurrent.ExecutionContext.Implicits.global	deploy/FaultToleranceTest.scala	/^import scala.concurrent.ExecutionContext.Implicits.global$/;"	i
scala.concurrent.ExecutionContext.Implicits.global	rdd/AsyncRDDActions.scala	/^import scala.concurrent.ExecutionContext.Implicits.global$/;"	i
scala.concurrent._	FutureAction.scala	/^import scala.concurrent._$/;"	i
scala.concurrent.duration.Duration	FutureAction.scala	/^import scala.concurrent.duration.Duration$/;"	i
scala.concurrent.duration._	deploy/FaultToleranceTest.scala	/^import scala.concurrent.duration._$/;"	i
scala.concurrent.ops._	deploy/master/SparkZooKeeperSession.scala	/^import scala.concurrent.ops._$/;"	i
scala.concurrent.{Await, future, promise}	deploy/FaultToleranceTest.scala	/^import scala.concurrent.{Await, future, promise}$/;"	i
scala.io.Source	network/ConnectionManagerTest.scala	/^import scala.io.Source$/;"	i
scala.io.Source	rdd/PipedRDD.scala	/^import scala.io.Source$/;"	i
scala.io.Source	util/Utils.scala	/^import scala.io.Source$/;"	i
scala.math	broadcast/BitTorrentBroadcast.scala	/^import scala.math$/;"	i
scala.math	broadcast/TorrentBroadcast.scala	/^import scala.math$/;"	i
scala.math.max	scheduler/cluster/ClusterTaskSetManager.scala	/^import scala.math.max$/;"	i
scala.math.min	scheduler/cluster/ClusterTaskSetManager.scala	/^import scala.math.min$/;"	i
scala.runtime.AbstractFunction1	api/java/function/WrappedFunction1.scala	/^import scala.runtime.AbstractFunction1$/;"	i
scala.runtime.AbstractFunction2	api/java/function/WrappedFunction2.scala	/^import scala.runtime.AbstractFunction2$/;"	i
scala.runtime.AbstractFunction3	api/java/function/WrappedFunction3.scala	/^import scala.runtime.AbstractFunction3$/;"	i
scala.runtime.AbstractFunction4	api/java/function/WrappedFunction4.scala	/^import scala.runtime.AbstractFunction4$/;"	i
scala.sys.process._	deploy/FaultToleranceTest.scala	/^import scala.sys.process._$/;"	i
scala.util.Properties	ui/env/EnvironmentUI.scala	/^import scala.util.Properties$/;"	i
scala.util.Random	broadcast/TorrentBroadcast.scala	/^import scala.util.Random$/;"	i
scala.util.Random	storage/BlockManager.scala	/^import scala.util.Random$/;"	i
scala.util.Random	ui/UIWorkloadGenerator.scala	/^import scala.util.Random$/;"	i
scala.util.Try	FutureAction.scala	/^import scala.util.Try$/;"	i
scala.util.matching.Regex	metrics/MetricsConfig.scala	/^import scala.util.matching.Regex$/;"	i
scala.util.{Try, Success, Failure}	ui/JettyUtils.scala	/^import scala.util.{Try, Success, Failure}$/;"	i
scala.xml.Node	deploy/master/ui/ApplicationPage.scala	/^import scala.xml.Node$/;"	i
scala.xml.Node	deploy/master/ui/IndexPage.scala	/^import scala.xml.Node$/;"	i
scala.xml.Node	deploy/worker/ui/IndexPage.scala	/^import scala.xml.Node$/;"	i
scala.xml.Node	ui/JettyUtils.scala	/^import scala.xml.Node$/;"	i
scala.xml.Node	ui/UIUtils.scala	/^import scala.xml.Node$/;"	i
scala.xml.Node	ui/env/EnvironmentUI.scala	/^import scala.xml.Node$/;"	i
scala.xml.Node	ui/exec/ExecutorsUI.scala	/^import scala.xml.Node$/;"	i
scala.xml.Node	ui/jobs/PoolTable.scala	/^import scala.xml.Node$/;"	i
scala.xml.Node	ui/jobs/StagePage.scala	/^import scala.xml.Node$/;"	i
scala.xml.Node	ui/jobs/StageTable.scala	/^import scala.xml.Node$/;"	i
scala.xml.Node	ui/storage/IndexPage.scala	/^import scala.xml.Node$/;"	i
scala.xml.Node	ui/storage/RDDPage.scala	/^import scala.xml.Node$/;"	i
scala.xml.XML	scheduler/SchedulableBuilder.scala	/^import scala.xml.XML$/;"	i
scala.xml.{NodeSeq, Node}	ui/jobs/IndexPage.scala	/^import scala.xml.{NodeSeq, Node}$/;"	i
scala.xml.{NodeSeq, Node}	ui/jobs/PoolPage.scala	/^import scala.xml.{NodeSeq, Node}$/;"	i
sched	scheduler/Pool.scala	/^      var sched = schedulable.getSchedulableByName(schedulableName)$/;"	v
schedulableBuilder	scheduler/cluster/ClusterScheduler.scala	/^  var schedulableBuilder: SchedulableBuilder = null$/;"	v
schedulableBuilder	scheduler/local/LocalScheduler.scala	/^  var schedulableBuilder: SchedulableBuilder = null$/;"	v
schedulableNameToSchedulable	scheduler/Pool.scala	/^  var schedulableNameToSchedulable = new HashMap[String, Schedulable]$/;"	v
schedulableQueue	scheduler/Pool.scala	/^  var schedulableQueue = new ArrayBuffer[Schedulable]$/;"	v
schedulableQueue	scheduler/Schedulable.scala	/^  def schedulableQueue: ArrayBuffer[Schedulable]$/;"	m
schedulableQueue	scheduler/TaskSetManager.scala	/^  def schedulableQueue = null$/;"	m
scheduler	SparkContext.scala	/^        val scheduler = new ClusterScheduler(this)$/;"	V
scheduler	SparkContext.scala	/^        val scheduler = try {$/;"	V
scheduler	scheduler/cluster/mesos/CoarseMesosSchedulerBackend.scala	/^          val scheduler = CoarseMesosSchedulerBackend.this$/;"	V
scheduler	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^          val scheduler = MesosSchedulerBackend.this$/;"	V
schedulerAllocFile	scheduler/SchedulableBuilder.scala	/^  val schedulerAllocFile = Option(System.getProperty("spark.scheduler.allocation.file"))$/;"	V
schedulingMode	scheduler/Pool.scala	/^    val schedulingMode: SchedulingMode,$/;"	V
schedulingMode	scheduler/Schedulable.scala	/^  def schedulingMode: SchedulingMode$/;"	m
schedulingMode	scheduler/SchedulableBuilder.scala	/^      var schedulingMode = DEFAULT_SCHEDULING_MODE$/;"	v
schedulingMode	scheduler/TaskScheduler.scala	/^  def schedulingMode: SchedulingMode$/;"	m
schedulingMode	scheduler/TaskSetManager.scala	/^  def schedulingMode = SchedulingMode.NONE$/;"	m
schedulingMode	scheduler/cluster/ClusterScheduler.scala	/^  val schedulingMode: SchedulingMode = SchedulingMode.withName($/;"	V
schedulingMode	scheduler/local/LocalScheduler.scala	/^  val schedulingMode: SchedulingMode = SchedulingMode.withName($/;"	V
schedulingMode	ui/UIWorkloadGenerator.scala	/^    val schedulingMode = SchedulingMode.withName(args(1))$/;"	V
second	util/Utils.scala	/^    val second = 1000$/;"	V
seconds	deploy/WebUI.scala	/^    val seconds = milliseconds.toDouble \/ 1000$/;"	V
seconds	scheduler/SparkListener.scala	/^  val seconds = 1000L$/;"	V
seen	util/ClosureCleaner.scala	/^    val seen = Set[Class[_]](obj.getClass)$/;"	V
segment	storage/DiskStore.scala	/^    val segment = diskManager.getBlockLocation(blockId)$/;"	V
segment	storage/ShuffleBlockManager.scala	/^      val segment = fileGroup.getFileSegmentFor(id.mapId, id.reduceId)$/;"	V
selectedBlocks	storage/MemoryStore.scala	/^      val selectedBlocks = new ArrayBuffer[BlockId]()$/;"	V
selectedKeys	network/ConnectionManager.scala	/^          val selectedKeys = selector.selectedKeys().iterator()$/;"	V
selectedKeysCount	network/ConnectionManager.scala	/^        val selectedKeysCount =$/;"	V
selectedMemory	storage/MemoryStore.scala	/^      var selectedMemory = 0L$/;"	v
selectedPeerToTalkTo	broadcast/BitTorrentBroadcast.scala	/^      var selectedPeerToTalkTo: SourceInfo = null$/;"	v
selectedSource	broadcast/TreeBroadcast.scala	/^        var selectedSource: SourceInfo = null$/;"	v
selectedSourceInfo	broadcast/TreeBroadcast.scala	/^      private var selectedSourceInfo: SourceInfo = null$/;"	v
selectedSources	broadcast/BitTorrentBroadcast.scala	/^        var selectedSources = ListBuffer[SourceInfo]()$/;"	v
selectedSources	broadcast/BitTorrentBroadcast.scala	/^      private var selectedSources: ListBuffer[SourceInfo] = null$/;"	v
selector	network/ConnectionManager.scala	/^  private val selector = SelectorProvider.provider.openSelector()$/;"	V
selectorThread	network/ConnectionManager.scala	/^  private val selectorThread = new Thread("connection-manager-thread") {$/;"	V
selfIndex	storage/BlockManagerMasterActor.scala	/^    val selfIndex = peers.indexOf(blockManagerId)$/;"	V
send	network/Connection.scala	/^  def send(message: Message) {$/;"	m
sendFrom	broadcast/TreeBroadcast.scala	/^      private var sendFrom = 0$/;"	v
sendHeartBeat	storage/BlockManagerMaster.scala	/^  def sendHeartBeat(blockManagerId: BlockManagerId): Boolean = {$/;"	m
sendMessageReliably	network/ConnectionManager.scala	/^  def sendMessageReliably(connectionManagerId: ConnectionManagerId, message: Message)$/;"	m
sendMessageReliablySync	network/ConnectionManager.scala	/^  def sendMessageReliablySync(connectionManagerId: ConnectionManagerId, message: Message): Option[Message] = {$/;"	m
sendUntil	broadcast/TreeBroadcast.scala	/^      private var sendUntil = totalBlocks$/;"	v
sender	network/netty/ShuffleSender.scala	/^    val sender = new ShuffleSender(port, pResovler)$/;"	V
senderAddress	network/Message.scala	/^  var senderAddress: InetSocketAddress = null$/;"	v
sendingConnection	network/ConnectionManager.scala	/^        val sendingConnection = connection.asInstanceOf[SendingConnection]$/;"	V
sendingConnection	network/ConnectionManager.scala	/^        val sendingConnection = sendingConnectionOpt.get$/;"	V
sendingConnectionManagerId	network/ConnectionManager.scala	/^        val sendingConnectionManagerId = sendingConnection.getRemoteConnectionManagerId()$/;"	V
sendingConnectionOpt	network/ConnectionManager.scala	/^        val sendingConnectionOpt = connectionsById.get(remoteConnectionManagerId)$/;"	V
sentBlocks	broadcast/BitTorrentBroadcast.scala	/^  @transient var sentBlocks = new AtomicInteger(0)$/;"	v
sentMessageStatus	network/ConnectionManager.scala	/^          val sentMessageStatus = messageStatuses.synchronized {$/;"	V
seq	rdd/SubtractedRDD.scala	/^        val seq = new ArrayBuffer[V]()$/;"	V
seq	rdd/SubtractedRDD.scala	/^      val seq = map.get(k)$/;"	V
seqSeqManifest	rdd/PairRDDFunctions.scala	/^  val seqSeqManifest = classManifest[Seq[Seq[_]]]$/;"	V
sequenceFile	SparkContext.scala	/^   def sequenceFile[K, V](path: String, minSplits: Int = defaultMinSplits)$/;"	m
sequenceFile	SparkContext.scala	/^  def sequenceFile[K, V](path: String, keyClass: Class[K], valueClass: Class[V]): RDD[(K, V)] =$/;"	m
sequenceFile	SparkContext.scala	/^  def sequenceFile[K, V](path: String,$/;"	m
sequenceFile	api/java/JavaSparkContext.scala	/^  def sequenceFile[K, V](path: String, keyClass: Class[K], valueClass: Class[V]):$/;"	m
sequenceFile	api/java/JavaSparkContext.scala	/^  def sequenceFile[K, V](path: String,$/;"	m
ser	Accumulators.scala	/^    val ser = new JavaSerializer().newInstance()$/;"	V
ser	broadcast/HttpBroadcast.scala	/^    val ser = SparkEnv.get.serializer.newInstance()$/;"	V
ser	executor/Executor.scala	/^      val ser = SparkEnv.get.closureSerializer.newInstance()$/;"	V
ser	rdd/CoGroupedRDD.scala	/^    val ser = SparkEnv.get.serializerManager.get(serializerClass)$/;"	V
ser	rdd/ParallelCollectionRDD.scala	/^        val ser = sfactory.newInstance()$/;"	V
ser	scheduler/ResultTask.scala	/^        val ser = SparkEnv.get.closureSerializer.newInstance()$/;"	V
ser	scheduler/ResultTask.scala	/^    val ser = SparkEnv.get.closureSerializer.newInstance()$/;"	V
ser	scheduler/ShuffleMapTask.scala	/^        val ser = SparkEnv.get.closureSerializer.newInstance()$/;"	V
ser	scheduler/ShuffleMapTask.scala	/^      val ser = SparkEnv.get.closureSerializer.newInstance()$/;"	V
ser	scheduler/ShuffleMapTask.scala	/^      val ser = SparkEnv.get.serializerManager.get(dep.serializerClass)$/;"	V
ser	scheduler/cluster/ClusterTaskSetManager.scala	/^  val ser = env.closureSerializer.newInstance()$/;"	V
ser	scheduler/local/LocalTaskSetManager.scala	/^  val ser = env.closureSerializer.newInstance()$/;"	V
ser	storage/BlockManager.scala	/^    val ser = serializer.newInstance()$/;"	V
serIn	broadcast/HttpBroadcast.scala	/^    val serIn = ser.deserializeStream(in)$/;"	V
serOut	broadcast/HttpBroadcast.scala	/^    val serOut = ser.serializeStream(out)$/;"	V
serializableConf	rdd/NewHadoopRDD.scala	/^  \/\/ private val serializableConf = new SerializableWritable(conf)$/;"	V
serializableHadoopSplit	rdd/NewHadoopRDD.scala	/^  val serializableHadoopSplit = new SerializableWritable(rawSplit)$/;"	V
serialization	deploy/master/FileSystemPersistenceEngine.scala	/^    val serialization: Serialization)$/;"	V
serialize	serializer/JavaSerializer.scala	/^  def serialize[T](t: T): ByteBuffer = {$/;"	m
serialize	serializer/KryoSerializer.scala	/^  def serialize[T](t: T): ByteBuffer = {$/;"	m
serialize	serializer/Serializer.scala	/^  def serialize[T](t: T): ByteBuffer$/;"	m
serialize	util/Utils.scala	/^  def serialize[T](o: T): Array[Byte] = {$/;"	m
serializeInfo	scheduler/ResultTask.scala	/^  def serializeInfo(stageId: Int, rdd: RDD[_], func: (TaskContext, Iterator[_]) => _): Array[Byte] = {$/;"	m
serializeInfo	scheduler/ShuffleMapTask.scala	/^  def serializeInfo(stageId: Int, rdd: RDD[_], dep: ShuffleDependency[_,_]): Array[Byte] = {$/;"	m
serializeMany	serializer/Serializer.scala	/^  def serializeMany[T](iterator: Iterator[T]): ByteBuffer = {$/;"	m
serializeStream	rdd/CheckpointRDD.scala	/^    val serializeStream = serializer.serializeStream(fileOutputStream)$/;"	V
serializeStream	serializer/JavaSerializer.scala	/^  def serializeStream(s: OutputStream): SerializationStream = {$/;"	m
serializeStream	serializer/KryoSerializer.scala	/^  def serializeStream(s: OutputStream): SerializationStream = {$/;"	m
serializeStream	serializer/Serializer.scala	/^  def serializeStream(s: OutputStream): SerializationStream$/;"	m
serializeViaNestedStream	util/Utils.scala	/^  def serializeViaNestedStream(os: OutputStream, ser: SerializerInstance)(f: SerializationStream => Unit) = {$/;"	m
serialized	deploy/master/FileSystemPersistenceEngine.scala	/^    val serialized = serializer.toBinary(value)$/;"	V
serialized	deploy/master/ZooKeeperPersistenceEngine.scala	/^    val serialized = serializer.toBinary(value)$/;"	V
serializedDirectResult	executor/Executor.scala	/^        val serializedDirectResult = ser.serialize(directResult)$/;"	V
serializedInfoCache	scheduler/ResultTask.scala	/^  val serializedInfoCache = new TimeStampedHashMap[Int, Array[Byte]]$/;"	V
serializedInfoCache	scheduler/ShuffleMapTask.scala	/^  val serializedInfoCache = new TimeStampedHashMap[Int, Array[Byte]]$/;"	V
serializedResult	executor/Executor.scala	/^        val serializedResult = {$/;"	V
serializedTask	scheduler/TaskDescription.scala	/^  def serializedTask: ByteBuffer = buffer.value$/;"	m
serializedTask	scheduler/cluster/ClusterTaskSetManager.scala	/^          val serializedTask = Task.serializeWithDependencies($/;"	V
serializedTaskResult	scheduler/cluster/TaskResultGetter.scala	/^              val serializedTaskResult = sparkEnv.blockManager.getRemoteBytes(blockId)$/;"	V
serializer	SparkEnv.scala	/^    val serializer = serializerManager.setDefault($/;"	V
serializer	SparkEnv.scala	/^    val serializer: Serializer,$/;"	V
serializer	deploy/master/FileSystemPersistenceEngine.scala	/^    val serializer = serialization.findSerializerFor(value)$/;"	V
serializer	deploy/master/FileSystemPersistenceEngine.scala	/^    val serializer = serialization.serializerFor(clazz)$/;"	V
serializer	deploy/master/ZooKeeperPersistenceEngine.scala	/^    val serializer = serialization.findSerializerFor(value)$/;"	V
serializer	deploy/master/ZooKeeperPersistenceEngine.scala	/^    val serializer = serialization.serializerFor(clazz)$/;"	V
serializer	rdd/CheckpointRDD.scala	/^    val serializer = env.serializer.newInstance()$/;"	V
serializer	rdd/SubtractedRDD.scala	/^    val serializer = SparkEnv.get.serializerManager.get(serializerClass)$/;"	V
serializer	scheduler/cluster/TaskResultGetter.scala	/^  protected val serializer = new ThreadLocal[SerializerInstance] {$/;"	V
serializer	serializer/SerializerManager.scala	/^      var serializer = serializers.get(clsName)$/;"	v
serializer	storage/ThreadingTest.scala	/^    val serializer = new KryoSerializer$/;"	V
serializer.Serializer	SparkEnv.scala	/^import serializer.Serializer$/;"	i
serializerClass	Dependency.scala	/^    val serializerClass: String = null)$/;"	V
serializerClass	rdd/CoGroupedRDD.scala	/^  private var serializerClass: String = null$/;"	v
serializerClass	rdd/ShuffledRDD.scala	/^  private var serializerClass: String = null$/;"	v
serializerClass	rdd/SubtractedRDD.scala	/^  private var serializerClass: String = null$/;"	v
serializerManager	SparkEnv.scala	/^    val serializerManager = new SerializerManager$/;"	V
serializerManager	SparkEnv.scala	/^    val serializerManager: SerializerManager,$/;"	V
serializers	serializer/SerializerManager.scala	/^  private val serializers = new ConcurrentHashMap[String, Serializer]$/;"	V
serveMR	broadcast/BitTorrentBroadcast.scala	/^  @transient var serveMR: ServeMultipleRequests = null$/;"	v
serveMR	broadcast/TreeBroadcast.scala	/^  @transient var serveMR: ServeMultipleRequests = null$/;"	v
server	HttpServer.scala	/^  private var server: Server = null$/;"	v
server	broadcast/HttpBroadcast.scala	/^  private var server: HttpServer = null$/;"	v
server	deploy/master/ui/MasterWebUI.scala	/^  var server: Option[Server] = None$/;"	v
server	deploy/worker/ui/WorkerWebUI.scala	/^  var server: Option[Server] = None$/;"	v
server	network/netty/ShuffleSender.scala	/^  val server = new FileServer(pResolver, portIn)$/;"	V
server	ui/JettyUtils.scala	/^      val server = new Server(currentPort)$/;"	V
server	ui/SparkUI.scala	/^  var server: Option[Server] = None$/;"	v
server	util/SizeEstimator.scala	/^      val server = ManagementFactory.getPlatformMBeanServer()$/;"	V
serverChannel	network/ConnectionManager.scala	/^    val serverChannel = key.channel.asInstanceOf[ServerSocketChannel]$/;"	V
serverChannel	network/ConnectionManager.scala	/^  private val serverChannel = ServerSocketChannel.open()$/;"	V
serverSocket	api/python/PythonWorkerFactory.scala	/^    var serverSocket: ServerSocket = null$/;"	v
serverSocket	broadcast/BitTorrentBroadcast.scala	/^      var serverSocket = new ServerSocket(0)$/;"	v
serverSocket	broadcast/BitTorrentBroadcast.scala	/^      var serverSocket: ServerSocket = null$/;"	v
serverSocket	broadcast/MultiTracker.scala	/^      var serverSocket: ServerSocket = null$/;"	v
serverSocket	broadcast/TreeBroadcast.scala	/^      var serverSocket = new ServerSocket(0)$/;"	v
serverSocket	broadcast/TreeBroadcast.scala	/^      var serverSocket: ServerSocket = null$/;"	v
serverUri	HttpFileServer.scala	/^  var serverUri : String = null$/;"	v
serverUri	broadcast/HttpBroadcast.scala	/^  private var serverUri: String = null$/;"	v
serviceQuantiles	ui/jobs/StagePage.scala	/^          val serviceQuantiles = "Duration" +: Distribution(serviceTimes).get.getQuantiles().map($/;"	V
serviceTime	executor/Executor.scala	/^          val serviceTime = (System.currentTimeMillis() - taskStart).toInt$/;"	V
serviceTime	scheduler/DAGScheduler.scala	/^      val serviceTime = stageToInfos(stage).submissionTime match {$/;"	V
serviceTimes	ui/jobs/StagePage.scala	/^          val serviceTimes = validTasks.map{case (info, metrics, exception) =>$/;"	V
servletPath	metrics/sink/MetricsServlet.scala	/^  val servletPath = property.getProperty(SERVLET_KEY_PATH)$/;"	V
servletShowSample	metrics/sink/MetricsServlet.scala	/^  val servletShowSample = Option(property.getProperty(SERVLET_KEY_SAMPLE)).map(_.toBoolean)$/;"	V
sessionMonitorThread	deploy/master/SparkZooKeeperSession.scala	/^  def sessionMonitorThread(): Unit = {$/;"	m
set	SparkEnv.scala	/^  def set(e: SparkEnv) {$/;"	m
set	scheduler/InputFormatInfo.scala	/^        val set = nodeToSplit.getOrElseUpdate(location, new HashSet[SplitInfo])$/;"	V
set	scheduler/ShuffleMapTask.scala	/^    val set = objIn.readObject().asInstanceOf[Array[(String, Long)]].toMap$/;"	V
set	storage/BlockMessage.scala	/^  def set(buffer: ByteBuffer) {$/;"	m
set	storage/BlockMessage.scala	/^  def set(bufferMsg: BufferMessage) {$/;"	m
set	storage/BlockMessage.scala	/^  def set(getBlock: GetBlock) {$/;"	m
set	storage/BlockMessage.scala	/^  def set(gotBlock: GotBlock) {$/;"	m
set	storage/BlockMessage.scala	/^  def set(putBlock: PutBlock) {$/;"	m
set	storage/BlockMessageArray.scala	/^  def set(bufferMessage: BufferMessage) {$/;"	m
set	util/ClosureCleaner.scala	/^      val set = Set[Class[_]]()$/;"	V
set	util/collection/BitSet.scala	/^  def set(index: Int) {$/;"	m
setCheckpointDir	SparkContext.scala	/^  def setCheckpointDir(dir: String, useExisting: Boolean = false) {$/;"	m
setCheckpointDir	api/java/JavaSparkContext.scala	/^  def setCheckpointDir(dir: String) {$/;"	m
setCheckpointDir	api/java/JavaSparkContext.scala	/^  def setCheckpointDir(dir: String, useExisting: Boolean) {$/;"	m
setCustomHostname	util/Utils.scala	/^  def setCustomHostname(hostname: String) {$/;"	m
setDAGScheduler	scheduler/TaskScheduler.scala	/^  def setDAGScheduler(dagScheduler: DAGScheduler): Unit$/;"	m
setDefault	serializer/SerializerManager.scala	/^  def setDefault(clsName: String): Serializer = {$/;"	m
setDelaySeconds	util/MetadataCleaner.scala	/^  def setDelaySeconds(cleanerType: MetadataCleanerType.MetadataCleanerType, delay: Int) {$/;"	m
setDelaySeconds	util/MetadataCleaner.scala	/^  def setDelaySeconds(delay: Int, resetAll: Boolean = true) {$/;"	m
setGenerator	rdd/RDD.scala	/^  def setGenerator(_generator: String) = {$/;"	m
setInputPathsFunc	SparkContext.scala	/^    val setInputPathsFunc = (jobConf: JobConf) => FileInputFormat.setInputPaths(jobConf, path)$/;"	V
setJobDescription	SparkContext.scala	/^  def setJobDescription(value: String) {$/;"	m
setJobGroup	SparkContext.scala	/^  def setJobGroup(groupId: String, description: String) {$/;"	m
setLocalProperty	SparkContext.scala	/^  def setLocalProperty(key: String, value: String) {$/;"	m
setName	rdd/RDD.scala	/^  def setName(_name: String) = {$/;"	m
setOfCompletedSources	broadcast/BitTorrentBroadcast.scala	/^    private var setOfCompletedSources = Set[SourceInfo]()$/;"	v
setOfCompletedSources	broadcast/TreeBroadcast.scala	/^    private var setOfCompletedSources = Set[SourceInfo]()$/;"	v
setProperties	ui/UIWorkloadGenerator.scala	/^    def setProperties(s: String) = {$/;"	m
setSerializer	rdd/CoGroupedRDD.scala	/^  def setSerializer(cls: String): CoGroupedRDD[K] = {$/;"	m
setSerializer	rdd/ShuffledRDD.scala	/^  def setSerializer(cls: String): ShuffledRDD[K, V, P] = {$/;"	m
setSerializer	rdd/SubtractedRDD.scala	/^  def setSerializer(cls: String): SubtractedRDD[K, V, W] = {$/;"	m
setState	deploy/master/WorkerInfo.scala	/^  def setState(state: WorkerState.Value) = {$/;"	m
setValue	Accumulators.scala	/^  def setValue(newValue: R) {$/;"	m
setup	SparkHadoopWriter.scala	/^  def setup(jobid: Int, splitid: Int, attemptid: Int) {$/;"	m
setupGroups	rdd/CoalescedRDD.scala	/^  def setupGroups(targetLen: Int) {$/;"	m
sfactory	rdd/ParallelCollectionRDD.scala	/^    val sfactory = SparkEnv.get.serializer$/;"	V
shellSize	util/SizeEstimator.scala	/^    val shellSize: Long,$/;"	V
shellSize	util/SizeEstimator.scala	/^    var shellSize = parent.shellSize$/;"	v
shouldCompress	storage/BlockManager.scala	/^  def shouldCompress(blockId: BlockId): Boolean = blockId match {$/;"	m
shouldRevive	scheduler/Pool.scala	/^    var shouldRevive = false$/;"	v
shouldRevive	scheduler/cluster/ClusterScheduler.scala	/^    var shouldRevive = false$/;"	v
showBytesDistribution	scheduler/SparkListener.scala	/^  def showBytesDistribution(heading: String, dOpt: Option[Distribution]) {$/;"	m
showBytesDistribution	scheduler/SparkListener.scala	/^  def showBytesDistribution(heading: String, dist: Distribution) {$/;"	m
showBytesDistribution	scheduler/SparkListener.scala	/^  def showBytesDistribution(heading:String, getMetric: (TaskInfo,TaskMetrics) => Option[Long])$/;"	m
showDistribution	scheduler/SparkListener.scala	/^  def showDistribution(heading: String, d: Distribution, formatNumber: Double => String) {$/;"	m
showDistribution	scheduler/SparkListener.scala	/^  def showDistribution(heading: String, dOpt: Option[Distribution], format:String) {$/;"	m
showDistribution	scheduler/SparkListener.scala	/^  def showDistribution(heading: String, dOpt: Option[Distribution], formatNumber: Double => String) {$/;"	m
showDistribution	scheduler/SparkListener.scala	/^  def showDistribution(heading:String, format: String, getMetric: (TaskInfo,TaskMetrics) => Option[Double])$/;"	m
showMillisDistribution	scheduler/SparkListener.scala	/^  def showMillisDistribution(heading: String, dOpt: Option[Distribution]) {$/;"	m
showMillisDistribution	scheduler/SparkListener.scala	/^  def showMillisDistribution(heading: String, getMetric: (TaskInfo, TaskMetrics) => Option[Long])$/;"	m
showQuantiles	util/Distribution.scala	/^  def showQuantiles(out: PrintStream = System.out) = {$/;"	m
showQuantiles	util/Distribution.scala	/^  def showQuantiles(out: PrintStream = System.out, quantiles: Traversable[Double]) {$/;"	m
shuffle	scheduler/ShuffleMapTask.scala	/^    var shuffle: ShuffleWriterGroup = null$/;"	v
shuffle	storage/StoragePerfTester.scala	/^      val shuffle = blockManager.shuffleBlockManager.forMapTask(1, mapId, numOutputSplits,$/;"	V
shuffleBlockManager	scheduler/ShuffleMapTask.scala	/^    val shuffleBlockManager = blockManager.shuffleBlockManager$/;"	V
shuffleBlockManager	storage/BlockManager.scala	/^  val shuffleBlockManager = new ShuffleBlockManager(this)$/;"	V
shuffleBytesWritten	executor/TaskMetrics.scala	/^  var shuffleBytesWritten: Long = _$/;"	v
shuffleDep	scheduler/Stage.scala	/^    val shuffleDep: Option[ShuffleDependency[_,_]],  \/\/ Output shuffle if stage is a map stage$/;"	V
shuffleFetcher	SparkEnv.scala	/^    val shuffleFetcher = instantiateClass[ShuffleFetcher]($/;"	V
shuffleFetcher	SparkEnv.scala	/^    val shuffleFetcher: ShuffleFetcher,$/;"	V
shuffleFinishTime	executor/TaskMetrics.scala	/^  var shuffleFinishTime: Long = _$/;"	v
shuffleId	Dependency.scala	/^  val shuffleId: Int = rdd.context.newShuffleId()$/;"	V
shuffleId	storage/ShuffleBlockManager.scala	/^  private class ShuffleFileGroup(val shuffleId: Int, val fileId: Int, val files: Array[File]) {$/;"	V
shuffleMetrics	BlockStoreShuffleFetcher.scala	/^      val shuffleMetrics = new ShuffleReadMetrics$/;"	V
shuffleMetrics	scheduler/ShuffleMapTask.scala	/^      val shuffleMetrics = new ShuffleWriteMetrics$/;"	V
shuffleRead	ui/jobs/JobProgressListener.scala	/^    val shuffleRead = metrics.flatMap(m => m.shuffleReadMetrics).map(s =>$/;"	V
shuffleRead	ui/jobs/StageTable.scala	/^    val shuffleRead = shuffleReadSortable match {$/;"	V
shuffleReadBytes	ui/jobs/StagePage.scala	/^      val shuffleReadBytes = listener.stageIdToShuffleRead.getOrElse(stageId, 0L)$/;"	V
shuffleReadMetrics	executor/TaskMetrics.scala	/^  var shuffleReadMetrics: Option[ShuffleReadMetrics] = None$/;"	v
shuffleReadQuantiles	ui/jobs/StagePage.scala	/^          val shuffleReadQuantiles = "Shuffle Read (Remote)" +: getQuantileCols(shuffleReadSizes)$/;"	V
shuffleReadReadable	ui/jobs/StagePage.scala	/^    val shuffleReadReadable = maybeShuffleRead.map{Utils.bytesToString(_)}.getOrElse("")$/;"	V
shuffleReadSizes	ui/jobs/StagePage.scala	/^          val shuffleReadSizes = validTasks.map {$/;"	V
shuffleReadSortable	ui/jobs/StagePage.scala	/^    val shuffleReadSortable = maybeShuffleRead.map(_.toString).getOrElse("")$/;"	V
shuffleReadSortable	ui/jobs/StageTable.scala	/^    val shuffleReadSortable = listener.stageIdToShuffleRead.getOrElse(s.stageId, 0L)$/;"	V
shuffleSender	storage/DiskBlockManager.scala	/^  private var shuffleSender : ShuffleSender = null$/;"	v
shuffleState	storage/ShuffleBlockManager.scala	/^      private val shuffleState = shuffleStates(shuffleId)$/;"	V
shuffleState	storage/ShuffleBlockManager.scala	/^    val shuffleState = shuffleStates(id.shuffleId)$/;"	V
shuffleStates	storage/ShuffleBlockManager.scala	/^  private val shuffleStates = new TimeStampedHashMap[ShuffleId, ShuffleState]$/;"	V
shuffleToMapStage	scheduler/DAGScheduler.scala	/^  private val shuffleToMapStage = new TimeStampedHashMap[Int, Stage]$/;"	V
shuffleWrite	ui/jobs/JobProgressListener.scala	/^    val shuffleWrite = metrics.flatMap(m => m.shuffleWriteMetrics).map(s =>$/;"	V
shuffleWrite	ui/jobs/StageTable.scala	/^    val shuffleWrite = shuffleWriteSortable match {$/;"	V
shuffleWriteBytes	ui/jobs/StagePage.scala	/^      val shuffleWriteBytes = listener.stageIdToShuffleWrite.getOrElse(stageId, 0L)$/;"	V
shuffleWriteMetrics	executor/TaskMetrics.scala	/^  var shuffleWriteMetrics: Option[ShuffleWriteMetrics] = None$/;"	v
shuffleWriteQuantiles	ui/jobs/StagePage.scala	/^          val shuffleWriteQuantiles = "Shuffle Write" +: getQuantileCols(shuffleWriteSizes)$/;"	V
shuffleWriteReadable	ui/jobs/StagePage.scala	/^    val shuffleWriteReadable = maybeShuffleWrite.map{Utils.bytesToString(_)}.getOrElse("")$/;"	V
shuffleWriteSizes	ui/jobs/StagePage.scala	/^          val shuffleWriteSizes = validTasks.map {$/;"	V
shuffleWriteSortable	ui/jobs/StagePage.scala	/^    val shuffleWriteSortable = maybeShuffleWrite.map(_.toString).getOrElse("")$/;"	V
shuffleWriteSortable	ui/jobs/StageTable.scala	/^    val shuffleWriteSortable = listener.stageIdToShuffleWrite.getOrElse(s.stageId, 0L)$/;"	V
shuffleWriteTime	executor/TaskMetrics.scala	/^  var shuffleWriteTime: Long = _$/;"	v
shuffled	rdd/OrderedRDDFunctions.scala	/^    val shuffled = new ShuffledRDD[K, V, P](self, part)$/;"	V
shuffledId	rdd/ShuffledRDD.scala	/^    val shuffledId = dependencies.head.asInstanceOf[ShuffleDependency[K, V]].shuffleId$/;"	V
shutdownCallback	scheduler/cluster/SparkDeploySchedulerBackend.scala	/^  var shutdownCallback : (SparkDeploySchedulerBackend) => Unit = _$/;"	v
shutdownDeletePaths	util/Utils.scala	/^  private val shutdownDeletePaths = new scala.collection.mutable.HashSet[String]()$/;"	V
shutdownHook	deploy/worker/ExecutorRunner.scala	/^  var shutdownHook: Thread = null$/;"	v
sid	ui/jobs/JobProgressListener.scala	/^    val sid = taskEnd.task.stageId$/;"	V
sid	ui/jobs/JobProgressListener.scala	/^    val sid = taskStart.task.stageId$/;"	V
sign	rdd/ParallelCollectionRDD.scala	/^        val sign = if (r.step < 0) {$/;"	V
sink	metrics/MetricsSystem.scala	/^        val sink = Class.forName(classPath)$/;"	V
sinkConfigs	metrics/MetricsSystem.scala	/^    val sinkConfigs = metricsConfig.subProperties(instConfig, MetricsSystem.SINK_REGEX)$/;"	V
sinks	metrics/MetricsSystem.scala	/^  val sinks = new mutable.ArrayBuffer[Sink]$/;"	V
size	network/BufferMessage.scala	/^  def size = initialSize$/;"	m
size	network/ConnectionManager.scala	/^    val size = 10 * 1024 * 1024$/;"	V
size	network/ConnectionManagerTest.scala	/^    val size = ( if (args.length > 3) (args(3).toInt) else 10 ) * 1024 * 1024 $/;"	V
size	network/Message.scala	/^  def size: Int$/;"	m
size	network/MessageChunk.scala	/^  val size = if (buffer == null) 0 else buffer.remaining$/;"	V
size	network/SenderTest.scala	/^    val size =  100 * 1024  * 1024 $/;"	V
size	partial/StudentTCacher.scala	/^      val size = sampleSize.toInt$/;"	V
size	rdd/CoalescedRDD.scala	/^  def size = arr.size$/;"	m
size	scheduler/ShuffleMapTask.scala	/^        val size = writer.fileSegment().length$/;"	V
size	storage/BlockFetcherIterator.scala	/^    val size = blocks.map(_._2).sum$/;"	V
size	storage/BlockInfo.scala	/^  @volatile var size: Long = BlockInfo.BLOCK_PENDING$/;"	v
size	storage/BlockManager.scala	/^    var size = 0L$/;"	v
size	storage/BlockMessageArray.scala	/^      val size = buffer.getInt()$/;"	V
size	util/SizeEstimator.scala	/^        var size = 0.0$/;"	v
size	util/SizeEstimator.scala	/^    var size = 0L$/;"	v
size	util/collection/OpenHashSet.scala	/^  def size: Int = _size$/;"	m
size	util/collection/PrimitiveVector.scala	/^  def size: Int = _numElements$/;"	m
sizeBefore	scheduler/DAGScheduler.scala	/^    var sizeBefore = stageIdToStage.size$/;"	v
sizeBuffer	storage/BlockMessageArray.scala	/^      val sizeBuffer = ByteBuffer.allocate(4).putInt(bufferMessage.size)$/;"	V
sizeEstimate	storage/MemoryStore.scala	/^      val sizeEstimate = SizeEstimator.estimate(elements.asInstanceOf[AnyRef])$/;"	V
sizeEstimate	storage/MemoryStore.scala	/^      val sizeEstimate = SizeEstimator.estimate(values.asInstanceOf[AnyRef])$/;"	V
sizeMap	storage/BlockFetcherIterator.scala	/^      val sizeMap = req.blocks.toMap  \/\/ so we can look up the size of each blockID$/;"	V
sizes	rdd/ZippedPartitionsRDD.scala	/^    val sizes = rdds.map(x => x.partitions.size)$/;"	V
slack	rdd/CoalescedRDD.scala	/^  val slack = (balanceSlack * prev.partitions.size).toInt$/;"	V
slaveActor	storage/BlockManager.scala	/^  val slaveActor = actorSystem.actorOf(Props(new BlockManagerSlaveActor(this)),$/;"	V
slaveActor	storage/BlockManagerMasterActor.scala	/^      val slaveActor: ActorRef)$/;"	V
slaveConnManagerIds	network/ConnectionManagerTest.scala	/^    val slaveConnManagerIds = sc.parallelize(0 until tasknum, tasknum).map($/;"	V
slaveId	scheduler/cluster/mesos/CoarseMesosSchedulerBackend.scala	/^        val slaveId = offer.getSlaveId.toString$/;"	V
slaveId	scheduler/cluster/mesos/CoarseMesosSchedulerBackend.scala	/^        val slaveId = taskIdToSlaveId(taskId)$/;"	V
slaveId	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^            val slaveId = offers(offerNum).getSlaveId.getValue$/;"	V
slaveId	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^          val slaveId = o.getSlaveId.getValue$/;"	V
slaveIdsWithExecutors	scheduler/cluster/mesos/CoarseMesosSchedulerBackend.scala	/^  val slaveIdsWithExecutors = new HashSet[String]$/;"	V
slaveIdsWithExecutors	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^  val slaveIdsWithExecutors = new HashSet[String]$/;"	V
slaveTimeout	storage/BlockManagerMasterActor.scala	/^  val slaveTimeout = System.getProperty("spark.storage.blockManagerSlaveTimeoutMs",$/;"	V
slaves	network/ConnectionManagerTest.scala	/^    val slaves = slavesFile.mkString.split("\\n")$/;"	V
slavesFile	network/ConnectionManagerTest.scala	/^    val slavesFile = Source.fromFile(args(1))$/;"	V
sleepTime	util/RateLimitedOutputStream.scala	/^      val sleepTime = MILLISECONDS.convert((bytesWrittenSinceSync \/ bytesPerSec - elapsedSecs), SECONDS)$/;"	V
slice	rdd/ParallelCollectionRDD.scala	/^    var slice: Int,$/;"	v
slice	rdd/ParallelCollectionRDD.scala	/^  def slice[T: ClassManifest](seq: Seq[T], numSlices: Int): Seq[Seq[T]] = {$/;"	m
sliceSize	rdd/ParallelCollectionRDD.scala	/^        val sliceSize = (nr.size + numSlices - 1) \/ numSlices \/\/ Round up to catch everything$/;"	V
slices	rdd/ParallelCollectionRDD.scala	/^        val slices = new ArrayBuffer[Seq[T]](numSlices)$/;"	V
slices	rdd/ParallelCollectionRDD.scala	/^    val slices = ParallelCollectionRDD.slice(data, numSlices).toArray$/;"	V
socket	api/python/PythonRDD.scala	/^      val socket = new Socket(serverHost, serverPort)$/;"	V
socket	broadcast/MultiTracker.scala	/^    val socket = new Socket(MultiTracker.DriverHostAddress, DriverTrackerPort)$/;"	V
socketRemoteConnectionManagerId	network/Connection.scala	/^    val socketRemoteConnectionManagerId: ConnectionManagerId)$/;"	V
sortByKey	api/java/JavaPairRDD.scala	/^  def sortByKey(): JavaPairRDD[K, V] = sortByKey(true)$/;"	m
sortByKey	api/java/JavaPairRDD.scala	/^  def sortByKey(ascending: Boolean): JavaPairRDD[K, V] = {$/;"	m
sortByKey	api/java/JavaPairRDD.scala	/^  def sortByKey(comp: Comparator[K]): JavaPairRDD[K, V] = sortByKey(comp, true)$/;"	m
sortByKey	api/java/JavaPairRDD.scala	/^  def sortByKey(comp: Comparator[K], ascending: Boolean): JavaPairRDD[K, V] = {$/;"	m
sortByKey	rdd/OrderedRDDFunctions.scala	/^  def sortByKey(ascending: Boolean = true, numPartitions: Int = self.partitions.size): RDD[P] = {$/;"	m
sortedFiles	deploy/master/FileSystemPersistenceEngine.scala	/^    val sortedFiles = new File(dir).listFiles().sortBy(_.getName)$/;"	V
sortedFiles	deploy/master/ZooKeeperPersistenceEngine.scala	/^    val sortedFiles = zk.getChildren(WORKING_DIR).toList.sorted$/;"	V
sortedSchedulableQueue	scheduler/Pool.scala	/^    val sortedSchedulableQueue = schedulableQueue.sortWith(taskSetSchedulingAlgorithm.comparator)$/;"	V
sortedTaskSetQueue	scheduler/Pool.scala	/^    var sortedTaskSetQueue = new ArrayBuffer[TaskSetManager]$/;"	v
sortedTaskSetQueue	scheduler/cluster/ClusterTaskSetManager.scala	/^    var sortedTaskSetQueue = ArrayBuffer[TaskSetManager](this)$/;"	v
sortedTaskSetQueue	scheduler/local/LocalScheduler.scala	/^      val sortedTaskSetQueue = rootPool.getSortedTaskSetQueue()$/;"	V
sortedTaskSetQueue	scheduler/local/LocalTaskSetManager.scala	/^    var sortedTaskSetQueue = new ArrayBuffer[TaskSetManager]$/;"	v
sortedTaskSets	scheduler/cluster/ClusterScheduler.scala	/^    val sortedTaskSets = rootPool.getSortedTaskSetQueue()$/;"	V
source	metrics/MetricsSystem.scala	/^        val source = Class.forName(classPath).newInstance()$/;"	V
sourceConfigs	metrics/MetricsSystem.scala	/^    val sourceConfigs = metricsConfig.subProperties(instConfig, MetricsSystem.SOURCE_REGEX)$/;"	V
sourceFile	util/Utils.scala	/^        val sourceFile = if (uri.isAbsolute) new File(uri) else new File(url)$/;"	V
sourceInfo	broadcast/BitTorrentBroadcast.scala	/^      private var sourceInfo: SourceInfo = null$/;"	v
sourceInfo	broadcast/TreeBroadcast.scala	/^          var sourceInfo = listIter.next$/;"	v
sourceInfo	broadcast/TreeBroadcast.scala	/^          var sourceInfo = ois.readObject.asInstanceOf[SourceInfo]$/;"	v
sourceInfo	broadcast/TreeBroadcast.scala	/^      var sourceInfo = oisDriver.readObject.asInstanceOf[SourceInfo]$/;"	v
sourceName	deploy/master/ApplicationSource.scala	/^  val sourceName = "%s.%s.%s".format("application", application.desc.name,$/;"	V
sourceName	deploy/master/MasterSource.scala	/^  val sourceName = "master"$/;"	V
sourceName	deploy/worker/WorkerSource.scala	/^  val sourceName = "worker"$/;"	V
sourceName	executor/ExecutorSource.scala	/^  val sourceName = "executor.%s".format(executorId)$/;"	V
sourceName	metrics/source/JvmSource.scala	/^  val sourceName = "jvm"$/;"	V
sourceName	metrics/source/Source.scala	/^  def sourceName: String$/;"	m
sourceName	scheduler/DAGSchedulerSource.scala	/^  val sourceName = "%s.DAGScheduler".format(sc.appName)$/;"	V
sourceName	storage/BlockManagerSource.scala	/^  val sourceName = "%s.BlockManager".format(sc.appName)$/;"	V
sources	metrics/MetricsSystem.scala	/^  val sources = new mutable.ArrayBuffer[Source]$/;"	V
sparkContext	rdd/RDD.scala	/^  def sparkContext: SparkContext = sc$/;"	m
sparkFilesDir	SparkEnv.scala	/^    val sparkFilesDir: String = if (isDriver) {$/;"	V
sparkFilesDir	SparkEnv.scala	/^    val sparkFilesDir: String,$/;"	V
sparkHome	SparkContext.scala	/^    val sparkHome: String = null,$/;"	V
sparkHome	api/python/PythonWorkerFactory.scala	/^        val sparkHome = new ProcessBuilder().environment().get("SPARK_HOME")$/;"	V
sparkHome	api/python/PythonWorkerFactory.scala	/^      val sparkHome = new ProcessBuilder().environment().get("SPARK_HOME")$/;"	V
sparkHome	deploy/ApplicationDescription.scala	/^    val sparkHome: String,$/;"	V
sparkHome	deploy/FaultToleranceTest.scala	/^  val sparkHome = System.getenv("SPARK_HOME")$/;"	V
sparkHome	deploy/worker/ExecutorRunner.scala	/^    val sparkHome: File,$/;"	V
sparkHome	deploy/worker/Worker.scala	/^  var sparkHome: File = null$/;"	v
sparkHome	scheduler/cluster/SparkDeploySchedulerBackend.scala	/^    val sparkHome = sc.getSparkHome().getOrElse(null)$/;"	V
sparkHome	scheduler/cluster/mesos/CoarseMesosSchedulerBackend.scala	/^  val sparkHome = sc.getSparkHome().getOrElse(throw new SparkException($/;"	V
sparkHome	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^    val sparkHome = sc.getSparkHome().getOrElse(throw new SparkException($/;"	V
sparkHostPort	executor/CoarseGrainedExecutorBackend.scala	/^    val sparkHostPort = hostname + ":" + boundPort$/;"	V
sparkListeners	scheduler/SparkListenerBus.scala	/^  private val sparkListeners = new ArrayBuffer[SparkListener]() with SynchronizedBuffer[SparkListener]$/;"	V
sparkProperties	ui/env/EnvironmentUI.scala	/^    val sparkProperties = properties.filter(_._1.startsWith("spark")).sorted$/;"	V
sparkPropertyTable	ui/env/EnvironmentUI.scala	/^    val sparkPropertyTable =$/;"	V
sparkUrlRegex	deploy/master/Master.scala	/^  private val sparkUrlRegex = "spark:\/\/([^:]+):([0-9]+)".r$/;"	V
sparkUser	SparkContext.scala	/^  val sparkUser = Option {$/;"	V
sparkUser	executor/Executor.scala	/^  val sparkUser = Option(System.getenv("SPARK_USER")).getOrElse(SparkContext.SPARK_UNKNOWN_USER)$/;"	V
speculatableTasks	scheduler/cluster/ClusterTaskSetManager.scala	/^  val speculatableTasks = new HashSet[Int]$/;"	V
split	rdd/CoGroupedRDD.scala	/^    val split = s.asInstanceOf[CoGroupPartition]$/;"	V
split	rdd/CoGroupedRDD.scala	/^    var split: Partition$/;"	v
split	rdd/HadoopRDD.scala	/^      val split = theSplit.asInstanceOf[HadoopPartition]$/;"	V
split	rdd/NewHadoopRDD.scala	/^      val split = theSplit.asInstanceOf[NewHadoopPartition]$/;"	V
split	rdd/SampledRDD.scala	/^    val split = splitIn.asInstanceOf[SampledRDDPartition]$/;"	V
split	rdd/UnionRDD.scala	/^  var split: Partition = rdd.partitions(splitIndex)$/;"	v
split	scheduler/DAGScheduler.scala	/^      val split = rdd.partitions(job.partitions(0))$/;"	V
split	scheduler/ResultTask.scala	/^  var split = if (rdd == null) null else rdd.partitions(partitionId)$/;"	v
split	scheduler/ShuffleMapTask.scala	/^  var split = if (rdd == null) null else rdd.partitions(partitionId)$/;"	v
splitCommandString	util/Utils.scala	/^  def splitCommandString(s: String): Seq[String] = {$/;"	m
splitID	SparkHadoopWriter.scala	/^  private var splitID = 0$/;"	v
splitId	TaskContext.scala	/^  def splitId = partitionId$/;"	m
splitIdToFile	rdd/CheckpointRDD.scala	/^  def splitIdToFile(splitId: Int): String = {$/;"	m
splitWords	util/Utils.scala	/^  def splitWords(s: String): Seq[String] = {$/;"	m
splits	api/java/JavaRDDLike.scala	/^  def splits: JList[Partition] = new java.util.ArrayList(rdd.partitions.toSeq)$/;"	m
splits	scheduler/InputFormatInfo.scala	/^      val splits = inputSplit.findPreferredLocations()$/;"	V
splitsByAddress	BlockStoreShuffleFetcher.scala	/^    val splitsByAddress = new HashMap[BlockManagerId, ArrayBuffer[(Int, Long)]]$/;"	V
spreadOutApps	deploy/master/Master.scala	/^  val spreadOutApps = System.getProperty("spark.deploy.spreadOut", "true").toBoolean$/;"	V
squaredDist	util/Vector.scala	/^  def squaredDist(other: Vector): Double = {$/;"	m
srdd	api/java/JavaDoubleRDD.scala	/^class JavaDoubleRDD(val srdd: RDD[scala.Double]) extends JavaRDDLike[Double, JavaDoubleRDD] {$/;"	V
stack	util/ClosureCleaner.scala	/^    var stack = List[Class[_]](obj.getClass)$/;"	v
stack	util/SizeEstimator.scala	/^    val stack = new ArrayBuffer[AnyRef]$/;"	V
stage	scheduler/DAGScheduler.scala	/^        val stage = newStage(shuffleDep.rdd, shuffleDep.rdd.partitions.size, Some(shuffleDep), jobId)$/;"	V
stage	scheduler/DAGScheduler.scala	/^    val stage = stageIdToStage(task.stageId)$/;"	V
stage	scheduler/DAGScheduler.scala	/^    val stage =$/;"	V
stage	scheduler/SparkListener.scala	/^case class StageCompleted(val stage: StageInfo) extends SparkListenerEvents$/;"	V
stage	ui/jobs/JobProgressListener.scala	/^    val stage = stageCompleted.stage$/;"	V
stage	ui/jobs/JobProgressListener.scala	/^    val stage = stageSubmitted.stage$/;"	V
stageIDToJobID	scheduler/JobLogger.scala	/^  private val stageIDToJobID = new HashMap[Int, Int]$/;"	V
stageId	TaskContext.scala	/^  val stageId: Int,$/;"	V
stageId	rdd/PairRDDFunctions.scala	/^    val stageId = self.id$/;"	V
stageId	scheduler/Pool.scala	/^  var stageId = -1$/;"	v
stageId	scheduler/ResultTask.scala	/^    val stageId = in.readInt()$/;"	V
stageId	scheduler/Schedulable.scala	/^  def stageId: Int$/;"	m
stageId	scheduler/ShuffleMapTask.scala	/^    val stageId = in.readInt()$/;"	V
stageId	scheduler/StageInfo.scala	/^  val stageId = stage.id$/;"	V
stageId	scheduler/Task.scala	/^private[spark] abstract class Task[T](val stageId: Int, var partitionId: Int) extends Serializable {$/;"	V
stageId	scheduler/TaskSet.scala	/^    val stageId: Int,$/;"	V
stageId	scheduler/cluster/ClusterTaskSetManager.scala	/^  var stageId = taskSet.stageId$/;"	v
stageId	scheduler/local/LocalTaskSetManager.scala	/^  var stageId: Int = taskSet.stageId$/;"	v
stageId	ui/jobs/StagePage.scala	/^      val stageId = request.getParameter("id").toInt$/;"	V
stageId1	scheduler/SchedulingAlgorithm.scala	/^      val stageId1 = s1.stageId$/;"	V
stageId2	scheduler/SchedulingAlgorithm.scala	/^      val stageId2 = s2.stageId$/;"	V
stageIdToDescription	ui/jobs/JobProgressListener.scala	/^  val stageIdToDescription = new HashMap[Int, String]()$/;"	V
stageIdToPool	ui/jobs/JobProgressListener.scala	/^  val stageIdToPool = new HashMap[Int, String]()$/;"	V
stageIdToShuffleRead	ui/jobs/JobProgressListener.scala	/^  val stageIdToShuffleRead = HashMap[Int, Long]()$/;"	V
stageIdToShuffleWrite	ui/jobs/JobProgressListener.scala	/^  val stageIdToShuffleWrite = HashMap[Int, Long]()$/;"	V
stageIdToStage	scheduler/DAGScheduler.scala	/^  private val stageIdToStage = new TimeStampedHashMap[Int, Stage]$/;"	V
stageIdToTaskInfos	ui/jobs/JobProgressListener.scala	/^  val stageIdToTaskInfos =$/;"	V
stageIdToTasksActive	ui/jobs/JobProgressListener.scala	/^  val stageIdToTasksActive = HashMap[Int, HashSet[TaskInfo]]()$/;"	V
stageIdToTasksComplete	ui/jobs/JobProgressListener.scala	/^  val stageIdToTasksComplete = HashMap[Int, Int]()$/;"	V
stageIdToTasksFailed	ui/jobs/JobProgressListener.scala	/^  val stageIdToTasksFailed = HashMap[Int, Int]()$/;"	V
stageIdToTime	ui/jobs/JobProgressListener.scala	/^  val stageIdToTime = HashMap[Int, Long]()$/;"	V
stageInfo	scheduler/JobLogger.scala	/^    val stageInfo = if (stage.isShuffleMap) {$/;"	V
stageInfo	ui/jobs/JobProgressListener.scala	/^            val stageInfo = activeStages.filter(s => s.stageId == stage.id).headOption$/;"	V
stageList	scheduler/JobLogger.scala	/^        case None => val stageList = new  ListBuffer[Stage]$/;"	V
stagePage	ui/jobs/JobProgressUI.scala	/^  private val stagePage = new StagePage(this)$/;"	V
stageToInfos	scheduler/DAGScheduler.scala	/^  private[spark] val stageToInfos = new TimeStampedHashMap[Stage, StageInfo]$/;"	V
stages	ui/jobs/JobProgressListener.scala	/^    val stages = poolToActiveStages.getOrElseUpdate(poolName, new HashSet[StageInfo]())$/;"	V
stages	ui/jobs/StageTable.scala	/^private[spark] class StageTable(val stages: Seq[StageInfo], val parent: JobProgressUI) {$/;"	V
start	SparkContext.scala	/^    val start = System.nanoTime$/;"	V
start	broadcast/BitTorrentBroadcast.scala	/^          val start = System.nanoTime$/;"	V
start	broadcast/HttpBroadcast.scala	/^          val start = System.nanoTime$/;"	V
start	broadcast/TorrentBroadcast.scala	/^          val start = System.nanoTime$/;"	V
start	broadcast/TreeBroadcast.scala	/^          val start = System.nanoTime$/;"	V
start	broadcast/TreeBroadcast.scala	/^      val start = System.nanoTime$/;"	V
start	deploy/LocalSparkCluster.scala	/^  def start(): Array[String] = {$/;"	m
start	metrics/sink/Sink.scala	/^  def start: Unit$/;"	m
start	rdd/JdbcRDD.scala	/^      val start = lowerBound + ((i * length) \/ numPartitions).toLong$/;"	V
start	rdd/ParallelCollectionRDD.scala	/^          val start = ((i * array.length.toLong) \/ numSlices).toInt$/;"	V
start	rdd/ParallelCollectionRDD.scala	/^          val start = ((i * r.length.toLong) \/ numSlices).toInt$/;"	V
start	scheduler/TaskScheduler.scala	/^  def start(): Unit$/;"	m
start	scheduler/cluster/SchedulerBackend.scala	/^  def start(): Unit$/;"	m
start	storage/BlockManager.scala	/^      val start = System.nanoTime$/;"	V
start	storage/BlockObjectWriter.scala	/^        val start = System.nanoTime()$/;"	V
start	storage/BlockObjectWriter.scala	/^      val start = System.nanoTime()$/;"	V
start	storage/StoragePerfTester.scala	/^    val start = System.currentTimeMillis()$/;"	V
startBlockManagerWorker	storage/BlockManagerWorker.scala	/^  def startBlockManagerWorker(manager: BlockManager) {$/;"	m
startByte	deploy/worker/ui/WorkerWebUI.scala	/^    val startByte =$/;"	V
startFetchWait	storage/BlockFetcherIterator.scala	/^      val startFetchWait = System.currentTimeMillis()$/;"	V
startGCTime	executor/Executor.scala	/^      val startGCTime = gcTime$/;"	V
startJettyServer	ui/JettyUtils.scala	/^  def startJettyServer(ip: String, port: Int, handlers: Seq[(String, Handler)]): (Server, Int) = {$/;"	m
startMaster	deploy/FaultToleranceTest.scala	/^  def startMaster(mountDir: String): TestMasterInfo = {$/;"	m
startNewConnection	network/ConnectionManager.scala	/^    def startNewConnection(): SendingConnection = {$/;"	m
startSystemAndActor	deploy/master/Master.scala	/^  def startSystemAndActor(host: String, port: Int, webUiPort: Int): (ActorSystem, Int, Int) = {$/;"	m
startSystemAndActor	deploy/worker/Worker.scala	/^  def startSystemAndActor(host: String, port: Int, webUiPort: Int, cores: Int, memory: Int,$/;"	m
startTime	BlockStoreShuffleFetcher.scala	/^    val startTime = System.currentTimeMillis$/;"	V
startTime	SparkContext.scala	/^  val startTime = System.currentTimeMillis()$/;"	V
startTime	api/python/PythonRDD.scala	/^    val startTime = System.currentTimeMillis$/;"	V
startTime	broadcast/BitTorrentBroadcast.scala	/^          val startTime = System.currentTimeMillis$/;"	V
startTime	deploy/master/ApplicationInfo.scala	/^    val startTime: Long,$/;"	V
startTime	executor/Executor.scala	/^      val startTime = System.currentTimeMillis()$/;"	V
startTime	network/ConnectionManager.scala	/^    val startTime = System.currentTimeMillis$/;"	V
startTime	network/ConnectionManagerTest.scala	/^        val startTime = System.currentTimeMillis  $/;"	V
startTime	network/Message.scala	/^  var startTime = -1L$/;"	v
startTime	network/SenderTest.scala	/^      val startTime = System.currentTimeMillis  $/;"	V
startTime	partial/ApproximateActionListener.scala	/^  val startTime = System.currentTimeMillis()$/;"	V
startTime	scheduler/cluster/ClusterTaskSetManager.scala	/^          val startTime = clock.getTime()$/;"	V
startTime	storage/BlockFetcherIterator.scala	/^    protected var startTime = System.currentTimeMillis$/;"	v
startTime	storage/BlockMessage.scala	/^    val startTime = System.currentTimeMillis$/;"	V
startTime	storage/BlockMessageArray.scala	/^    val startTime = System.currentTimeMillis$/;"	V
startTime	storage/DiskStore.scala	/^    val startTime = System.currentTimeMillis$/;"	V
startTime	storage/ThreadingTest.scala	/^        val startTime = System.currentTimeMillis()$/;"	V
startTimeMs	storage/BlockManager.scala	/^    val startTimeMs = System.currentTimeMillis$/;"	V
startTimeMs	storage/BlockManagerWorker.scala	/^    val startTimeMs = System.currentTimeMillis()$/;"	V
startWidth	ui/jobs/StageTable.scala	/^    val startWidth = "width: %s%%".format((started.toDouble\/total)*100)$/;"	V
startWorker	deploy/FaultToleranceTest.scala	/^  def startWorker(mountDir: String, masters: String): TestWorkerInfo = {$/;"	m
started	network/Message.scala	/^  var started = false$/;"	v
startedTasks	ui/jobs/StageTable.scala	/^    val startedTasks = listener.stageIdToTasksActive.getOrElse(s.stageId, HashSet[TaskInfo]()).size$/;"	V
starvationTimer	scheduler/cluster/ClusterScheduler.scala	/^  private val starvationTimer = new Timer(true)$/;"	V
statCounter	util/Distribution.scala	/^  def statCounter = StatCounter(data.slice(startIdx, endIdx))$/;"	m
state	deploy/ExecutorDescription.scala	/^    val state: ExecutorState.Value)$/;"	V
state	deploy/FaultToleranceTest.scala	/^  var state: RecoveryState.Value = _$/;"	v
state	deploy/master/ApplicationInfo.scala	/^  @transient var state: ApplicationState.Value = _$/;"	v
state	deploy/master/ExecutorInfo.scala	/^  var state = ExecutorState.LAUNCHING$/;"	v
state	deploy/master/Master.scala	/^  var state = RecoveryState.STANDBY$/;"	v
state	deploy/master/WorkerInfo.scala	/^  @transient var state: WorkerState.Value = _$/;"	v
state	deploy/master/ui/ApplicationPage.scala	/^    val state = Await.result(stateFuture, 30 seconds)$/;"	V
state	deploy/master/ui/IndexPage.scala	/^    val state = Await.result(stateFuture, 30 seconds)$/;"	V
state	deploy/worker/ExecutorRunner.scala	/^    var state: ExecutorState.Value)$/;"	v
state	scheduler/cluster/mesos/CoarseMesosSchedulerBackend.scala	/^    val state = status.getState$/;"	V
state	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^      val state = TaskState.fromMesos(status.getState)$/;"	V
state	util/SizeEstimator.scala	/^    val state = new SearchState(visited)$/;"	V
stateFuture	deploy/master/ui/ApplicationPage.scala	/^    val stateFuture = (master ? RequestMasterState)(timeout).mapTo[MasterStateResponse]$/;"	V
stateFuture	deploy/master/ui/IndexPage.scala	/^    val stateFuture = (master ? RequestMasterState)(timeout).mapTo[MasterStateResponse]$/;"	V
stateFuture	deploy/worker/ui/IndexPage.scala	/^    val stateFuture = (workerActor ? RequestWorkerState)(timeout).mapTo[WorkerStateResponse]$/;"	V
stateString	deploy/FaultToleranceTest.scala	/^      val stateString = status.extract[String]$/;"	V
stateValid	deploy/FaultToleranceTest.scala	/^    def stateValid(): Boolean = {$/;"	m
staticHandler	ui/JettyUtils.scala	/^    val staticHandler = new ResourceHandler$/;"	V
stats	api/java/JavaDoubleRDD.scala	/^  def stats(): StatCounter = srdd.stats()$/;"	m
stats	rdd/DoubleRDDFunctions.scala	/^  def stats(): StatCounter = {$/;"	m
stats	scheduler/SparkListener.scala	/^    val stats = d.statCounter$/;"	V
status	deploy/FaultToleranceTest.scala	/^      val status = json \\\\ "status"$/;"	V
status	deploy/master/ZooKeeperLeaderElectionAgent.scala	/^  private var status = LeadershipStatus.NOT_LEADER$/;"	v
status	network/ConnectionManager.scala	/^    val status = new MessageStatus(message, connectionManagerId, s => promise.success(s.ackMessage))$/;"	V
status	rdd/CheckpointRDD.scala	/^    val status = fs.getFileStatus(new Path(checkpointPath, CheckpointRDD.splitIdToFile(split.index)))$/;"	V
status	scheduler/DAGScheduler.scala	/^            val status = event.result.asInstanceOf[MapStatus]$/;"	V
status	scheduler/TaskInfo.scala	/^  def status: String = {$/;"	m
status	ui/exec/ExecutorsUI.scala	/^    val status = sc.getExecutorStorageStatus(statusId)$/;"	V
statusUpdate	executor/ExecutorBackend.scala	/^  def statusUpdate(taskId: Long, state: TaskState, data: ByteBuffer)$/;"	m
statusUpdate	scheduler/cluster/ClusterScheduler.scala	/^  def statusUpdate(tid: Long, state: TaskState, serializedData: ByteBuffer) {$/;"	m
statuses	BlockStoreShuffleFetcher.scala	/^    val statuses = SparkEnv.get.mapOutputTracker.getServerStatuses(shuffleId, reduceId)$/;"	V
statuses	MapOutputTracker.scala	/^    val statuses = mapStatuses.get(shuffleId).orNull$/;"	V
statuses	MapOutputTracker.scala	/^    var statuses: Array[MapStatus] = null$/;"	v
stderr	deploy/worker/ExecutorRunner.scala	/^      val stderr = new File(executorDir, "stderr")$/;"	V
stdev	api/java/JavaDoubleRDD.scala	/^  def stdev(): Double = srdd.stdev()$/;"	m
stdev	partial/CountEvaluator.scala	/^      val stdev = math.sqrt(variance)$/;"	V
stdev	partial/GroupedCountEvaluator.scala	/^        val stdev = math.sqrt(variance)$/;"	V
stdev	partial/GroupedMeanEvaluator.scala	/^        val stdev = math.sqrt(counter.sampleVariance \/ counter.count)$/;"	V
stdev	partial/MeanEvaluator.scala	/^      val stdev = math.sqrt(counter.sampleVariance \/ counter.count)$/;"	V
stdev	rdd/DoubleRDDFunctions.scala	/^  def stdev(): Double = stats().stdev$/;"	m
stdev	util/StatCounter.scala	/^  def stdev: Double = math.sqrt(variance)$/;"	m
stdout	deploy/worker/ExecutorRunner.scala	/^      val stdout = new File(executorDir, "stdout")$/;"	V
stdoutThread	util/Utils.scala	/^    val stdoutThread = new Thread("read stdout for " + command(0)) {$/;"	V
stmt	rdd/JdbcRDD.scala	/^    val stmt = conn.prepareStatement(sql, ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY)$/;"	V
stop	broadcast/BroadcastFactory.scala	/^  def stop(): Unit$/;"	m
stop	metrics/sink/Sink.scala	/^  def stop: Unit$/;"	m
stop	scheduler/TaskScheduler.scala	/^  def stop(): Unit$/;"	m
stop	scheduler/cluster/SchedulerBackend.scala	/^  def stop(): Unit$/;"	m
stopBroadcast	broadcast/BitTorrentBroadcast.scala	/^  @transient var stopBroadcast = false$/;"	v
stopBroadcast	broadcast/MultiTracker.scala	/^  private var stopBroadcast = false$/;"	v
stopBroadcast	broadcast/TreeBroadcast.scala	/^  @transient var stopBroadcast = false$/;"	v
stopFetchWait	storage/BlockFetcherIterator.scala	/^      val stopFetchWait = System.currentTimeMillis()$/;"	V
stopping	scheduler/cluster/SparkDeploySchedulerBackend.scala	/^  var stopping = false$/;"	v
storage	ui/SparkUI.scala	/^  val storage = new BlockManagerUI(sc)$/;"	V
storage	ui/UIUtils.scala	/^    val storage = page match {$/;"	V
storageLevel	rdd/RDD.scala	/^  private var storageLevel: StorageLevel = StorageLevel.NONE$/;"	v
storageLevel	storage/BlockManager.scala	/^          val storageLevel = StorageLevel(onDisk, inMem, level.deserialized, level.replication)$/;"	V
storageLevel	storage/BlockManagerMessages.scala	/^      var storageLevel: StorageLevel,$/;"	v
storageLevelCache	storage/StorageLevel.scala	/^  val storageLevelCache = new java.util.concurrent.ConcurrentHashMap[StorageLevel, StorageLevel]()$/;"	V
storageStatusList	storage/BlockManagerSource.scala	/^      val storageStatusList = blockManager.master.getStorageStatus$/;"	V
storageStatusList	ui/exec/ExecutorsUI.scala	/^    val storageStatusList = sc.getExecutorStorageStatus$/;"	V
storageStatusList	ui/storage/IndexPage.scala	/^    val storageStatusList = sc.getExecutorStorageStatus$/;"	V
storageStatusList	ui/storage/RDDPage.scala	/^    val storageStatusList = sc.getExecutorStorageStatus$/;"	V
stream	api/python/PythonRDD.scala	/^          val stream = new BufferedOutputStream(worker.getOutputStream, bufferSize)$/;"	V
stream	api/python/PythonRDD.scala	/^    val stream = new DataInputStream(new BufferedInputStream(worker.getInputStream, bufferSize))$/;"	V
stream	serializer/Serializer.scala	/^    val stream = new FastByteArrayOutputStream()$/;"	V
stream	storage/BlockManager.scala	/^    val stream = wrapForCompression(blockId, new ByteBufferInputStream(bytes, true))$/;"	V
stream	util/Utils.scala	/^    val stream = new FileInputStream(file)$/;"	V
stripPickle	api/python/PythonRDD.scala	/^  def stripPickle(arr: Array[Byte]) : Array[Byte] = {$/;"	m
studentTCacher	partial/GroupedMeanEvaluator.scala	/^      val studentTCacher = new StudentTCacher(confidence)$/;"	V
studentTCacher	partial/GroupedSumEvaluator.scala	/^      val studentTCacher = new StudentTCacher(confidence)$/;"	V
subBuffer	scheduler/Task.scala	/^    val subBuffer = serializedTask.slice()  \/\/ ByteBufferInputStream will have read just up to task$/;"	V
subDir	network/netty/ShuffleSender.scala	/^        val subDir = new File(localDirs(dirId), "%02x".format(subDirId))$/;"	V
subDir	storage/DiskBlockManager.scala	/^    var subDir = subDirs(dirId)(subDirId)$/;"	v
subDirId	network/netty/ShuffleSender.scala	/^        val subDirId = (hash \/ localDirs.length) % subDirsPerLocalDir$/;"	V
subDirId	storage/DiskBlockManager.scala	/^    val subDirId = (hash \/ localDirs.length) % subDirsPerLocalDir$/;"	V
subDirs	storage/DiskBlockManager.scala	/^  private val subDirs = Array.fill(localDirs.length)(new Array[File](subDirsPerLocalDir))$/;"	V
subDirsPerLocalDir	network/netty/ShuffleSender.scala	/^    val subDirsPerLocalDir = args(1).toInt$/;"	V
subDirsPerLocalDir	storage/DiskBlockManager.scala	/^  private val subDirsPerLocalDir = System.getProperty("spark.diskStore.subDirectories", "64").toInt$/;"	V
subIndex	util/collection/BitSet.scala	/^    val subIndex = fromIndex & 0x3f$/;"	V
subProperties	metrics/MetricsConfig.scala	/^    val subProperties = new mutable.HashMap[String, Properties]$/;"	V
subProperties	metrics/MetricsConfig.scala	/^  def subProperties(prop: Properties, regex: Regex): mutable.HashMap[String, Properties] = {$/;"	m
submissionTime	scheduler/StageInfo.scala	/^  var submissionTime: Option[Long] = None$/;"	v
submissionTime	ui/jobs/StageTable.scala	/^    val submissionTime = s.submissionTime match {$/;"	V
submitDate	deploy/master/ApplicationInfo.scala	/^    val submitDate: Date,$/;"	V
submitTasks	scheduler/TaskScheduler.scala	/^  def submitTasks(taskSet: TaskSet): Unit$/;"	m
substituteVariables	deploy/worker/ExecutorRunner.scala	/^  def substituteVariables(argument: String): String = argument match {$/;"	m
subtract	api/java/JavaDoubleRDD.scala	/^  def subtract(other: JavaDoubleRDD): JavaDoubleRDD =$/;"	m
subtract	api/java/JavaDoubleRDD.scala	/^  def subtract(other: JavaDoubleRDD, numPartitions: Int): JavaDoubleRDD =$/;"	m
subtract	api/java/JavaDoubleRDD.scala	/^  def subtract(other: JavaDoubleRDD, p: Partitioner): JavaDoubleRDD =$/;"	m
subtract	api/java/JavaPairRDD.scala	/^  def subtract(other: JavaPairRDD[K, V]): JavaPairRDD[K, V] =$/;"	m
subtract	api/java/JavaPairRDD.scala	/^  def subtract(other: JavaPairRDD[K, V], numPartitions: Int): JavaPairRDD[K, V] =$/;"	m
subtract	api/java/JavaPairRDD.scala	/^  def subtract(other: JavaPairRDD[K, V], p: Partitioner): JavaPairRDD[K, V] =$/;"	m
subtract	api/java/JavaRDD.scala	/^  def subtract(other: JavaRDD[T]): JavaRDD[T] = wrapRDD(rdd.subtract(other))$/;"	m
subtract	api/java/JavaRDD.scala	/^  def subtract(other: JavaRDD[T], numPartitions: Int): JavaRDD[T] =$/;"	m
subtract	api/java/JavaRDD.scala	/^  def subtract(other: JavaRDD[T], p: Partitioner): JavaRDD[T] =$/;"	m
subtract	rdd/RDD.scala	/^  def subtract(other: RDD[T]): RDD[T] =$/;"	m
subtract	rdd/RDD.scala	/^  def subtract(other: RDD[T], numPartitions: Int): RDD[T] =$/;"	m
subtract	rdd/RDD.scala	/^  def subtract(other: RDD[T], p: Partitioner): RDD[T] = {$/;"	m
subtract	util/Vector.scala	/^  def subtract(other: Vector) = this - other$/;"	m
subtractByKey	rdd/PairRDDFunctions.scala	/^  def subtractByKey[W: ClassManifest](other: RDD[(K, W)]): RDD[(K, V)] =$/;"	m
subtractByKey	rdd/PairRDDFunctions.scala	/^  def subtractByKey[W: ClassManifest](other: RDD[(K, W)], numPartitions: Int): RDD[(K, V)] =$/;"	m
subtractByKey	rdd/PairRDDFunctions.scala	/^  def subtractByKey[W: ClassManifest](other: RDD[(K, W)], p: Partitioner): RDD[(K, V)] =$/;"	m
success	scheduler/ShuffleMapTask.scala	/^    var success = false$/;"	v
successful	scheduler/TaskInfo.scala	/^  def successful: Boolean = finished && !failed$/;"	m
successful	scheduler/cluster/ClusterTaskSetManager.scala	/^  val successful = new Array[Boolean](numTasks)$/;"	V
suitableSources	broadcast/BitTorrentBroadcast.scala	/^      var suitableSources =$/;"	v
sum	api/java/JavaDoubleRDD.scala	/^  def sum(): Double = srdd.sum()$/;"	m
sum	partial/CountEvaluator.scala	/^  var sum: Long = 0$/;"	v
sum	partial/GroupedCountEvaluator.scala	/^        val sum = entry.getLongValue$/;"	V
sum	partial/GroupedCountEvaluator.scala	/^        val sum = entry.getLongValue()$/;"	V
sum	partial/GroupedSumEvaluator.scala	/^        val sum = entry.getValue.sum$/;"	V
sum	rdd/DoubleRDDFunctions.scala	/^  def sum(): Double = {$/;"	m
sum	util/StatCounter.scala	/^  def sum: Double = n * mu$/;"	m
sum	util/Vector.scala	/^  def sum = elements.reduceLeft(_ + _)$/;"	m
sum	util/collection/BitSet.scala	/^    var sum = 0$/;"	v
sumApprox	api/java/JavaDoubleRDD.scala	/^  def sumApprox(timeout: Long): PartialResult[BoundedDouble] = srdd.sumApprox(timeout)$/;"	m
sumApprox	api/java/JavaDoubleRDD.scala	/^  def sumApprox(timeout: Long, confidence: Double): PartialResult[BoundedDouble] =$/;"	m
sumApprox	rdd/DoubleRDDFunctions.scala	/^  def sumApprox(timeout: Long, confidence: Double = 0.95): PartialResult[BoundedDouble] = {$/;"	m
sumEstimate	partial/GroupedSumEvaluator.scala	/^        val sumEstimate = meanEstimate * countEstimate$/;"	V
sumEstimate	partial/SumEvaluator.scala	/^      val sumEstimate = meanEstimate * countEstimate$/;"	V
sumStdev	partial/GroupedSumEvaluator.scala	/^        val sumStdev = math.sqrt(sumVar)$/;"	V
sumStdev	partial/SumEvaluator.scala	/^      val sumStdev = math.sqrt(sumVar)$/;"	V
sumVar	partial/GroupedSumEvaluator.scala	/^        val sumVar = (meanEstimate * meanEstimate * countVar) +$/;"	V
sumVar	partial/SumEvaluator.scala	/^      val sumVar = (meanEstimate * meanEstimate * countVar) +$/;"	V
summary	ui/jobs/IndexPage.scala	/^      val summary: NodeSeq =$/;"	V
summary	ui/jobs/StagePage.scala	/^      val summary =$/;"	V
summary	util/Distribution.scala	/^  def summary(out: PrintStream = System.out) {$/;"	m
summaryTable	ui/jobs/StagePage.scala	/^      val summaryTable: Option[Seq[Node]] =$/;"	V
sums	partial/GroupedCountEvaluator.scala	/^  var sums = new OLMap[T]   \/\/ Sum of counts for each key$/;"	v
sums	partial/GroupedMeanEvaluator.scala	/^  var sums = new JHashMap[T, StatCounter]   \/\/ Sum of counts for each key$/;"	v
sums	partial/GroupedSumEvaluator.scala	/^  var sums = new JHashMap[T, StatCounter]   \/\/ Sum of counts for each key$/;"	v
sun.nio.ch.DirectBuffer	storage/BlockManager.scala	/^import sun.nio.ch.DirectBuffer$/;"	i
syncGetBlock	storage/BlockManagerWorker.scala	/^  def syncGetBlock(msg: GetBlock, toConnManagerId: ConnectionManagerId): ByteBuffer = {$/;"	m
syncPutBlock	storage/BlockManagerWorker.scala	/^  def syncPutBlock(msg: PutBlock, toConnManagerId: ConnectionManagerId): Boolean = {$/;"	m
syncWrites	storage/BlockObjectWriter.scala	/^  private val syncWrites = System.getProperty("spark.shuffle.sync", "false").toBoolean$/;"	V
systemName	deploy/master/Master.scala	/^  private val systemName = "sparkMaster"$/;"	V
systemName	deploy/worker/Worker.scala	/^    val systemName = "sparkWorker" + workerNumber.map(_.toString).getOrElse("")$/;"	V
systemProperty	util/MetadataCleaner.scala	/^  def systemProperty(which: MetadataCleanerType.MetadataCleanerType) = "spark.cleaner.ttl." + which.toString$/;"	m
t	SerializableWritable.scala	/^class SerializableWritable[T <: Writable](@transient var t: T) extends Serializable {$/;"	v
t	api/python/PythonRDD.scala	/^      val t = elem.asInstanceOf[scala.Tuple2[Array[Byte], Array[Byte]]]$/;"	V
tInfo	broadcast/TorrentBroadcast.scala	/^            val tInfo = x.asInstanceOf[TorrentInfo]$/;"	V
tInfo	broadcast/TorrentBroadcast.scala	/^    var tInfo = TorrentBroadcast.blockifyObject(value_)$/;"	v
tInfo	broadcast/TorrentBroadcast.scala	/^    var tInfo = TorrentInfo(retVal, blockNum, byteArray.length)$/;"	v
tLevel	storage/BlockManager.scala	/^    val tLevel = StorageLevel(level.useDisk, level.useMemory, level.deserialized, 1)$/;"	V
taCtxt	SparkHadoopWriter.scala	/^    val taCtxt = getTaskContext()$/;"	V
taID	SparkHadoopWriter.scala	/^  private var taID: SerializableWritable[TaskAttemptID] = null$/;"	v
tableClass	ui/UIUtils.scala	/^    var tableClass = "table table-bordered table-striped table-condensed sortable"$/;"	v
take	api/java/JavaRDDLike.scala	/^  def take(num: Int): JList[T] = {$/;"	m
take	rdd/RDD.scala	/^  def take(num: Int): Array[T] = {$/;"	m
takeAsync	rdd/AsyncRDDActions.scala	/^  def takeAsync(num: Int): FutureAction[Seq[T]] = {$/;"	m
takeOrdered	api/java/JavaRDDLike.scala	/^  def takeOrdered(num: Int): JList[T] = {$/;"	m
takeOrdered	api/java/JavaRDDLike.scala	/^  def takeOrdered(num: Int, comp: Comparator[T]): JList[T] = {$/;"	m
takeOrdered	rdd/RDD.scala	/^  def takeOrdered(num: Int)(implicit ord: Ordering[T]): Array[T] = top(num)(ord.reverse)$/;"	m
takePartition	api/python/PythonRDD.scala	/^  def takePartition[T](rdd: RDD[T], partition: Int): Iterator[T] = {$/;"	m
takeSample	api/java/JavaRDDLike.scala	/^  def takeSample(withReplacement: Boolean, num: Int, seed: Int): JList[T] = {$/;"	m
takeSample	rdd/RDD.scala	/^  def takeSample(withReplacement: Boolean, num: Int, seed: Int): Array[T] = {$/;"	m
targetConnectionManagerId	network/SenderTest.scala	/^    val targetConnectionManagerId = new ConnectionManagerId(targetHost, targetPort)$/;"	V
targetFile	util/Utils.scala	/^    val targetFile = new File(targetDir, filename)$/;"	V
targetHost	network/SenderTest.scala	/^    val targetHost = args(0)$/;"	V
targetPort	network/SenderTest.scala	/^    val targetPort = args(1).toInt$/;"	V
targetServer	network/SenderTest.scala	/^    val targetServer = args(0)$/;"	V
task	executor/Executor.scala	/^    @volatile private var task: Task[Any] = _$/;"	v
task	scheduler/DAGScheduler.scala	/^    val task = event.task$/;"	V
task	scheduler/JobLogger.scala	/^    val task = taskEnd.task$/;"	V
task	scheduler/cluster/ClusterTaskSetManager.scala	/^          val task = tasks(index)$/;"	V
task	scheduler/cluster/mesos/CoarseMesosSchedulerBackend.scala	/^          val task = MesosTaskInfo.newBuilder()$/;"	V
task	scheduler/local/LocalTaskSetManager.scala	/^          val task = taskSet.tasks(index)$/;"	V
task	scheduler/local/LocalTaskSetManager.scala	/^    val task = taskSet.tasks(index)$/;"	V
task	storage/BlockManager.scala	/^    val task = asyncReregisterTask$/;"	V
task	util/MetadataCleaner.scala	/^  private val task = new TimerTask {$/;"	V
taskAttempts	scheduler/cluster/ClusterTaskSetManager.scala	/^  val taskAttempts = Array.fill[List[TaskInfo]](numTasks)(Nil)$/;"	V
taskBytes	scheduler/Task.scala	/^    val taskBytes = serializer.serialize(task).array()$/;"	V
taskContext	SparkHadoopWriter.scala	/^  @transient private var taskContext: TaskAttemptContext = null$/;"	v
taskContext	scheduler/DAGScheduler.scala	/^      val taskContext =$/;"	V
taskEnded	scheduler/local/LocalTaskSetManager.scala	/^  def taskEnded(tid: Long, state: TaskState, serializedData: ByteBuffer) {$/;"	m
taskFailed	scheduler/local/LocalTaskSetManager.scala	/^  def taskFailed(tid: Long, state: TaskState, serializedData: ByteBuffer) {$/;"	m
taskFiles	scheduler/Task.scala	/^    val taskFiles = new HashMap[String, Long]()$/;"	V
taskFinish	executor/Executor.scala	/^        val taskFinish = System.currentTimeMillis()$/;"	V
taskGettingResult	scheduler/DAGScheduler.scala	/^  def taskGettingResult(task: Task[_], taskInfo: TaskInfo) {$/;"	m
taskHeaders	ui/jobs/StagePage.scala	/^      val taskHeaders: Seq[String] =$/;"	V
taskId	executor/MesosExecutorBackend.scala	/^    val taskId = taskInfo.getTaskId.getValue.toLong$/;"	V
taskId	scheduler/TaskDescription.scala	/^    val taskId: Long,$/;"	V
taskId	scheduler/TaskInfo.scala	/^    val taskId: Long,$/;"	V
taskId	scheduler/cluster/ClusterTaskSetManager.scala	/^          val taskId = sched.newTaskId()$/;"	V
taskId	scheduler/cluster/mesos/CoarseMesosSchedulerBackend.scala	/^          val taskId = newMesosTaskId()$/;"	V
taskId	scheduler/cluster/mesos/CoarseMesosSchedulerBackend.scala	/^    val taskId = status.getTaskId.getValue.toInt$/;"	V
taskId	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^    val taskId = TaskID.newBuilder().setValue(task.taskId.toString).build()$/;"	V
taskId	scheduler/local/LocalTaskSetManager.scala	/^          val taskId = sched.attemptId.getAndIncrement()$/;"	V
taskIdToExecutorId	scheduler/cluster/ClusterScheduler.scala	/^  val taskIdToExecutorId = new HashMap[Long, String]$/;"	V
taskIdToSlaveId	scheduler/cluster/mesos/CoarseMesosSchedulerBackend.scala	/^  val taskIdToSlaveId = new HashMap[Int, String]$/;"	V
taskIdToSlaveId	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^  val taskIdToSlaveId = new HashMap[Long, String]$/;"	V
taskIdToTaskSetId	scheduler/cluster/ClusterScheduler.scala	/^  val taskIdToTaskSetId = new HashMap[Long, String]$/;"	V
taskIdToTaskSetId	scheduler/local/LocalScheduler.scala	/^  val taskIdToTaskSetId = new HashMap[Long, String]$/;"	V
taskIds	scheduler/cluster/ClusterScheduler.scala	/^      val taskIds = taskSetTaskIds(tsm.taskSet.id)$/;"	V
taskIds	scheduler/local/LocalScheduler.scala	/^      val taskIds = taskSetTaskIds(tsm.taskSet.id)$/;"	V
taskIdsOnSlave	scheduler/cluster/CoarseGrainedSchedulerBackend.scala	/^  val taskIdsOnSlave = new HashMap[String, HashSet[String]]$/;"	V
taskInfo	scheduler/JobLogger.scala	/^    val taskInfo = taskEnd.taskInfo$/;"	V
taskInfos	scheduler/StageInfo.scala	/^    val taskInfos: mutable.Buffer[(TaskInfo, TaskMetrics)] = mutable.Buffer[(TaskInfo, TaskMetrics)]()$/;"	V
taskInfos	scheduler/cluster/ClusterTaskSetManager.scala	/^  val taskInfos = new HashMap[Long, TaskInfo]$/;"	V
taskInfos	scheduler/local/LocalTaskSetManager.scala	/^  val taskInfos = new HashMap[Long, TaskInfo]$/;"	V
taskJars	scheduler/Task.scala	/^    val taskJars = new HashMap[String, Long]()$/;"	V
taskList	ui/jobs/JobProgressListener.scala	/^    val taskList = stageIdToTaskInfos.getOrElse($/;"	V
taskLists	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^        val taskLists = scheduler.resourceOffers(offerableWorkers)$/;"	V
taskLocality	scheduler/TaskInfo.scala	/^    val taskLocality: TaskLocality.TaskLocality) {$/;"	V
taskMetrics	TaskContext.scala	/^  private[spark] val taskMetrics: TaskMetrics = TaskMetrics.empty()$/;"	V
taskName	scheduler/cluster/ClusterTaskSetManager.scala	/^          val taskName = "task %s:%d".format(taskSet.id, index)$/;"	V
taskName	scheduler/local/LocalTaskSetManager.scala	/^          val taskName = "task %s:%d".format(taskSet.id, index)$/;"	V
taskResultGetter	scheduler/cluster/ClusterScheduler.scala	/^  private[spark] var taskResultGetter = new TaskResultGetter(sc.env, this)$/;"	v
taskRow	ui/jobs/StagePage.scala	/^  def taskRow(shuffleRead: Boolean, shuffleWrite: Boolean)$/;"	m
taskScheduler	SparkContext.scala	/^  private[spark] var taskScheduler: TaskScheduler = {$/;"	v
taskSet	scheduler/TaskSetManager.scala	/^  def taskSet: TaskSet$/;"	m
taskSet	scheduler/cluster/ClusterTaskSetManager.scala	/^    val taskSet: TaskSet,$/;"	V
taskSet	scheduler/local/LocalTaskSetManager.scala	/^private[spark] class LocalTaskSetManager(sched: LocalScheduler, val taskSet: TaskSet)$/;"	V
taskSetFailed	scheduler/DAGScheduler.scala	/^  def taskSetFailed(taskSet: TaskSet, reason: String) {$/;"	m
taskSetFinished	scheduler/cluster/ClusterScheduler.scala	/^  def taskSetFinished(manager: TaskSetManager): Unit = synchronized {$/;"	m
taskSetFinished	scheduler/local/LocalScheduler.scala	/^  def taskSetFinished(manager: TaskSetManager) {$/;"	m
taskSetManager	scheduler/local/LocalScheduler.scala	/^            val taskSetManager = activeTaskSets(taskSetId)$/;"	V
taskSetSchedulingAlgorithm	scheduler/Pool.scala	/^  var taskSetSchedulingAlgorithm: SchedulingAlgorithm = {$/;"	v
taskSetTaskIds	scheduler/cluster/ClusterScheduler.scala	/^  val taskSetTaskIds = new HashMap[String, HashSet[Long]]$/;"	V
taskSetTaskIds	scheduler/local/LocalScheduler.scala	/^  val taskSetTaskIds = new HashMap[String, HashSet[Long]]$/;"	V
taskStart	executor/Executor.scala	/^      var taskStart: Long = 0$/;"	v
taskStarted	scheduler/DAGScheduler.scala	/^  def taskStarted(task: Task[_], taskInfo: TaskInfo) {$/;"	m
taskStarted	scheduler/local/LocalTaskSetManager.scala	/^  def taskStarted(task: Task[_], info: TaskInfo) {$/;"	m
taskStatus	scheduler/JobLogger.scala	/^    var taskStatus = ""$/;"	v
taskSucceeded	scheduler/JobListener.scala	/^  def taskSucceeded(index: Int, result: Any)$/;"	m
taskTable	ui/jobs/StagePage.scala	/^      val taskTable = listingTable(taskHeaders, taskRow(hasShuffleRead, hasShuffleWrite), tasks)$/;"	V
taskToWeightRatio1	scheduler/SchedulingAlgorithm.scala	/^    val taskToWeightRatio1 = runningTasks1.toDouble \/ s1.weight.toDouble$/;"	V
taskToWeightRatio2	scheduler/SchedulingAlgorithm.scala	/^    val taskToWeightRatio2 = runningTasks2.toDouble \/ s2.weight.toDouble$/;"	V
tasknum	network/ConnectionManagerTest.scala	/^    val tasknum = if (args.length > 2) args(2).toInt else slaves.length$/;"	V
tasks	network/netty/ShuffleCopier.scala	/^    val tasks = (for (i <- Range(0, threads)) yield { $/;"	V
tasks	scheduler/DAGScheduler.scala	/^    var tasks = ArrayBuffer[Task[_]]()$/;"	v
tasks	scheduler/TaskSet.scala	/^    val tasks: Array[Task[_]],$/;"	V
tasks	scheduler/cluster/ClusterScheduler.scala	/^    val tasks = offers.map(o => new ArrayBuffer[TaskDescription](o.cores))$/;"	V
tasks	scheduler/cluster/ClusterScheduler.scala	/^    val tasks = taskSet.tasks$/;"	V
tasks	scheduler/cluster/ClusterTaskSetManager.scala	/^  val tasks = taskSet.tasks$/;"	V
tasks	scheduler/local/LocalScheduler.scala	/^      val tasks = new ArrayBuffer[TaskDescription](freeCores)$/;"	V
tasks	ui/jobs/StagePage.scala	/^      val tasks = listener.stageIdToTaskInfos(stageId).toSeq.sortBy(_._1.launchTime)$/;"	V
tasksActive	ui/jobs/JobProgressListener.scala	/^    val tasksActive = stageIdToTasksActive.getOrElseUpdate(sid, new HashSet[TaskInfo]())$/;"	V
tasksSuccessful	scheduler/cluster/ClusterTaskSetManager.scala	/^  var tasksSuccessful = 0$/;"	v
temp	scheduler/cluster/SimrSchedulerBackend.scala	/^    val temp = fs.create(tmpPath, true)$/;"	V
tempByteArray	broadcast/MultiTracker.scala	/^      var tempByteArray = new Array[Byte](thisBlockSize)$/;"	v
tempByteArray	broadcast/TorrentBroadcast.scala	/^      var tempByteArray = new Array[Byte](thisBlockSize)$/;"	v
tempDir	util/Utils.scala	/^    val tempDir = getLocalDir$/;"	V
tempFile	util/Utils.scala	/^    val tempFile =  File.createTempFile("fetchFileTemp", null, new File(tempDir))$/;"	V
tempHasBlocksBitVector	broadcast/BitTorrentBroadcast.scala	/^        var tempHasBlocksBitVector: BitSet = null$/;"	v
tempOutputPath	rdd/CheckpointRDD.scala	/^    val tempOutputPath = new Path(outputDir, "." + finalOutputName + "-attempt-" + ctx.attemptId)$/;"	V
tempSum	broadcast/BitTorrentBroadcast.scala	/^        var tempSum = 0.0$/;"	v
test	deploy/FaultToleranceTest.scala	/^  def test(name: String)(fn: => Unit) {$/;"	m
testContinuousSending	network/ConnectionManager.scala	/^  def testContinuousSending(manager: ConnectionManager) {$/;"	m
testParallelDecreasingSending	network/ConnectionManager.scala	/^  def testParallelDecreasingSending(manager: ConnectionManager) {$/;"	m
testParallelSending	network/ConnectionManager.scala	/^  def testParallelSending(manager: ConnectionManager) {$/;"	m
testSequentialSending	network/ConnectionManager.scala	/^  def testSequentialSending(manager: ConnectionManager) {$/;"	m
textFile	SparkContext.scala	/^  def textFile(path: String, minSplits: Int = defaultMinSplits): RDD[String] = {$/;"	m
textFile	api/java/JavaSparkContext.scala	/^  def textFile(path: String): JavaRDD[String] = sc.textFile(path)$/;"	m
textFile	api/java/JavaSparkContext.scala	/^  def textFile(path: String, minSplits: Int): JavaRDD[String] = sc.textFile(path, minSplits)$/;"	m
theSplit	rdd/NewHadoopRDD.scala	/^    val theSplit = split.asInstanceOf[NewHadoopPartition]$/;"	V
this	FetchFailedException.scala	/^  def this (bmAddress: BlockManagerId, shuffleId: Int, mapId: Int, reduceId: Int, cause: Throwable) =$/;"	m
this	FetchFailedException.scala	/^  def this (shuffleId: Int, reduceId: Int, cause: Throwable) =$/;"	m
this	SparkException.scala	/^  def this(message: String) = this(message, null)  $/;"	m
this	api/java/JavaSparkContext.scala	/^  def this(master: String, appName: String) = this(new SparkContext(master, appName))$/;"	m
this	api/java/JavaSparkContext.scala	/^  def this(master: String, appName: String, sparkHome: String, jarFile: String) =$/;"	m
this	api/java/JavaSparkContext.scala	/^  def this(master: String, appName: String, sparkHome: String, jars: Array[String]) =$/;"	m
this	api/java/JavaSparkContext.scala	/^  def this(master: String, appName: String, sparkHome: String, jars: Array[String],$/;"	m
this	api/python/PythonRDD.scala	/^  def this(parent: RDD[T], command: String, envVars: JMap[String, String],$/;"	m
this	network/Connection.scala	/^  def this(channel_ : SocketChannel, selector_ : Selector) = {$/;"	m
this	rdd/RDD.scala	/^  def this(@transient oneParent: RDD[_]) =$/;"	m
this	scheduler/DAGScheduler.scala	/^  def this(taskSched: TaskScheduler) {$/;"	m
this	scheduler/JobLogger.scala	/^  def this() = this(System.getProperty("user.name", "<unknown>"),$/;"	m
this	scheduler/MapStatus.scala	/^  def this() = this(null, null)  \/\/ For deserialization only$/;"	m
this	scheduler/ResultTask.scala	/^  def this() = this(0, null, null, 0, null, 0)$/;"	m
this	scheduler/TaskResult.scala	/^  def this() = this(null.asInstanceOf[T], null, null)$/;"	m
this	storage/BlockManager.scala	/^  def this(execId: String, actorSystem: ActorSystem, master: BlockManagerMaster,$/;"	m
this	storage/BlockManagerMessages.scala	/^    def this() = this(null, null, null, 0, 0)  \/\/ For deserialization only$/;"	m
this	storage/BlockMessageArray.scala	/^  def this() = this(null.asInstanceOf[Seq[BlockMessage]])$/;"	m
this	storage/BlockMessageArray.scala	/^  def this(bm: BlockMessage) = this(Array(bm))$/;"	m
this	storage/StorageLevel.scala	/^  def this() = this(false, true, false)  \/\/ For deserialization$/;"	m
this	util/Distribution.scala	/^  def this(data: Traversable[Double]) = this(data.toArray, 0, data.size)$/;"	m
this	util/StatCounter.scala	/^  def this() = this(Nil)$/;"	m
this	util/collection/OpenHashMap.scala	/^  def this() = this(64)$/;"	m
this	util/collection/OpenHashSet.scala	/^  def this() = this(64)$/;"	m
this	util/collection/OpenHashSet.scala	/^  def this(initialCapacity: Int) = this(initialCapacity, 0.7)$/;"	m
this	util/collection/PrimitiveKeyOpenHashMap.scala	/^  def this() = this(64)$/;"	m
thisBlockSize	broadcast/MultiTracker.scala	/^      val thisBlockSize = math.min(BlockSize, byteArray.length - i)$/;"	V
thisBlockSize	broadcast/TorrentBroadcast.scala	/^      val thisBlockSize = math.min(BLOCK_SIZE, byteArray.length - i)$/;"	V
thisConnManagerId	network/ConnectionManagerTest.scala	/^        val thisConnManagerId = connManager.id $/;"	V
thisWorkerInfo	broadcast/TreeBroadcast.scala	/^      private var thisWorkerInfo:SourceInfo = null$/;"	v
thread	FutureAction.scala	/^  @volatile private var thread: Thread = _$/;"	v
threadFactory	util/Utils.scala	/^    val threadFactory = daemonThreadFactoryBuilder.setNameFormat(prefix + "-%d").build()$/;"	V
threadPool	HttpServer.scala	/^      val threadPool = new QueuedThreadPool$/;"	V
threadPool	broadcast/BitTorrentBroadcast.scala	/^      var threadPool = Utils.newDaemonCachedThreadPool("Bit torrent guide multiple requests")$/;"	v
threadPool	broadcast/BitTorrentBroadcast.scala	/^      var threadPool = Utils.newDaemonFixedThreadPool($/;"	v
threadPool	broadcast/BitTorrentBroadcast.scala	/^    var threadPool = Utils.newDaemonFixedThreadPool($/;"	v
threadPool	broadcast/MultiTracker.scala	/^      var threadPool = Utils.newDaemonCachedThreadPool("Track multiple values")$/;"	v
threadPool	broadcast/TreeBroadcast.scala	/^      var threadPool = Utils.newDaemonCachedThreadPool("Tree broadcast guide multiple requests")$/;"	v
threadPool	broadcast/TreeBroadcast.scala	/^    var threadPool = Utils.newDaemonCachedThreadPool("Tree broadcast serve multiple requests")$/;"	v
threadPool	executor/Executor.scala	/^  val threadPool = Utils.newDaemonCachedThreadPool("Executor task launch worker")$/;"	V
threads	network/netty/ShuffleCopier.scala	/^    val threads = if (args.length > 3) args(3).toInt else 10$/;"	V
threshold	scheduler/cluster/ClusterTaskSetManager.scala	/^      val threshold = max(SPECULATION_MULTIPLIER * medianDuration, 100)$/;"	V
tid	scheduler/cluster/ClusterScheduler.scala	/^            val tid = task.taskId$/;"	V
tid	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^      val tid = status.getTaskId.getValue.toLong$/;"	V
time	FutureAction.scala	/^        val time = System.currentTimeMillis()$/;"	V
time	broadcast/BitTorrentBroadcast.scala	/^          val time = (System.nanoTime - start) \/ 1e9$/;"	V
time	broadcast/HttpBroadcast.scala	/^          val time = (System.nanoTime - start) \/ 1e9$/;"	V
time	broadcast/TorrentBroadcast.scala	/^          val time = (System.nanoTime - start) \/ 1e9$/;"	V
time	broadcast/TreeBroadcast.scala	/^          val time = (System.nanoTime - start) \/ 1e9$/;"	V
time	broadcast/TreeBroadcast.scala	/^      val time = (System.nanoTime - start) \/ 1e9$/;"	V
time	partial/ApproximateActionListener.scala	/^      val time = System.currentTimeMillis()$/;"	V
time	scheduler/DAGScheduler.scala	/^        val time = System.currentTimeMillis() \/\/ TODO: use a pluggable clock for testability$/;"	V
time	scheduler/cluster/ClusterTaskSetManager.scala	/^      val time = clock.getTime()$/;"	V
time	storage/StoragePerfTester.scala	/^    val time = (end - start) \/ 1000.0$/;"	V
time	ui/jobs/JobProgressListener.scala	/^    val time = metrics.map(m => m.executorRunTime).getOrElse(0)$/;"	V
timeOutTask	broadcast/BitTorrentBroadcast.scala	/^        var timeOutTask = new TimerTask {$/;"	v
timeOutTimer	broadcast/BitTorrentBroadcast.scala	/^        var timeOutTimer = new Timer$/;"	v
timeRunning	scheduler/TaskInfo.scala	/^  def timeRunning(currentTime: Long): Long = currentTime - launchTime$/;"	m
timeTaken	network/Message.scala	/^  def timeTaken(): String = (finishTime - startTime).toString + " ms"$/;"	m
timeTaken	scheduler/cluster/ClusterTaskSetManager.scala	/^          val timeTaken = clock.getTime() - startTime$/;"	V
timeTaken	storage/DiskStore.scala	/^    val timeTaken = System.currentTimeMillis - startTime$/;"	V
timeWriting	storage/BlockObjectWriter.scala	/^    def timeWriting = _timeWriting$/;"	m
timeWriting	storage/BlockObjectWriter.scala	/^  def timeWriting(): Long$/;"	m
timeout	MapOutputTracker.scala	/^  private val timeout = Duration.create(System.getProperty("spark.akka.askTimeout", "10").toLong, "seconds")$/;"	V
timeout	deploy/client/Client.scala	/^        val timeout = Duration.create(System.getProperty("spark.akka.askTimeout", "10").toLong, "seconds")$/;"	V
timeout	deploy/master/Master.scala	/^    implicit val timeout = Timeout(timeoutDuration)$/;"	V
timeout	deploy/master/ui/ApplicationPage.scala	/^  implicit val timeout = parent.timeout$/;"	V
timeout	deploy/master/ui/IndexPage.scala	/^  implicit val timeout = parent.timeout$/;"	V
timeout	deploy/master/ui/MasterWebUI.scala	/^  implicit val timeout = Duration.create($/;"	V
timeout	deploy/worker/ui/IndexPage.scala	/^  val timeout = parent.timeout$/;"	V
timeout	deploy/worker/ui/WorkerWebUI.scala	/^  implicit val timeout = Timeout($/;"	V
timeout	scheduler/cluster/CoarseGrainedSchedulerBackend.scala	/^  private val timeout = {$/;"	V
timeout	storage/BlockManagerMaster.scala	/^  val timeout = Duration.create(System.getProperty("spark.akka.askTimeout", "10").toLong, "seconds")$/;"	V
timeout	ui/storage/BlockManagerUI.scala	/^  implicit val timeout = Duration.create($/;"	V
timeoutCheckingTask	storage/BlockManagerMasterActor.scala	/^  var timeoutCheckingTask: Cancellable = null$/;"	v
timeoutDuration	deploy/master/Master.scala	/^    val timeoutDuration = Duration.create($/;"	V
timer	util/MetadataCleaner.scala	/^  private val timer = new Timer(name + " cleanup timer", true)$/;"	V
tinfo	storage/BlockManager.scala	/^      val tinfo = new BlockInfo(level, tellMaster)$/;"	V
tmp	util/Utils.scala	/^      val tmp = arr(j)$/;"	V
tmpPath	scheduler/cluster/SimrSchedulerBackend.scala	/^  val tmpPath = new Path(driverFilePath + "_tmp")$/;"	V
toAkkaUrl	deploy/master/Master.scala	/^  def toAkkaUrl(sparkUrl: String): String = {$/;"	m
toArray	rdd/RDD.scala	/^  def toArray(): Array[T] = collect()$/;"	m
toAssign	deploy/master/Master.scala	/^        var toAssign = math.min(app.coresLeft, usableWorkers.map(_.coresFree).sum)$/;"	v
toBufferMessage	storage/BlockMessage.scala	/^  def toBufferMessage: BufferMessage = {$/;"	m
toBufferMessage	storage/BlockMessageArray.scala	/^  def toBufferMessage: BufferMessage = {$/;"	m
toDebugString	api/java/JavaRDDLike.scala	/^  def toDebugString(): String = {$/;"	m
toDebugString	rdd/RDD.scala	/^  def toDebugString: String = {$/;"	m
toInt	storage/StorageLevel.scala	/^  def toInt: Int = {$/;"	m
toJavaRDD	rdd/RDD.scala	/^  def toJavaRDD() : JavaRDD[T] = {$/;"	m
toMap	util/TimeStampedHashMap.scala	/^  def toMap: immutable.Map[A, B] = iterator.toMap$/;"	m
toMesos	TaskState.scala	/^  def toMesos(state: TaskState): MesosTaskState = state match {$/;"	m
toNodeSeq	ui/jobs/PoolTable.scala	/^  def toNodeSeq(): Seq[Node] = {$/;"	m
toNodeSeq	ui/jobs/StageTable.scala	/^  def toNodeSeq(): Seq[Node] = {$/;"	m
toRegister	serializer/KryoSerializer.scala	/^  private val toRegister: Seq[Class[_]] = Seq($/;"	V
toRemove	deploy/master/Master.scala	/^        val toRemove = math.max(RETAINED_APPLICATIONS \/ 10, 1)$/;"	V
toRemove	deploy/master/Master.scala	/^    val toRemove = workers.filter(_.lastHeartbeat < currentTime - WORKER_TIMEOUT).toArray$/;"	V
toRemove	storage/BlockManagerMasterActor.scala	/^    val toRemove = new mutable.HashSet[BlockManagerId]$/;"	V
toRemove	ui/jobs/JobProgressListener.scala	/^      val toRemove = RETAINED_STAGES \/ 10$/;"	V
toSocketAddress	network/ConnectionManagerId.scala	/^  def toSocketAddress() = new InetSocketAddress(host, port)$/;"	m
toSplitInfo	scheduler/SplitInfo.scala	/^  def toSplitInfo(inputFormatClazz: Class[_], path: String,$/;"	m
toTaskEndReason	FetchFailedException.scala	/^  def toTaskEndReason: TaskEndReason = taskEndReason$/;"	m
tok	rdd/PipedRDD.scala	/^    val tok = new StringTokenizer(command)$/;"	V
tokenize	rdd/PipedRDD.scala	/^  def tokenize(command: String): Seq[String] = {$/;"	m
top	api/java/JavaRDDLike.scala	/^  def top(num: Int): JList[T] = {$/;"	m
top	api/java/JavaRDDLike.scala	/^  def top(num: Int, comp: Comparator[T]): JList[T] = {$/;"	m
top	rdd/RDD.scala	/^  def top(num: Int)(implicit ord: Ordering[T]): Array[T] = {$/;"	m
topElems	api/java/JavaRDDLike.scala	/^    val topElems = rdd.takeOrdered(num)(Ordering.comparatorToOrdering(comp))$/;"	V
topElems	api/java/JavaRDDLike.scala	/^    val topElems = rdd.top(num)(Ordering.comparatorToOrdering(comp))$/;"	V
total	api/python/PythonRDD.scala	/^              val total = finishTime - startTime$/;"	V
total	rdd/RDD.scala	/^    var total = 0$/;"	v
totalBlocks	broadcast/BitTorrentBroadcast.scala	/^  @transient var totalBlocks = -1$/;"	v
totalBlocks	broadcast/TorrentBroadcast.scala	/^  @transient var totalBlocks = -1$/;"	v
totalBlocks	broadcast/TreeBroadcast.scala	/^  @transient var totalBlocks = -1$/;"	v
totalBlocks	storage/BlockFetchTracker.scala	/^  def totalBlocks : Int$/;"	m
totalBlocksFetched	executor/TaskMetrics.scala	/^  var totalBlocksFetched: Int = _$/;"	v
totalBlocksLock	broadcast/BitTorrentBroadcast.scala	/^  @transient var totalBlocksLock = new Object$/;"	v
totalBlocksLock	broadcast/TreeBroadcast.scala	/^  @transient var totalBlocksLock = new Object$/;"	v
totalBytes	broadcast/BitTorrentBroadcast.scala	/^  @transient var totalBytes = -1$/;"	v
totalBytes	broadcast/TorrentBroadcast.scala	/^  @transient var totalBytes = -1$/;"	v
totalBytes	broadcast/TreeBroadcast.scala	/^  @transient var totalBytes = -1$/;"	v
totalBytes	scheduler/ShuffleMapTask.scala	/^      var totalBytes = 0L$/;"	v
totalBytes	storage/StoragePerfTester.scala	/^    val totalBytes = new AtomicLong()$/;"	V
totalCoreCount	scheduler/cluster/CoarseGrainedSchedulerBackend.scala	/^  var totalCoreCount = new AtomicInteger(0)$/;"	v
totalCoresAcquired	scheduler/cluster/mesos/CoarseMesosSchedulerBackend.scala	/^  var totalCoresAcquired = 0$/;"	v
totalCount	rdd/AsyncRDDActions.scala	/^    val totalCount = new AtomicLong$/;"	V
totalMb	deploy/worker/WorkerArguments.scala	/^    var totalMb = 0$/;"	v
totalParts	rdd/AsyncRDDActions.scala	/^      val totalParts = self.partitions.length$/;"	V
totalParts	rdd/RDD.scala	/^    val totalParts = this.partitions.length$/;"	V
totalRareBlocks	broadcast/BitTorrentBroadcast.scala	/^      var totalRareBlocks = 0$/;"	v
totalRecords	storage/StoragePerfTester.scala	/^    val totalRecords = dataSizeMb * 1000$/;"	V
totalShuffleRead	ui/jobs/JobProgressListener.scala	/^  var totalShuffleRead = 0L$/;"	v
totalShuffleWrite	ui/jobs/JobProgressListener.scala	/^  var totalShuffleWrite = 0L$/;"	v
totalSize	network/MessageChunkHeader.scala	/^    val totalSize = buffer.getInt()$/;"	V
totalSize	network/MessageChunkHeader.scala	/^    val totalSize: Int,$/;"	V
totalSize	storage/BlockMessageArray.scala	/^    val totalSize = bufferMessage.size$/;"	V
totalTasks	partial/ApproximateActionListener.scala	/^  val totalTasks = rdd.partitions.size$/;"	V
totalTasks	ui/exec/ExecutorsUI.scala	/^    val totalTasks = activeTasks + failedTasks + completedTasks$/;"	V
totalTasks	ui/jobs/StageTable.scala	/^    val totalTasks = s.numTasks$/;"	V
totalTime	scheduler/ShuffleMapTask.scala	/^      var totalTime = 0L$/;"	v
totalTime	ui/jobs/JobProgressListener.scala	/^  var totalTime = 0L$/;"	v
tput	network/ConnectionManager.scala	/^      val tput = mb * 1000.0 \/ ms$/;"	V
tput	network/ConnectionManager.scala	/^    val tput = mb * 1000.0 \/ ms$/;"	V
tr	executor/Executor.scala	/^    val tr = new TaskRunner(context, taskId, serializedTask)$/;"	V
tr	executor/Executor.scala	/^    val tr = runningTasks.get(taskId)$/;"	V
trace	util/Utils.scala	/^    val trace = Thread.currentThread.getStackTrace().filter( el =>$/;"	V
trackMV	broadcast/MultiTracker.scala	/^  private var trackMV: TrackMultipleValues = null$/;"	v
trackerActor	MapOutputTracker.scala	/^  var trackerActor: ActorRef = _$/;"	v
tries	network/ConnectionManager.scala	/^        var tries: Int = 10$/;"	v
tries	rdd/CoalescedRDD.scala	/^      var tries = 0$/;"	v
tries	rdd/CoalescedRDD.scala	/^    var tries = 0$/;"	v
tries	storage/DiskBlockManager.scala	/^      var tries = 0$/;"	v
trim	util/collection/PrimitiveVector.scala	/^  def trim(): PrimitiveVector[V] = resize(size)$/;"	m
trimIfNecessary	ui/jobs/JobProgressListener.scala	/^  def trimIfNecessary(stages: ListBuffer[StageInfo]) = synchronized {$/;"	m
ts	storage/BlockObjectWriter.scala	/^  private var ts: TimeTrackingOutputStream = null$/;"	v
ttGuide	broadcast/BitTorrentBroadcast.scala	/^    var ttGuide = new TalkToGuide(gInfo)$/;"	v
ttGuide	broadcast/BitTorrentBroadcast.scala	/^  @transient var ttGuide: TalkToGuide = null$/;"	v
ttl	metrics/sink/GangliaSink.scala	/^  val ttl = propertyToOption(GANGLIA_KEY_TTL).map(_.toInt).getOrElse(GANGLIA_DEFAULT_TTL)$/;"	V
typ	network/Message.scala	/^private[spark] abstract class Message(val typ: Long, val id: Int) {$/;"	V
typ	network/MessageChunkHeader.scala	/^    val typ = buffer.getLong()$/;"	V
typ	network/MessageChunkHeader.scala	/^    val typ: Long,$/;"	V
typ	storage/BlockMessage.scala	/^  private var typ: Int = BlockMessage.TYPE_NON_INITIALIZED$/;"	v
uator	SparkContext.scala	/^      evaluator: ApproximateEvaluator[U, R],$/;"	V
uator	partial/ApproximateActionListener.scala	/^    evaluator: ApproximateEvaluator[U, R],$/;"	V
uator	scheduler/DAGScheduler.scala	/^      evaluator: ApproximateEvaluator[U, R],$/;"	V
ue	Accumulators.scala	/^    this.value = newValue$/;"	V
ue	Accumulators.scala	/^  def value: R = {$/;"	V
ue	FutureAction.scala	/^  override def value: Option[Try[T]] = p.future.value$/;"	V
ue	FutureAction.scala	/^  override def value: Option[Try[T]] = {$/;"	V
ue	FutureAction.scala	/^  override def value: Option[Try[T]]$/;"	V
ue	SerializableWritable.scala	/^  def value = t$/;"	V
ue	SparkContext.scala	/^    if (value == null) {$/;"	V
ue	SparkContext.scala	/^  def broadcast[T](value: T) = env.broadcastManager.newBroadcast[T](value, isLocal)$/;"	V
ue	SparkContext.scala	/^  def setJobDescription(value: String) {$/;"	V
ue	SparkContext.scala	/^  def setLocalProperty(key: String, value: String) {$/;"	V
ue	SparkHadoopWriter.scala	/^  def write(key: AnyRef, value: AnyRef) {$/;"	V
ue	api/java/JavaSparkContext.scala	/^  def broadcast[T](value: T): Broadcast[T] = sc.broadcast(value)$/;"	V
ue	api/python/PythonRDD.scala	/^      case x          => throw new SparkException("PairwiseRDD: unexpected value: " + x)$/;"	V
ue	api/python/PythonRDD.scala	/^  override def zero(value: JList[Array[Byte]]): JList[Array[Byte]] = new JArrayList$/;"	V
ue	broadcast/BitTorrentBroadcast.scala	/^  def value = value_$/;"	V
ue	broadcast/Broadcast.scala	/^  def value: T$/;"	V
ue	broadcast/BroadcastFactory.scala	/^  def newBroadcast[T](value: T, isLocal: Boolean, id: Long): Broadcast[T]$/;"	V
ue	broadcast/HttpBroadcast.scala	/^  def value = value_$/;"	V
ue	broadcast/HttpBroadcast.scala	/^  def write(id: Long, value: Any) {$/;"	V
ue	broadcast/TorrentBroadcast.scala	/^  def value = value_$/;"	V
ue	broadcast/TreeBroadcast.scala	/^  def value = value_$/;"	V
ue	deploy/master/FileSystemPersistenceEngine.scala	/^  private def serializeIntoFile(file: File, value: Serializable) {$/;"	V
ue	deploy/master/MasterArguments.scala	/^    case ("--host" | "-h") :: value :: tail =>$/;"	V
ue	deploy/master/MasterArguments.scala	/^    case ("--ip" | "-i") :: value :: tail =>$/;"	V
ue	deploy/master/ZooKeeperPersistenceEngine.scala	/^  private def serializeIntoFile(path: String, value: Serializable) {$/;"	V
ue	deploy/worker/WorkerArguments.scala	/^    case ("--host" | "-h") :: value :: tail =>$/;"	V
ue	deploy/worker/WorkerArguments.scala	/^    case ("--ip" | "-i") :: value :: tail =>$/;"	V
ue	deploy/worker/WorkerArguments.scala	/^    case ("--work-dir" | "-d") :: value :: tail =>$/;"	V
ue	deploy/worker/WorkerArguments.scala	/^    case value :: tail =>$/;"	V
ue	partial/PartialResult.scala	/^  private[spark] def setFinalValue(value: R) {$/;"	V
ue	rdd/HadoopRDD.scala	/^  def putCachedMetadata(key: String, value: Any) =$/;"	V
ue	scheduler/TaskResult.scala	/^    value = objectSer.deserialize(ByteBuffer.wrap(byteVal))$/;"	V
ue	scheduler/TaskResult.scala	/^class DirectTaskResult[T](var value: T, var accumUpdates: Map[Long, Any], var metrics: TaskMetrics)$/;"	V
ue	scheduler/cluster/ClusterScheduler.scala	/^  def getRackForHost(value: String): Option[String] = None$/;"	V
ue	storage/BlockManager.scala	/^  def putSingle(blockId: BlockId, value: Any, level: StorageLevel, tellMaster: Boolean = true) {$/;"	V
ue	storage/BlockObjectWriter.scala	/^  def write(value: Any)$/;"	V
ue	storage/BlockObjectWriter.scala	/^  override def write(value: Any) {$/;"	V
ue	storage/MemoryStore.scala	/^  case class Entry(value: Any, size: Long, deserialized: Boolean)$/;"	V
ue	storage/MemoryStore.scala	/^  private def tryToPut(blockId: BlockId, value: Any, size: Long, deserialized: Boolean): Boolean = {$/;"	V
ue	util/AppendOnlyMap.scala	/^      if (value == null) {$/;"	V
ue	util/AppendOnlyMap.scala	/^  def update(key: K, value: V): Unit = {$/;"	V
ue	util/SerializableBuffer.scala	/^  def value = buffer$/;"	V
ue	util/StatCounter.scala	/^  def merge(value: Double): StatCounter = {$/;"	V
ue	util/TimeStampedHashMap.scala	/^    if (value == null) throw new NoSuchElementException()$/;"	V
ue	util/TimeStampedHashMap.scala	/^  def putIfAbsent(key: A, value: B): Option[B] = {$/;"	V
ue	util/TimeStampedHashMap.scala	/^  override def update(key: A, value: B) {$/;"	V
ue	util/Utils.scala	/^  def clone[T](value: T, serializer: SerializerInstance): T = {$/;"	V
ue	util/collection/PrimitiveVector.scala	/^  def +=(value: V) {$/;"	V
ueClass	SparkContext.scala	/^      valueClass: Class[V],$/;"	V
ueClass	SparkContext.scala	/^  def sequenceFile[K, V](path: String, keyClass: Class[K], valueClass: Class[V]): RDD[(K, V)] =$/;"	V
ueClass	api/java/JavaPairRDD.scala	/^    valueClass: Class[_],$/;"	V
ueClass	api/java/JavaSparkContext.scala	/^    valueClass: Class[V]$/;"	V
ueClass	api/java/JavaSparkContext.scala	/^    valueClass: Class[V],$/;"	V
ueClass	api/java/JavaSparkContext.scala	/^  def sequenceFile[K, V](path: String, keyClass: Class[K], valueClass: Class[V]):$/;"	V
ueClass	rdd/HadoopRDD.scala	/^      valueClass: Class[V],$/;"	V
ueClass	rdd/HadoopRDD.scala	/^    valueClass: Class[V],$/;"	V
ueClass	rdd/NewHadoopRDD.scala	/^    valueClass: Class[V],$/;"	V
ueClass	rdd/PairRDDFunctions.scala	/^      valueClass: Class[_],$/;"	V
ueClass	rdd/PairRDDFunctions.scala	/^    if (valueClass == null) {$/;"	V
ueToGuideMap	broadcast/MultiTracker.scala	/^  var valueToGuideMap = Map[Long, SourceInfo]()$/;"	V
ue_	Accumulators.scala	/^    if (!deserialized) value_ = newValue$/;"	V
ue_	Accumulators.scala	/^    value_ = zero$/;"	V
ue_	Accumulators.scala	/^  @transient private var value_ = initialValue \/\/ Current value on master$/;"	V
ue_	Accumulators.scala	/^  def ++= (term: R) { value_ = param.addInPlace(value_, term)}$/;"	V
ue_	Accumulators.scala	/^  def += (term: T) { value_ = param.addAccumulator(value_, term) }$/;"	V
ue_	Accumulators.scala	/^  def add(term: T) { value_ = param.addAccumulator(value_, term) }$/;"	V
ue_	Accumulators.scala	/^  def merge(term: R) { value_ = param.addInPlace(value_, term)}$/;"	V
ue_	Accumulators.scala	/^  def value_= (newValue: R) {$/;"	V
ue_	broadcast/BitTorrentBroadcast.scala	/^            value_ = MultiTracker.unBlockifyObject[T](arrayOfBlocks, totalBytes, totalBlocks)$/;"	V
ue_	broadcast/BitTorrentBroadcast.scala	/^          value_ = x.asInstanceOf[T]$/;"	V
ue_	broadcast/BitTorrentBroadcast.scala	/^  def newBroadcast[T](value_ : T, isLocal: Boolean, id: Long) =$/;"	V
ue_	broadcast/BitTorrentBroadcast.scala	/^private[spark] class BitTorrentBroadcast[T](@transient var value_ : T, isLocal: Boolean, id: Long)$/;"	V
ue_	broadcast/Broadcast.scala	/^  def newBroadcast[T](value_ : T, isLocal: Boolean) =$/;"	V
ue_	broadcast/HttpBroadcast.scala	/^          value_ = HttpBroadcast.read[T](id)$/;"	V
ue_	broadcast/HttpBroadcast.scala	/^        case Some(x) => value_ = x.asInstanceOf[T]$/;"	V
ue_	broadcast/HttpBroadcast.scala	/^  def newBroadcast[T](value_ : T, isLocal: Boolean, id: Long) =$/;"	V
ue_	broadcast/HttpBroadcast.scala	/^private[spark] class HttpBroadcast[T](@transient var value_ : T, isLocal: Boolean, id: Long)$/;"	V
ue_	broadcast/TorrentBroadcast.scala	/^            value_ = TorrentBroadcast.unBlockifyObject[T](arrayOfBlocks, totalBytes, totalBlocks)$/;"	V
ue_	broadcast/TorrentBroadcast.scala	/^          value_ = x.asInstanceOf[T]$/;"	V
ue_	broadcast/TorrentBroadcast.scala	/^  def newBroadcast[T](value_ : T, isLocal: Boolean, id: Long) =$/;"	V
ue_	broadcast/TorrentBroadcast.scala	/^private[spark] class TorrentBroadcast[T](@transient var value_ : T, isLocal: Boolean, id: Long)$/;"	V
ue_	broadcast/TreeBroadcast.scala	/^            value_ = MultiTracker.unBlockifyObject[T](arrayOfBlocks, totalBytes, totalBlocks)$/;"	V
ue_	broadcast/TreeBroadcast.scala	/^          value_ = x.asInstanceOf[T]$/;"	V
ue_	broadcast/TreeBroadcast.scala	/^  def newBroadcast[T](value_ : T, isLocal: Boolean, id: Long) =$/;"	V
ue_	broadcast/TreeBroadcast.scala	/^private[spark] class TreeBroadcast[T](@transient var value_ : T, isLocal: Boolean, id: Long)$/;"	V
ues	Accumulators.scala	/^  def add(values: Map[Long, Any]): Unit = synchronized {$/;"	V
ues	Accumulators.scala	/^  def values: Map[Long, Any] = synchronized {$/;"	V
ues	rdd/PairRDDFunctions.scala	/^  def values: RDD[V] = self.map(_._2)$/;"	V
ues	rdd/ParallelCollectionRDD.scala	/^        Utils.deserializeViaNestedStream(in, ser)(ds => values = ds.readObject())$/;"	V
ues	rdd/ParallelCollectionRDD.scala	/^    var values: Seq[T])$/;"	V
ues	storage/BlockManager.scala	/^      values: Iterator[Any],$/;"	V
ues	storage/BlockManager.scala	/^  def put(blockId: BlockId, values: ArrayBuffer[Any], level: StorageLevel,$/;"	V
ues	storage/BlockManager.scala	/^  def put(blockId: BlockId, values: Iterator[Any], level: StorageLevel, tellMaster: Boolean)$/;"	V
ues	storage/BlockStore.scala	/^  def putValues(blockId: BlockId, values: ArrayBuffer[Any], level: StorageLevel,$/;"	V
ues	storage/DiskStore.scala	/^      values: ArrayBuffer[Any],$/;"	V
ues	storage/MemoryStore.scala	/^      values: ArrayBuffer[Any],$/;"	V
ues	util/StatCounter.scala	/^  def apply(values: Double*) = new StatCounter(values)$/;"	V
ues	util/StatCounter.scala	/^  def apply(values: TraversableOnce[Double]) = new StatCounter(values)$/;"	V
ues	util/StatCounter.scala	/^  def merge(values: TraversableOnce[Double]): StatCounter = {$/;"	V
ues	util/StatCounter.scala	/^class StatCounter(values: TraversableOnce[Double]) extends Serializable {$/;"	V
ues	util/collection/OpenHashMap.scala	/^    _values = new Array[V](newCapacity)$/;"	V
ues	util/collection/OpenHashMap.scala	/^  _values = new Array[V](_keySet.capacity)$/;"	V
ues	util/collection/OpenHashMap.scala	/^  private var _values: Array[V] = _$/;"	V
ues	util/collection/PrimitiveKeyOpenHashMap.scala	/^    _values = new Array[V](newCapacity)$/;"	V
ues	util/collection/PrimitiveKeyOpenHashMap.scala	/^  _values = new Array[V](_keySet.capacity)$/;"	V
ues	util/collection/PrimitiveKeyOpenHashMap.scala	/^  private var _values: Array[V] = _$/;"	V
uesAfterPut	storage/BlockManager.scala	/^                case Left(newIterator) => valuesAfterPut = newIterator$/;"	V
uesAfterPut	storage/BlockManager.scala	/^            if (valuesAfterPut == null) {$/;"	V
uesAfterPut	storage/BlockManager.scala	/^    var valuesAfterPut: Iterator[Any] = null$/;"	V
ugi	deploy/SparkHadoopUtil.scala	/^      val ugi = UserGroupInformation.createRemoteUser(user)$/;"	V
ui	SparkContext.scala	/^  private[spark] val ui = new SparkUI(this)$/;"	V
uiRoot	ui/UIUtils.scala	/^  private[spark] val uiRoot : String = Option(System.getenv("APPLICATION_WEB_PROXY_BASE")).$/;"	V
unBlockifyObject	broadcast/MultiTracker.scala	/^  def unBlockifyObject[OUT](arrayOfBlocks: Array[BroadcastBlock],$/;"	m
unBlockifyObject	broadcast/TorrentBroadcast.scala	/^  def unBlockifyObject[T](arrayOfBlocks: Array[TorrentBlock],$/;"	m
unapply	storage/BlockManagerMessages.scala	/^    def unapply(h: UpdateBlockInfo): Option[(BlockManagerId, BlockId, StorageLevel, Long, Long)] = {$/;"	m
unapply	util/IntParam.scala	/^  def unapply(str: String): Option[Int] = {$/;"	m
unapply	util/MemoryParam.scala	/^  def unapply(str: String): Option[Int] = {$/;"	m
unary_	util/Vector.scala	/^  def unary_- = this * -1$/;"	m
underlying	util/BoundedPriorityQueue.scala	/^  private val underlying = new JPriorityQueue[A](maxSize, ord)$/;"	V
union	SparkContext.scala	/^  def union[T: ClassManifest](first: RDD[T], rest: RDD[T]*): RDD[T] =$/;"	m
union	SparkContext.scala	/^  def union[T: ClassManifest](rdds: Seq[RDD[T]]): RDD[T] = new UnionRDD(this, rdds)$/;"	m
union	api/java/JavaDoubleRDD.scala	/^  def union(other: JavaDoubleRDD): JavaDoubleRDD = fromRDD(srdd.union(other.srdd))$/;"	m
union	api/java/JavaPairRDD.scala	/^  def union(other: JavaPairRDD[K, V]): JavaPairRDD[K, V] =$/;"	m
union	api/java/JavaRDD.scala	/^  def union(other: JavaRDD[T]): JavaRDD[T] = wrapRDD(rdd.union(other.rdd))$/;"	m
union	api/java/JavaSparkContextVarargsWorkaround.java	/^  abstract public <K, V> JavaPairRDD<K, V> union(JavaPairRDD<K, V> first, List<JavaPairRDD<K, V>> rest);$/;"	m	class:JavaSparkContextVarargsWorkaround
union	api/java/JavaSparkContextVarargsWorkaround.java	/^  abstract public <T> JavaRDD<T> union(JavaRDD<T> first, List<JavaRDD<T>> rest);$/;"	m	class:JavaSparkContextVarargsWorkaround
union	api/java/JavaSparkContextVarargsWorkaround.java	/^  abstract public JavaDoubleRDD union(JavaDoubleRDD first, List<JavaDoubleRDD> rest);$/;"	m	class:JavaSparkContextVarargsWorkaround
union	api/java/JavaSparkContextVarargsWorkaround.java	/^  public <K, V> JavaPairRDD<K, V> union(JavaPairRDD<K, V>... rdds) {$/;"	m	class:JavaSparkContextVarargsWorkaround
union	api/java/JavaSparkContextVarargsWorkaround.java	/^  public <T> JavaRDD<T> union(JavaRDD<T>... rdds) {$/;"	m	class:JavaSparkContextVarargsWorkaround
union	api/java/JavaSparkContextVarargsWorkaround.java	/^  public JavaDoubleRDD union(JavaDoubleRDD... rdds) {$/;"	m	class:JavaSparkContextVarargsWorkaround
union	rdd/RDD.scala	/^  def union(other: RDD[T]): RDD[T] = new UnionRDD(sc, Array(this, other))$/;"	m
unpackBlock	BlockStoreShuffleFetcher.scala	/^    def unpackBlock(blockPair: (BlockId, Option[Iterator[Any]])) : Iterator[T] = {$/;"	m
unpersist	api/java/JavaDoubleRDD.scala	/^  def unpersist(): JavaDoubleRDD = fromRDD(srdd.unpersist())$/;"	m
unpersist	api/java/JavaDoubleRDD.scala	/^  def unpersist(blocking: Boolean): JavaDoubleRDD = fromRDD(srdd.unpersist(blocking))$/;"	m
unpersist	api/java/JavaPairRDD.scala	/^  def unpersist(): JavaPairRDD[K, V] = wrapRDD(rdd.unpersist())$/;"	m
unpersist	api/java/JavaPairRDD.scala	/^  def unpersist(blocking: Boolean): JavaPairRDD[K, V] = wrapRDD(rdd.unpersist(blocking))$/;"	m
unpersist	api/java/JavaRDD.scala	/^  def unpersist(): JavaRDD[T] = wrapRDD(rdd.unpersist())$/;"	m
unpersist	api/java/JavaRDD.scala	/^  def unpersist(blocking: Boolean): JavaRDD[T] = wrapRDD(rdd.unpersist(blocking))$/;"	m
unpersist	rdd/RDD.scala	/^  def unpersist(blocking: Boolean = true): RDD[T] = {$/;"	m
unregisterBroadcast	broadcast/MultiTracker.scala	/^  def unregisterBroadcast(id: Long) {$/;"	m
unregisterMapOutput	MapOutputTracker.scala	/^  def unregisterMapOutput(shuffleId: Int, mapId: Int, bmAddress: BlockManagerId) {$/;"	m
unusedFileGroups	storage/ShuffleBlockManager.scala	/^    val unusedFileGroups = new ConcurrentLinkedQueue[ShuffleFileGroup]()$/;"	V
update	Aggregator.scala	/^    val update = (hadValue: Boolean, oldValue: C) => {$/;"	V
update	api/python/PythonRDD.scala	/^                val update = new Array[Byte](len2)$/;"	V
update	rdd/CoGroupedRDD.scala	/^    val update: (Boolean, Seq[ArrayBuffer[Any]]) => Seq[ArrayBuffer[Any]] = (hadVal, oldVal) => {$/;"	V
update	util/AppendOnlyMap.scala	/^  def update(key: K, value: V): Unit = {$/;"	m
update	util/collection/OpenHashMap.scala	/^  def update(k: K, v: V) {$/;"	m
update	util/collection/PrimitiveKeyOpenHashMap.scala	/^  def update(k: K, v: V) {$/;"	m
updateBlockInfo	storage/BlockManagerMasterActor.scala	/^    def updateBlockInfo(blockId: BlockId, storageLevel: StorageLevel, memSize: Long,$/;"	m
updateEpoch	MapOutputTracker.scala	/^  def updateEpoch(newEpoch: Long) {$/;"	m
updateLeadershipStatus	deploy/master/ZooKeeperLeaderElectionAgent.scala	/^  def updateLeadershipStatus(isLeader: Boolean) {$/;"	m
updatedConf	SparkContext.scala	/^    val updatedConf = job.getConfiguration$/;"	V
uri	HttpServer.scala	/^  def uri: String = {$/;"	m
uri	SparkContext.scala	/^        val uri = new URI(path)$/;"	V
uri	SparkContext.scala	/^    val uri = cls.getResource("\/" + cls.getName.replace('.', '\/') + ".class")$/;"	V
uri	SparkContext.scala	/^    val uri = new URI(path)$/;"	V
uri	deploy/DeployMessage.scala	/^    def uri = "spark:\/\/" + host + ":" + port$/;"	m
uri	scheduler/cluster/mesos/CoarseMesosSchedulerBackend.scala	/^    val uri = System.getProperty("spark.executor.uri")$/;"	V
uri	scheduler/cluster/mesos/MesosSchedulerBackend.scala	/^    val uri = System.getProperty("spark.executor.uri")$/;"	V
uri	util/Utils.scala	/^        val uri = new URI(url)$/;"	V
uri	util/Utils.scala	/^    val uri = new URI(url)$/;"	V
uriStr	SparkContext.scala	/^      val uriStr = uri.toString$/;"	V
url	SparkEnv.scala	/^        val url = "akka:\/\/spark@%s:%s\/user\/%s".format(driverHost, driverPort, name)$/;"	V
url	broadcast/HttpBroadcast.scala	/^    val url = serverUri + "\/" + BroadcastBlockId(id).name$/;"	V
url	deploy/client/TestClient.scala	/^    val url = args(0)$/;"	V
url	executor/Executor.scala	/^        val url = new File(SparkFiles.getRootDirectory, localName).toURI.toURL$/;"	V
urlClassLoader	executor/Executor.scala	/^  private val urlClassLoader = createClassLoader()$/;"	V
urls	executor/Executor.scala	/^    val urls = currentJars.keySet.map { uri =>$/;"	V
usableWorkers	deploy/master/Master.scala	/^        val usableWorkers = workers.toArray.filter(_.state == WorkerState.ALIVE)$/;"	V
useDaemon	api/python/PythonWorkerFactory.scala	/^  val useDaemon = !System.getProperty("os.name").startsWith("Windows")$/;"	V
useDisk	storage/StorageLevel.scala	/^  def useDisk = useDisk_$/;"	m
useDisk_	storage/StorageLevel.scala	/^    private var useDisk_ : Boolean,$/;"	v
useMemory	storage/StorageLevel.scala	/^  def useMemory = useMemory_$/;"	m
useMemory_	storage/StorageLevel.scala	/^    private var useMemory_ : Boolean,$/;"	v
useNetty	storage/BlockManager.scala	/^    val useNetty = System.getProperty("spark.shuffle.use.netty", "false").toBoolean$/;"	V
user	deploy/ApplicationDescription.scala	/^  val user = System.getProperty("user.name", "<unknown>")$/;"	V
user	scheduler/JobLogger.scala	/^class JobLogger(val user: String, val logDirName: String)$/;"	V
userOpts	deploy/worker/ExecutorRunner.scala	/^    val userOpts = getAppEnv("SPARK_JAVA_OPTS").map(Utils.splitCommandString).getOrElse(Nil)$/;"	V
util.Random	storage/ThreadingTest.scala	/^import util.Random$/;"	i
v	rdd/RDD.scala	/^        val v = iter.next()$/;"	V
vManifest	api/java/JavaPairRDD.scala	/^  implicit val vManifest: ClassManifest[V]) extends JavaRDDLike[(K, V), JavaPairRDD[K, V]] {$/;"	V
validExecutors	deploy/master/Master.scala	/^          val validExecutors = executors.filter(exec => idToApp.get(exec.appId).isDefined)$/;"	V
validTasks	ui/jobs/StagePage.scala	/^      val validTasks = tasks.filter(t => t._1.status == "SUCCESS" && (t._2.isDefined))$/;"	V
value	Accumulators.scala	/^  def value: R = {$/;"	m
value	SerializableWritable.scala	/^  def value = t$/;"	m
value	SparkContext.scala	/^    val value = System.getenv(key)$/;"	V
value	broadcast/BitTorrentBroadcast.scala	/^  def value = value_$/;"	m
value	broadcast/Broadcast.scala	/^  def value: T$/;"	m
value	broadcast/HttpBroadcast.scala	/^  def value = value_$/;"	m
value	broadcast/TorrentBroadcast.scala	/^  def value = value_$/;"	m
value	broadcast/TreeBroadcast.scala	/^  def value = value_$/;"	m
value	executor/Executor.scala	/^        val value = task.run(taskId.toInt)$/;"	V
value	rdd/HadoopRDD.scala	/^      val value: V = reader.createValue()$/;"	V
value	scheduler/TaskResult.scala	/^class DirectTaskResult[T](var value: T, var accumUpdates: Map[Long, Any], var metrics: TaskMetrics)$/;"	v
value	util/AppendOnlyMap.scala	/^        val value = data(2 * oldPos + 1)$/;"	V
value	util/AppendOnlyMap.scala	/^      val value = nextValue()$/;"	V
value	util/ClosureCleaner.scala	/^        val value = field.get(obj)$/;"	V
value	util/SerializableBuffer.scala	/^  def value = buffer$/;"	m
value	util/TimeStampedHashMap.scala	/^    val value = internalMap.get(key)$/;"	V
valueClass	rdd/PairRDDFunctions.scala	/^    val valueClass = conf.getOutputValueClass$/;"	V
valueClass	rdd/SequenceFileRDDFunctions.scala	/^    val valueClass = getWritableClass[V]$/;"	V
valueToGuideMap	broadcast/MultiTracker.scala	/^  var valueToGuideMap = Map[Long, SourceInfo]()$/;"	v
valueType	api/java/function/PairFlatMapFunction.java	/^  public ClassManifest<V> valueType() {$/;"	m	class:PairFlatMapFunction
valueType	api/java/function/PairFunction.java	/^  public ClassManifest<V> valueType() {$/;"	m	class:PairFunction
value_	Accumulators.scala	/^  @transient private var value_ = initialValue \/\/ Current value on master$/;"	v
value_	broadcast/BitTorrentBroadcast.scala	/^private[spark] class BitTorrentBroadcast[T](@transient var value_ : T, isLocal: Boolean, id: Long)$/;"	v
value_	broadcast/HttpBroadcast.scala	/^private[spark] class HttpBroadcast[T](@transient var value_ : T, isLocal: Boolean, id: Long)$/;"	v
value_	broadcast/TorrentBroadcast.scala	/^private[spark] class TorrentBroadcast[T](@transient var value_ : T, isLocal: Boolean, id: Long)$/;"	v
value_	broadcast/TreeBroadcast.scala	/^private[spark] class TreeBroadcast[T](@transient var value_ : T, isLocal: Boolean, id: Long)$/;"	v
value_=	Accumulators.scala	/^  def value_= (newValue: R) {$/;"	m
values	Accumulators.scala	/^  def values: Map[Long, Any] = synchronized {$/;"	m
values	api/java/JavaPairRDD.scala	/^  def values(): JavaRDD[V] = JavaRDD.fromRDD[V](rdd.map(_._2))$/;"	m
values	rdd/PairRDDFunctions.scala	/^      val values = new ShuffledRDD[K, V, (K, V)](self, partitioner).setSerializer(serializerClass)$/;"	V
values	rdd/PairRDDFunctions.scala	/^  def values: RDD[V] = self.map(_._2)$/;"	m
values	rdd/ParallelCollectionRDD.scala	/^    var values: Seq[T])$/;"	v
values	storage/BlockManager.scala	/^              val values = dataDeserialize(blockId, bytes)$/;"	V
values	storage/MemoryStore.scala	/^      val values = blockManager.dataDeserialize(blockId, bytes)$/;"	V
valuesAfterPut	storage/BlockManager.scala	/^    var valuesAfterPut: Iterator[Any] = null$/;"	v
valuesBuffer	storage/BlockManager.scala	/^                val valuesBuffer = new ArrayBuffer[Any]$/;"	V
variableInfo	broadcast/BitTorrentBroadcast.scala	/^    var variableInfo = MultiTracker.blockifyObject(value_)$/;"	v
variableInfo	broadcast/MultiTracker.scala	/^    var variableInfo = VariableInfo(retVal, blockNum, byteArray.length)$/;"	v
variableInfo	broadcast/TreeBroadcast.scala	/^    var variableInfo = MultiTracker.blockifyObject(value_)$/;"	v
variance	api/java/JavaDoubleRDD.scala	/^  def variance(): Double = srdd.variance()$/;"	m
variance	partial/CountEvaluator.scala	/^      val variance = (sum + 1) * (1 - p) \/ (p * p)$/;"	V
variance	partial/GroupedCountEvaluator.scala	/^        val variance = (sum + 1) * (1 - p) \/ (p * p)$/;"	V
variance	rdd/DoubleRDDFunctions.scala	/^  def variance(): Double = stats().variance$/;"	m
variance	util/StatCounter.scala	/^  def variance: Double = {$/;"	m
vc	SparkContext.scala	/^    val vc = vcf()$/;"	V
vcm	api/java/JavaRDDLike.scala	/^    implicit val vcm: ClassManifest[JList[T]] =$/;"	V
vcm	api/java/JavaSparkContext.scala	/^    implicit val vcm = ClassManifest.fromClass(vClass)$/;"	V
vcm	api/java/JavaSparkContext.scala	/^    implicit val vcm = ClassManifest.fromClass(valueClass)$/;"	V
vcm	api/java/JavaSparkContext.scala	/^    implicit val vcm: ClassManifest[V] = first.vManifest$/;"	V
vcm	api/java/JavaSparkContext.scala	/^    implicit val vcm: ClassManifest[V] =$/;"	V
visit	scheduler/DAGScheduler.scala	/^    def visit(r: RDD[_]) {$/;"	m
visit	scheduler/DAGScheduler.scala	/^    def visit(rdd: RDD[_]) {$/;"	m
visited	scheduler/DAGScheduler.scala	/^    val visited = new HashSet[RDD[_]]$/;"	V
visited	util/SizeEstimator.scala	/^  private class SearchState(val visited: IdentityHashMap[AnyRef, AnyRef]) {$/;"	V
visitedRdds	scheduler/DAGScheduler.scala	/^    val visitedRdds = new HashSet[RDD[_]]$/;"	V
visitedStages	scheduler/DAGScheduler.scala	/^    val visitedStages = new HashSet[Stage]$/;"	V
wClass	SparkContext.scala	/^    val wClass = classManifest[W].erasure.asInstanceOf[Class[W]]$/;"	V
waitForReady	storage/BlockInfo.scala	/^  def waitForReady(): Boolean = {$/;"	m
waitUntilEmpty	scheduler/SparkListenerBus.scala	/^  def waitUntilEmpty(timeoutMillis: Int): Boolean = {$/;"	m
waiter	SparkContext.scala	/^    val waiter = dagScheduler.submitJob($/;"	V
waiter	scheduler/DAGScheduler.scala	/^    val waiter = new JobWaiter(this, jobId, partitions.size, resultHandler)$/;"	V
waiter	scheduler/DAGScheduler.scala	/^    val waiter = submitJob(rdd, func, partitions, callSite, allowLocal, resultHandler, properties)$/;"	V
waiting	scheduler/DAGScheduler.scala	/^  val waiting = new HashSet[Stage] \/\/ Stages we need to run whose parents aren't done$/;"	V
waiting2	scheduler/DAGScheduler.scala	/^    val waiting2 = waiting.toArray$/;"	V
waitingApps	deploy/master/Master.scala	/^  val waitingApps = new ArrayBuffer[ApplicationInfo]$/;"	V
watcher	deploy/master/SparkZooKeeperSession.scala	/^  private val watcher = new ZooKeeperWatcher()$/;"	V
watcher	deploy/master/ZooKeeperLeaderElectionAgent.scala	/^  private val watcher = new ZooKeeperWatcher()$/;"	V
webUi	deploy/master/Master.scala	/^  val webUi = new MasterWebUI(this, webUiPort)$/;"	V
webUi	deploy/worker/Worker.scala	/^  var webUi: WorkerWebUI = null$/;"	v
webUiAddress	deploy/master/WorkerInfo.scala	/^  def webUiAddress : String = {$/;"	m
webUiPort	deploy/master/MasterArguments.scala	/^  var webUiPort = 8080$/;"	v
webUiPort	deploy/master/WorkerInfo.scala	/^    val webUiPort: Int,$/;"	V
webUiPort	deploy/worker/WorkerArguments.scala	/^  var webUiPort = 8081$/;"	v
weight	scheduler/Pool.scala	/^  var weight = initWeight$/;"	v
weight	scheduler/Schedulable.scala	/^  def weight: Int$/;"	m
weight	scheduler/SchedulableBuilder.scala	/^      var weight = DEFAULT_WEIGHT$/;"	v
weight	scheduler/cluster/ClusterTaskSetManager.scala	/^  var weight = 1$/;"	v
weight	scheduler/local/LocalTaskSetManager.scala	/^  var weight: Int = 1$/;"	v
word	util/collection/BitSet.scala	/^    var word = words(wordIndex) >> subIndex$/;"	v
wordIndex	util/collection/BitSet.scala	/^    var wordIndex = fromIndex >> 6$/;"	v
words	util/collection/BitSet.scala	/^  private[this] val words = new Array[Long](bit2words(numBits))$/;"	V
workDir	deploy/worker/ExecutorRunner.scala	/^    val workDir: File,$/;"	V
workDir	deploy/worker/Worker.scala	/^  var workDir: File = null$/;"	v
workDir	deploy/worker/WorkerArguments.scala	/^  var workDir: String = null$/;"	v
worker	api/python/PythonRDD.scala	/^    val worker = env.createPythonWorker(pythonExec, envVars.toMap)$/;"	V
worker	api/python/PythonWorkerFactory.scala	/^      val worker = pb.start()$/;"	V
worker	deploy/master/ExecutorInfo.scala	/^    val worker: WorkerInfo,$/;"	V
worker	deploy/master/Master.scala	/^        val worker = new WorkerInfo(id, host, port, cores, memory, sender, webUiPort, publicAddress)$/;"	V
worker	deploy/worker/ExecutorRunner.scala	/^    val worker: ActorRef,$/;"	V
worker	deploy/worker/WorkerSource.scala	/^private[spark] class WorkerSource(val worker: Worker) extends Source {$/;"	V
worker	deploy/worker/ui/IndexPage.scala	/^  val worker = parent.worker$/;"	V
worker	deploy/worker/ui/WorkerWebUI.scala	/^class WorkerWebUI(val worker: Worker, val workDir: File, requestedPort: Option[Int] = None)$/;"	V
workerActor	deploy/worker/ui/IndexPage.scala	/^  val workerActor = parent.worker.self$/;"	V
workerActorSystems	deploy/LocalSparkCluster.scala	/^  private val workerActorSystems = ArrayBuffer[ActorSystem]()$/;"	V
workerAddress	deploy/master/Master.scala	/^    val workerAddress = worker.actor.path.address$/;"	V
workerEnv	api/python/PythonWorkerFactory.scala	/^        val workerEnv = pb.environment()$/;"	V
workerEnv	api/python/PythonWorkerFactory.scala	/^      val workerEnv = pb.environment()$/;"	V
workerFile	deploy/master/FileSystemPersistenceEngine.scala	/^    val workerFile = new File(dir + File.separator + "worker_" + worker.id)$/;"	V
workerFiles	deploy/master/FileSystemPersistenceEngine.scala	/^    val workerFiles = sortedFiles.filter(_.getName.startsWith("worker_"))$/;"	V
workerFiles	deploy/master/ZooKeeperPersistenceEngine.scala	/^    val workerFiles = sortedFiles.filter(_.startsWith("worker_"))$/;"	V
workerHeaders	deploy/master/ui/IndexPage.scala	/^    val workerHeaders = Seq("Id", "Address", "State", "Cores", "Memory")$/;"	V
workerHeaders	ui/storage/RDDPage.scala	/^    val workerHeaders = Seq("Host", "Memory Usage", "Disk Usage")$/;"	V
workerId	deploy/worker/ExecutorRunner.scala	/^    val workerId: String,$/;"	V
workerId	deploy/worker/Worker.scala	/^  val workerId = generateWorkerId()$/;"	V
workerLocalOpts	deploy/worker/ExecutorRunner.scala	/^    val workerLocalOpts = Option(getenv("SPARK_JAVA_OPTS")).map(Utils.splitCommandString).getOrElse(Nil)$/;"	V
workerRow	deploy/master/ui/IndexPage.scala	/^  def workerRow(worker: WorkerInfo): Seq[Node] = {$/;"	m
workerRow	ui/storage/RDDPage.scala	/^  def workerRow(worker: (Int, StorageStatus)): Seq[Node] = {$/;"	m
workerSource	deploy/worker/Worker.scala	/^  val workerSource = new WorkerSource(this)$/;"	V
workerState	deploy/worker/ui/IndexPage.scala	/^    val workerState = Await.result(stateFuture, 30 seconds)$/;"	V
workerTable	deploy/master/ui/IndexPage.scala	/^    val workerTable = UIUtils.listingTable(workerHeaders, workerRow, workers)$/;"	V
workerTable	ui/storage/RDDPage.scala	/^    val workerTable = listingTable(workerHeaders, workerRow, workers)$/;"	V
workerThread	deploy/worker/ExecutorRunner.scala	/^  var workerThread: Thread = null$/;"	v
workers	deploy/FaultToleranceTest.scala	/^      val workers = json \\ "workers"$/;"	V
workers	deploy/FaultToleranceTest.scala	/^  val workers = ListBuffer[TestWorkerInfo]()$/;"	V
workers	deploy/master/FileSystemPersistenceEngine.scala	/^    val workers = workerFiles.map(deserializeFromFile[WorkerInfo])$/;"	V
workers	deploy/master/Master.scala	/^  val workers = new HashSet[WorkerInfo]$/;"	V
workers	deploy/master/ZooKeeperPersistenceEngine.scala	/^    val workers = workerFiles.map(deserializeFromFile[WorkerInfo])$/;"	V
workers	deploy/master/ui/IndexPage.scala	/^    val workers = state.workers.sortBy(_.id)$/;"	V
workers	ui/storage/RDDPage.scala	/^    val workers = filteredStorageStatusList.map((id, _))$/;"	V
workersAlive	deploy/master/Master.scala	/^    val workersAlive = workers.filter(_.state == WorkerState.ALIVE).toArray$/;"	V
wrapForCompression	storage/BlockManager.scala	/^  def wrapForCompression(blockId: BlockId, s: InputStream): InputStream = {$/;"	m
wrapForCompression	storage/BlockManager.scala	/^  def wrapForCompression(blockId: BlockId, s: OutputStream): OutputStream = {$/;"	m
wrapRDD	api/java/JavaRDDLike.scala	/^  def wrapRDD(rdd: RDD[T]): This$/;"	m
wrappedConf	rdd/PairRDDFunctions.scala	/^    val wrappedConf = new SerializableWritable(job.getConfiguration)$/;"	V
writableClass	SparkContext.scala	/^    val writableClass: ClassManifest[T] => Class[_ <: Writable],$/;"	V
writables	SparkContext.scala	/^    val writables = hadoopFile(path, format,$/;"	V
write	SparkHadoopWriter.scala	/^  def write(key: AnyRef, value: AnyRef) {$/;"	m
write	broadcast/HttpBroadcast.scala	/^  def write(id: Long, value: Any) {$/;"	m
write	network/Connection.scala	/^  def write(): Boolean = {$/;"	m
write	storage/BlockObjectWriter.scala	/^    def write(i: Int): Unit = callWithTiming(out.write(i))$/;"	m
write	storage/BlockObjectWriter.scala	/^  def write(value: Any)$/;"	m
write	util/Utils.scala	/^      def write(b: Int) = os.write(b)$/;"	m
writeAll	serializer/Serializer.scala	/^  def writeAll[T](iter: Iterator[T]): SerializationStream = {$/;"	m
writeApplicationDescription	deploy/JsonProtocol.scala	/^  def writeApplicationDescription(obj: ApplicationDescription) = {$/;"	m
writeApplicationInfo	deploy/JsonProtocol.scala	/^  def writeApplicationInfo(obj: ApplicationInfo) = {$/;"	m
writeAsPickle	api/python/PythonRDD.scala	/^  def writeAsPickle(elem: Any, dOut: DataOutputStream) {$/;"	m
writeByteBuffer	util/Utils.scala	/^  def writeByteBuffer(bb: ByteBuffer, out: ObjectOutput) = {$/;"	m
writeData	storage/StoragePerfTester.scala	/^    val writeData = "1" * recordLength$/;"	V
writeExecutorRunner	deploy/JsonProtocol.scala	/^  def writeExecutorRunner(obj: ExecutorRunner) = {$/;"	m
writeExternal	scheduler/MapStatus.scala	/^  def writeExternal(out: ObjectOutput) {$/;"	m
writeInfo	scheduler/JobLogger.scala	/^    var writeInfo = info$/;"	v
writeIteratorToPickleFile	api/python/PythonRDD.scala	/^  def writeIteratorToPickleFile[T](items: Iterator[T], filename: String) {$/;"	m
writeIteratorToPickleFile	api/python/PythonRDD.scala	/^  def writeIteratorToPickleFile[T](items: java.util.Iterator[T], filename: String) {$/;"	m
writeMasterState	deploy/JsonProtocol.scala	/^  def writeMasterState(obj: MasterStateResponse) = {$/;"	m
writeMetrics	scheduler/JobLogger.scala	/^    val writeMetrics = taskMetrics.shuffleWriteMetrics match {$/;"	V
writeObject	serializer/JavaSerializer.scala	/^  def writeObject[T](t: T): SerializationStream = { objOut.writeObject(t); this }$/;"	m
writeObject	serializer/KryoSerializer.scala	/^  def writeObject[T](t: T): SerializationStream = {$/;"	m
writeObject	serializer/Serializer.scala	/^  def writeObject[T](t: T): SerializationStream$/;"	m
writeOutputBytes	storage/StoragePerfTester.scala	/^    def writeOutputBytes(mapId: Int, total: AtomicLong) = {$/;"	m
writeRunnableStarted	network/ConnectionManager.scala	/^  private val writeRunnableStarted: HashSet[SelectionKey] = new HashSet[SelectionKey]()$/;"	V
writeShard	rdd/PairRDDFunctions.scala	/^    def writeShard(context: TaskContext, iter: Iterator[(K,V)]): Int = {$/;"	m
writeSize	util/RateLimitedOutputStream.scala	/^    val writeSize = math.min(length - offset, CHUNK_SIZE)$/;"	V
writeTimeReadable	ui/jobs/StagePage.scala	/^    val writeTimeReadable = maybeWriteTime.map{ t => t \/ (1000 * 1000)}.map{ ms =>$/;"	V
writeTimeSortable	ui/jobs/StagePage.scala	/^    val writeTimeSortable = maybeWriteTime.map(_.toString).getOrElse("")$/;"	V
writeToFile	rdd/CheckpointRDD.scala	/^  def writeToFile[T](path: String, blockSize: Int = -1)(ctx: TaskContext, iterator: Iterator[T]) {$/;"	m
writeToFile	rdd/PairRDDFunctions.scala	/^    def writeToFile(context: TaskContext, iter: Iterator[(K, V)]) {$/;"	m
writeWorkerInfo	deploy/JsonProtocol.scala	/^ def writeWorkerInfo(obj: WorkerInfo) = {$/;"	m
writeWorkerState	deploy/JsonProtocol.scala	/^  def writeWorkerState(obj: WorkerStateResponse) = {$/;"	m
writer	SparkHadoopWriter.scala	/^  @transient private var writer: RecordWriter[AnyRef,AnyRef] = null$/;"	v
writer	rdd/PairRDDFunctions.scala	/^      val writer = format.getRecordWriter(hadoopContext).asInstanceOf[NewRecordWriter[K,V]]$/;"	V
writer	rdd/PairRDDFunctions.scala	/^    val writer = new SparkHadoopWriter(conf)$/;"	V
writers	storage/ShuffleBlockManager.scala	/^      val writers: Array[BlockObjectWriter] = if (consolidateShuffleFiles) {$/;"	V
writers	storage/ShuffleBlockManager.scala	/^  val writers: Array[BlockObjectWriter]$/;"	V
writers	storage/StoragePerfTester.scala	/^      val writers = shuffle.writers$/;"	V
writtenBytes	network/Connection.scala	/^          val writtenBytes = channel.write(buffer)$/;"	V
xml	scheduler/SchedulableBuilder.scala	/^    val xml = XML.load(is)$/;"	V
xmlMinShare	scheduler/SchedulableBuilder.scala	/^      val xmlMinShare = (poolNode \\ MINIMUM_SHARES_PROPERTY).text$/;"	V
xmlSchedulingMode	scheduler/SchedulableBuilder.scala	/^      val xmlSchedulingMode = (poolNode \\ SCHEDULING_MODE_PROPERTY).text$/;"	V
xmlWeight	scheduler/SchedulableBuilder.scala	/^      val xmlWeight = (poolNode \\ WEIGHT_PROPERTY).text$/;"	V
yarnMode	deploy/SparkHadoopUtil.scala	/^    val yarnMode = java.lang.Boolean.valueOf(System.getProperty("SPARK_YARN_MODE", System.getenv("SPARK_YARN_MODE")))$/;"	V
zero	Accumulators.scala	/^  def zero(initialValue: R): R = {$/;"	m
zero	Accumulators.scala	/^  def zero(initialValue: R): R$/;"	m
zero	Accumulators.scala	/^  val zero = param.zero(initialValue)  \/\/ Zero value to be passed to workers$/;"	V
zero	SparkContext.scala	/^    def zero(initialValue: Double) = 0.0$/;"	m
zero	SparkContext.scala	/^    def zero(initialValue: Float) = 0f$/;"	m
zero	SparkContext.scala	/^    def zero(initialValue: Int) = 0$/;"	m
zero	SparkContext.scala	/^    def zero(initialValue: Long) = 0l$/;"	m
zero	util/Vector.scala	/^    def zero(initialValue: Vector) = Vector.zeros(initialValue.length)$/;"	m
zeroArray	rdd/PairRDDFunctions.scala	/^    val zeroArray = new Array[Byte](zeroBuffer.limit)$/;"	V
zeroBuffer	rdd/PairRDDFunctions.scala	/^    val zeroBuffer = SparkEnv.get.closureSerializer.newInstance().serialize(zeroValue)$/;"	V
zeros	util/Vector.scala	/^  def zeros(length: Int) = new Vector(new Array[Double](length))$/;"	m
zip	api/java/JavaRDDLike.scala	/^  def zip[U](other: JavaRDDLike[U, _]): JavaPairRDD[T, U] = {$/;"	m
zip	rdd/RDD.scala	/^  def zip[U: ClassManifest](other: RDD[U]): RDD[(T, U)] = new ZippedRDD(sc, this, other)$/;"	m
zipPartitions	rdd/RDD.scala	/^  def zipPartitions[B: ClassManifest, C: ClassManifest, D: ClassManifest, V: ClassManifest]$/;"	m
zipPartitions	rdd/RDD.scala	/^  def zipPartitions[B: ClassManifest, C: ClassManifest, V: ClassManifest]$/;"	m
zipPartitions	rdd/RDD.scala	/^  def zipPartitions[B: ClassManifest, V: ClassManifest]$/;"	m
zk	deploy/master/SparkZooKeeperSession.scala	/^  private var zk: ZooKeeper = _$/;"	v
zk	deploy/master/ZooKeeperLeaderElectionAgent.scala	/^  private val zk = new SparkZooKeeperSession(this)$/;"	V
zk	deploy/master/ZooKeeperPersistenceEngine.scala	/^  val zk = new SparkZooKeeperSession(this)$/;"	V
